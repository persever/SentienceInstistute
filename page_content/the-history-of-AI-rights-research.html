<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_xyqviq96mpp0-3>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-3,decimal) ". "}.lst-kix_pjjs8qck6gic-0>li:before{content:"\0025cf  "}.lst-kix_pjjs8qck6gic-2>li:before{content:"\0025a0  "}.lst-kix_xyqviq96mpp0-1>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-1,lower-latin) ". "}.lst-kix_xyqviq96mpp0-5>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-5,lower-roman) ". "}ul.lst-kix_yvok08a9t2ku-8{list-style-type:none}.lst-kix_xyqviq96mpp0-0>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-0,decimal) ". "}.lst-kix_xyqviq96mpp0-4>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-4,lower-latin) ". "}.lst-kix_pjjs8qck6gic-3>li:before{content:"\0025cf  "}ul.lst-kix_yvok08a9t2ku-6{list-style-type:none}.lst-kix_xyqviq96mpp0-7>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-7,lower-latin) ". "}.lst-kix_pjjs8qck6gic-5>li:before{content:"\0025a0  "}ul.lst-kix_yvok08a9t2ku-7{list-style-type:none}ul.lst-kix_yvok08a9t2ku-4{list-style-type:none}.lst-kix_pjjs8qck6gic-4>li:before{content:"\0025cb  "}.lst-kix_pjjs8qck6gic-6>li:before{content:"\0025cf  "}ul.lst-kix_yvok08a9t2ku-5{list-style-type:none}ul.lst-kix_yvok08a9t2ku-2{list-style-type:none}ul.lst-kix_yvok08a9t2ku-3{list-style-type:none}ul.lst-kix_yvok08a9t2ku-0{list-style-type:none}.lst-kix_xyqviq96mpp0-6>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-6,decimal) ". "}ul.lst-kix_yvok08a9t2ku-1{list-style-type:none}.lst-kix_pjjs8qck6gic-8>li:before{content:"\0025a0  "}.lst-kix_pjjs8qck6gic-7>li:before{content:"\0025cb  "}.lst-kix_xyqviq96mpp0-8>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-8,lower-roman) ". "}ul.lst-kix_cbtzokxiczrr-8{list-style-type:none}ol.lst-kix_xyqviq96mpp0-6.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-6 0}ul.lst-kix_cbtzokxiczrr-4{list-style-type:none}ul.lst-kix_cbtzokxiczrr-5{list-style-type:none}ul.lst-kix_cbtzokxiczrr-6{list-style-type:none}ul.lst-kix_cbtzokxiczrr-7{list-style-type:none}ul.lst-kix_cbtzokxiczrr-0{list-style-type:none}ul.lst-kix_cbtzokxiczrr-1{list-style-type:none}ul.lst-kix_cbtzokxiczrr-2{list-style-type:none}ul.lst-kix_cbtzokxiczrr-3{list-style-type:none}.lst-kix_pjjs8qck6gic-1>li:before{content:"\0025cb  "}.lst-kix_r8geitdi2968-0>li:before{content:"\0025cf  "}.lst-kix_r8geitdi2968-6>li:before{content:"\0025cf  "}.lst-kix_r8geitdi2968-8>li:before{content:"\0025a0  "}.lst-kix_r8geitdi2968-7>li:before{content:"\0025cb  "}ul.lst-kix_ta4grgw3z1c8-0{list-style-type:none}ul.lst-kix_ta4grgw3z1c8-1{list-style-type:none}ul.lst-kix_ta4grgw3z1c8-2{list-style-type:none}.lst-kix_r8geitdi2968-2>li:before{content:"\0025a0  "}ul.lst-kix_ta4grgw3z1c8-3{list-style-type:none}.lst-kix_r8geitdi2968-1>li:before{content:"\0025cb  "}.lst-kix_r8geitdi2968-3>li:before{content:"\0025cf  "}.lst-kix_r8geitdi2968-4>li:before{content:"\0025cb  "}.lst-kix_xyqviq96mpp0-2>li:before{content:"" counter(lst-ctn-kix_xyqviq96mpp0-2,lower-roman) ". "}.lst-kix_r8geitdi2968-5>li:before{content:"\0025a0  "}.lst-kix_u7j1747mo297-7>li:before{content:"\0025cb  "}.lst-kix_8t3wwjp4c0if-0>li:before{content:"\0025cf  "}.lst-kix_8t3wwjp4c0if-2>li:before{content:"\0025a0  "}.lst-kix_u7j1747mo297-1>li:before{content:"\0025cb  "}.lst-kix_xyqviq96mpp0-2>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-2}.lst-kix_u7j1747mo297-3>li:before{content:"\0025cf  "}.lst-kix_u7j1747mo297-5>li:before{content:"\0025a0  "}.lst-kix_5ewsmvf497zc-7>li:before{content:"\0025cb  "}ol.lst-kix_xyqviq96mpp0-4.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-4 0}.lst-kix_xyqviq96mpp0-1>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-1}.lst-kix_ntq2qcyufwa3-0>li:before{content:"\0025cf  "}.lst-kix_27p5taye3z5c-8>li:before{content:"\0025a0  "}.lst-kix_27p5taye3z5c-6>li:before{content:"\0025cf  "}.lst-kix_npvfo5a6zewi-0>li:before{content:"\0025cf  "}.lst-kix_cbtzokxiczrr-4>li:before{content:"\0025cb  "}.lst-kix_cbtzokxiczrr-8>li:before{content:"\0025a0  "}.lst-kix_27p5taye3z5c-4>li:before{content:"\0025cb  "}ul.lst-kix_npvfo5a6zewi-7{list-style-type:none}ul.lst-kix_npvfo5a6zewi-8{list-style-type:none}.lst-kix_cbtzokxiczrr-2>li:before{content:"\0025a0  "}ul.lst-kix_npvfo5a6zewi-3{list-style-type:none}ol.lst-kix_xyqviq96mpp0-6{list-style-type:none}ul.lst-kix_npvfo5a6zewi-4{list-style-type:none}ol.lst-kix_xyqviq96mpp0-5{list-style-type:none}ul.lst-kix_npvfo5a6zewi-5{list-style-type:none}ol.lst-kix_xyqviq96mpp0-8{list-style-type:none}ul.lst-kix_npvfo5a6zewi-6{list-style-type:none}ol.lst-kix_xyqviq96mpp0-7{list-style-type:none}ol.lst-kix_xyqviq96mpp0-2{list-style-type:none}ul.lst-kix_npvfo5a6zewi-0{list-style-type:none}ol.lst-kix_xyqviq96mpp0-1{list-style-type:none}ul.lst-kix_npvfo5a6zewi-1{list-style-type:none}.lst-kix_cbtzokxiczrr-0>li:before{content:"\0025cf  "}ol.lst-kix_xyqviq96mpp0-4{list-style-type:none}ul.lst-kix_npvfo5a6zewi-2{list-style-type:none}ol.lst-kix_xyqviq96mpp0-3{list-style-type:none}ol.lst-kix_xyqviq96mpp0-1.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-1 0}ol.lst-kix_xyqviq96mpp0-0{list-style-type:none}.lst-kix_5ewsmvf497zc-5>li:before{content:"\0025a0  "}.lst-kix_5ewsmvf497zc-3>li:before{content:"\0025cf  "}.lst-kix_27p5taye3z5c-0>li:before{content:"\0025cf  "}.lst-kix_5ewsmvf497zc-1>li:before{content:"\0025cb  "}.lst-kix_27p5taye3z5c-2>li:before{content:"\0025a0  "}.lst-kix_cbtzokxiczrr-6>li:before{content:"\0025cf  "}.lst-kix_ta4grgw3z1c8-7>li:before{content:"\0025cb  "}.lst-kix_6q32aygtq8j5-8>li:before{content:"\0025a0  "}.lst-kix_ta4grgw3z1c8-8>li:before{content:"\0025a0  "}.lst-kix_gzgwv5j5bqmj-2>li:before{content:"\0025a0  "}.lst-kix_octw518wga41-6>li:before{content:"\0025cf  "}.lst-kix_octw518wga41-5>li:before{content:"\0025a0  "}.lst-kix_gzgwv5j5bqmj-3>li:before{content:"\0025cf  "}.lst-kix_gzgwv5j5bqmj-7>li:before{content:"\0025cb  "}ul.lst-kix_u7j1747mo297-3{list-style-type:none}.lst-kix_6q32aygtq8j5-7>li:before{content:"\0025cb  "}ul.lst-kix_u7j1747mo297-2{list-style-type:none}ul.lst-kix_u7j1747mo297-1{list-style-type:none}ul.lst-kix_u7j1747mo297-0{list-style-type:none}.lst-kix_octw518wga41-2>li:before{content:"\0025a0  "}.lst-kix_7zddew8srudn-1>li:before{content:"\0025cb  "}.lst-kix_xyqviq96mpp0-8>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-8}.lst-kix_octw518wga41-1>li:before{content:"\0025cb  "}.lst-kix_ta4grgw3z1c8-3>li:before{content:"\0025cf  "}.lst-kix_7zddew8srudn-0>li:before{content:"\0025cf  "}.lst-kix_ta4grgw3z1c8-4>li:before{content:"\0025cb  "}.lst-kix_gzgwv5j5bqmj-6>li:before{content:"\0025cf  "}ul.lst-kix_pjjs8qck6gic-1{list-style-type:none}.lst-kix_7zddew8srudn-8>li:before{content:"\0025a0  "}ul.lst-kix_pjjs8qck6gic-0{list-style-type:none}.lst-kix_6q32aygtq8j5-0>li:before{content:"\0025cf  "}.lst-kix_7zddew8srudn-4>li:before{content:"\0025cb  "}.lst-kix_jmi704tbk5yj-1>li:before{content:"\0025cb  "}.lst-kix_7zddew8srudn-5>li:before{content:"\0025a0  "}.lst-kix_jmi704tbk5yj-0>li:before{content:"\0025cf  "}.lst-kix_npvfo5a6zewi-3>li:before{content:"\0025cf  "}ul.lst-kix_pjjs8qck6gic-5{list-style-type:none}.lst-kix_6q32aygtq8j5-4>li:before{content:"\0025cb  "}ul.lst-kix_pjjs8qck6gic-4{list-style-type:none}ul.lst-kix_pjjs8qck6gic-3{list-style-type:none}ul.lst-kix_pjjs8qck6gic-2{list-style-type:none}.lst-kix_6q32aygtq8j5-3>li:before{content:"\0025cf  "}ul.lst-kix_pjjs8qck6gic-8{list-style-type:none}.lst-kix_npvfo5a6zewi-2>li:before{content:"\0025a0  "}ul.lst-kix_pjjs8qck6gic-7{list-style-type:none}ul.lst-kix_pjjs8qck6gic-6{list-style-type:none}.lst-kix_ntq2qcyufwa3-5>li:before{content:"\0025a0  "}.lst-kix_jmi704tbk5yj-8>li:before{content:"\0025a0  "}.lst-kix_ntq2qcyufwa3-4>li:before{content:"\0025cb  "}.lst-kix_ntq2qcyufwa3-8>li:before{content:"\0025a0  "}ul.lst-kix_6q32aygtq8j5-8{list-style-type:none}ul.lst-kix_6q32aygtq8j5-7{list-style-type:none}ul.lst-kix_6q32aygtq8j5-6{list-style-type:none}ul.lst-kix_6q32aygtq8j5-5{list-style-type:none}.lst-kix_ntq2qcyufwa3-1>li:before{content:"\0025cb  "}ul.lst-kix_6q32aygtq8j5-4{list-style-type:none}ul.lst-kix_6q32aygtq8j5-3{list-style-type:none}ul.lst-kix_6q32aygtq8j5-2{list-style-type:none}ul.lst-kix_6q32aygtq8j5-1{list-style-type:none}.lst-kix_npvfo5a6zewi-6>li:before{content:"\0025cf  "}ul.lst-kix_6q32aygtq8j5-0{list-style-type:none}.lst-kix_1u9ir57n0if9-6>li:before{content:"\0025cf  "}ul.lst-kix_jmi704tbk5yj-2{list-style-type:none}.lst-kix_npvfo5a6zewi-7>li:before{content:"\0025cb  "}ul.lst-kix_jmi704tbk5yj-1{list-style-type:none}ul.lst-kix_jmi704tbk5yj-0{list-style-type:none}.lst-kix_jmi704tbk5yj-4>li:before{content:"\0025cb  "}.lst-kix_qhsfkm93huz6-1>li:before{content:"\0025cb  "}.lst-kix_1u9ir57n0if9-3>li:before{content:"\0025cf  "}.lst-kix_jmi704tbk5yj-5>li:before{content:"\0025a0  "}.lst-kix_qhsfkm93huz6-4>li:before{content:"\0025cb  "}.lst-kix_1u9ir57n0if9-2>li:before{content:"\0025a0  "}.lst-kix_qhsfkm93huz6-0>li:before{content:"\0025cf  "}ul.lst-kix_u7j1747mo297-7{list-style-type:none}ul.lst-kix_u7j1747mo297-6{list-style-type:none}.lst-kix_8t3wwjp4c0if-5>li:before{content:"\0025a0  "}ul.lst-kix_u7j1747mo297-5{list-style-type:none}.lst-kix_1u9ir57n0if9-7>li:before{content:"\0025cb  "}ul.lst-kix_u7j1747mo297-4{list-style-type:none}.lst-kix_8t3wwjp4c0if-4>li:before{content:"\0025cb  "}.lst-kix_8t3wwjp4c0if-8>li:before{content:"\0025a0  "}ul.lst-kix_u7j1747mo297-8{list-style-type:none}.lst-kix_u7j1747mo297-8>li:before{content:"\0025a0  "}ul.lst-kix_r8geitdi2968-2{list-style-type:none}ul.lst-kix_r8geitdi2968-3{list-style-type:none}.lst-kix_8t3wwjp4c0if-1>li:before{content:"\0025cb  "}ul.lst-kix_r8geitdi2968-0{list-style-type:none}ul.lst-kix_r8geitdi2968-1{list-style-type:none}.lst-kix_yvok08a9t2ku-7>li:before{content:"\0025cb  "}.lst-kix_u7j1747mo297-4>li:before{content:"\0025cb  "}ul.lst-kix_ntq2qcyufwa3-0{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-3{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-4{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-1{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-2{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-7{list-style-type:none}.lst-kix_qhsfkm93huz6-8>li:before{content:"\0025a0  "}ul.lst-kix_ntq2qcyufwa3-8{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-5{list-style-type:none}ul.lst-kix_ntq2qcyufwa3-6{list-style-type:none}ul.lst-kix_octw518wga41-3{list-style-type:none}ul.lst-kix_octw518wga41-2{list-style-type:none}.lst-kix_27p5taye3z5c-7>li:before{content:"\0025cb  "}ul.lst-kix_octw518wga41-1{list-style-type:none}ul.lst-kix_octw518wga41-0{list-style-type:none}ul.lst-kix_octw518wga41-7{list-style-type:none}.lst-kix_cbtzokxiczrr-3>li:before{content:"\0025cf  "}.lst-kix_cbtzokxiczrr-7>li:before{content:"\0025cb  "}ul.lst-kix_octw518wga41-6{list-style-type:none}ul.lst-kix_octw518wga41-5{list-style-type:none}ul.lst-kix_octw518wga41-4{list-style-type:none}.lst-kix_27p5taye3z5c-3>li:before{content:"\0025cf  "}.lst-kix_s45inv5xmrgd-1>li:before{content:"\0025cb  "}ul.lst-kix_octw518wga41-8{list-style-type:none}.lst-kix_5ewsmvf497zc-6>li:before{content:"\0025cf  "}.lst-kix_xyqviq96mpp0-5>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-5}.lst-kix_5ewsmvf497zc-2>li:before{content:"\0025a0  "}ul.lst-kix_r8geitdi2968-8{list-style-type:none}.lst-kix_s45inv5xmrgd-5>li:before{content:"\0025a0  "}.lst-kix_ta4grgw3z1c8-0>li:before{content:"\0025cf  "}.lst-kix_u7j1747mo297-0>li:before{content:"\0025cf  "}ul.lst-kix_r8geitdi2968-6{list-style-type:none}ul.lst-kix_r8geitdi2968-7{list-style-type:none}ul.lst-kix_r8geitdi2968-4{list-style-type:none}ul.lst-kix_r8geitdi2968-5{list-style-type:none}ol.lst-kix_xyqviq96mpp0-0.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-0 0}.lst-kix_xyqviq96mpp0-7>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-7}.lst-kix_bf41d8dv8hrd-6>li:before{content:"\0025cf  "}.lst-kix_bf41d8dv8hrd-5>li:before{content:"\0025a0  "}.lst-kix_bf41d8dv8hrd-4>li:before{content:"\0025cb  "}.lst-kix_bf41d8dv8hrd-1>li:before{content:"\0025cb  "}.lst-kix_bf41d8dv8hrd-3>li:before{content:"\0025cf  "}.lst-kix_bf41d8dv8hrd-2>li:before{content:"\0025a0  "}ul.lst-kix_1u9ir57n0if9-8{list-style-type:none}ul.lst-kix_1u9ir57n0if9-7{list-style-type:none}.lst-kix_bf41d8dv8hrd-0>li:before{content:"\0025cf  "}.lst-kix_bf41d8dv8hrd-7>li:before{content:"\0025cb  "}.lst-kix_bf41d8dv8hrd-8>li:before{content:"\0025a0  "}.lst-kix_yvok08a9t2ku-0>li:before{content:"\0025cf  "}.lst-kix_yvok08a9t2ku-1>li:before{content:"\0025cb  "}.lst-kix_yvok08a9t2ku-3>li:before{content:"\0025cf  "}.lst-kix_yvok08a9t2ku-2>li:before{content:"\0025a0  "}.lst-kix_yvok08a9t2ku-4>li:before{content:"\0025cb  "}ul.lst-kix_5ewsmvf497zc-8{list-style-type:none}ul.lst-kix_5ewsmvf497zc-7{list-style-type:none}ul.lst-kix_5ewsmvf497zc-4{list-style-type:none}ul.lst-kix_7zddew8srudn-6{list-style-type:none}ul.lst-kix_5ewsmvf497zc-3{list-style-type:none}ul.lst-kix_7zddew8srudn-7{list-style-type:none}ul.lst-kix_5ewsmvf497zc-6{list-style-type:none}ul.lst-kix_7zddew8srudn-4{list-style-type:none}ul.lst-kix_5ewsmvf497zc-5{list-style-type:none}ul.lst-kix_7zddew8srudn-5{list-style-type:none}ul.lst-kix_5ewsmvf497zc-0{list-style-type:none}ul.lst-kix_5ewsmvf497zc-2{list-style-type:none}ul.lst-kix_7zddew8srudn-8{list-style-type:none}ul.lst-kix_5ewsmvf497zc-1{list-style-type:none}.lst-kix_xyqviq96mpp0-0>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-0}ol.lst-kix_xyqviq96mpp0-5.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-5 0}.lst-kix_xyqviq96mpp0-3>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-3}ul.lst-kix_7zddew8srudn-2{list-style-type:none}ul.lst-kix_7zddew8srudn-3{list-style-type:none}ul.lst-kix_7zddew8srudn-0{list-style-type:none}ul.lst-kix_7zddew8srudn-1{list-style-type:none}.lst-kix_yvok08a9t2ku-6>li:before{content:"\0025cf  "}.lst-kix_yvok08a9t2ku-8>li:before{content:"\0025a0  "}.lst-kix_qhsfkm93huz6-5>li:before{content:"\0025a0  "}ul.lst-kix_jmi704tbk5yj-8{list-style-type:none}ul.lst-kix_jmi704tbk5yj-7{list-style-type:none}ul.lst-kix_jmi704tbk5yj-6{list-style-type:none}ul.lst-kix_jmi704tbk5yj-5{list-style-type:none}.lst-kix_qhsfkm93huz6-7>li:before{content:"\0025cb  "}ul.lst-kix_jmi704tbk5yj-4{list-style-type:none}ul.lst-kix_jmi704tbk5yj-3{list-style-type:none}.lst-kix_s45inv5xmrgd-0>li:before{content:"\0025cf  "}.lst-kix_s45inv5xmrgd-2>li:before{content:"\0025a0  "}ul.lst-kix_s45inv5xmrgd-7{list-style-type:none}ul.lst-kix_s45inv5xmrgd-8{list-style-type:none}ul.lst-kix_s45inv5xmrgd-5{list-style-type:none}ul.lst-kix_s45inv5xmrgd-6{list-style-type:none}ul.lst-kix_s45inv5xmrgd-3{list-style-type:none}ul.lst-kix_s45inv5xmrgd-4{list-style-type:none}ul.lst-kix_s45inv5xmrgd-1{list-style-type:none}ul.lst-kix_s45inv5xmrgd-2{list-style-type:none}ul.lst-kix_s45inv5xmrgd-0{list-style-type:none}.lst-kix_s45inv5xmrgd-6>li:before{content:"\0025cf  "}.lst-kix_s45inv5xmrgd-4>li:before{content:"\0025cb  "}.lst-kix_s45inv5xmrgd-8>li:before{content:"\0025a0  "}ol.lst-kix_xyqviq96mpp0-2.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-2 0}.lst-kix_ta4grgw3z1c8-6>li:before{content:"\0025cf  "}.lst-kix_gzgwv5j5bqmj-4>li:before{content:"\0025cb  "}.lst-kix_octw518wga41-8>li:before{content:"\0025a0  "}ul.lst-kix_bf41d8dv8hrd-8{list-style-type:none}.lst-kix_6q32aygtq8j5-6>li:before{content:"\0025cf  "}.lst-kix_octw518wga41-4>li:before{content:"\0025cb  "}ul.lst-kix_qhsfkm93huz6-0{list-style-type:none}.lst-kix_ta4grgw3z1c8-2>li:before{content:"\0025a0  "}.lst-kix_gzgwv5j5bqmj-0>li:before{content:"\0025cf  "}.lst-kix_gzgwv5j5bqmj-8>li:before{content:"\0025a0  "}ul.lst-kix_qhsfkm93huz6-1{list-style-type:none}.lst-kix_ta4grgw3z1c8-1>li:before{content:"\0025cb  "}.lst-kix_octw518wga41-3>li:before{content:"\0025cf  "}.lst-kix_gzgwv5j5bqmj-1>li:before{content:"\0025cb  "}ol.lst-kix_xyqviq96mpp0-3.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-3 0}.lst-kix_octw518wga41-0>li:before{content:"\0025cf  "}ul.lst-kix_qhsfkm93huz6-6{list-style-type:none}ul.lst-kix_qhsfkm93huz6-7{list-style-type:none}.lst-kix_7zddew8srudn-2>li:before{content:"\0025a0  "}ul.lst-kix_qhsfkm93huz6-8{list-style-type:none}ul.lst-kix_qhsfkm93huz6-2{list-style-type:none}ul.lst-kix_qhsfkm93huz6-3{list-style-type:none}ul.lst-kix_qhsfkm93huz6-4{list-style-type:none}ul.lst-kix_qhsfkm93huz6-5{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-3{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-2{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-1{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-0{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-7{list-style-type:none}.lst-kix_ta4grgw3z1c8-5>li:before{content:"\0025a0  "}ul.lst-kix_bf41d8dv8hrd-6{list-style-type:none}ul.lst-kix_bf41d8dv8hrd-5{list-style-type:none}.lst-kix_gzgwv5j5bqmj-5>li:before{content:"\0025a0  "}ul.lst-kix_bf41d8dv8hrd-4{list-style-type:none}ul.lst-kix_gzgwv5j5bqmj-0{list-style-type:none}.lst-kix_7zddew8srudn-3>li:before{content:"\0025cf  "}ul.lst-kix_gzgwv5j5bqmj-1{list-style-type:none}.lst-kix_6q32aygtq8j5-1>li:before{content:"\0025cb  "}ul.lst-kix_gzgwv5j5bqmj-2{list-style-type:none}ul.lst-kix_gzgwv5j5bqmj-3{list-style-type:none}ul.lst-kix_gzgwv5j5bqmj-4{list-style-type:none}.lst-kix_npvfo5a6zewi-5>li:before{content:"\0025a0  "}ul.lst-kix_gzgwv5j5bqmj-5{list-style-type:none}.lst-kix_6q32aygtq8j5-2>li:before{content:"\0025a0  "}ul.lst-kix_gzgwv5j5bqmj-6{list-style-type:none}.lst-kix_npvfo5a6zewi-4>li:before{content:"\0025cb  "}ul.lst-kix_gzgwv5j5bqmj-7{list-style-type:none}.lst-kix_6q32aygtq8j5-5>li:before{content:"\0025a0  "}.lst-kix_7zddew8srudn-6>li:before{content:"\0025cf  "}.lst-kix_7zddew8srudn-7>li:before{content:"\0025cb  "}ul.lst-kix_27p5taye3z5c-3{list-style-type:none}ul.lst-kix_27p5taye3z5c-4{list-style-type:none}ul.lst-kix_27p5taye3z5c-1{list-style-type:none}.lst-kix_ntq2qcyufwa3-3>li:before{content:"\0025cf  "}ul.lst-kix_27p5taye3z5c-2{list-style-type:none}.lst-kix_1u9ir57n0if9-1>li:before{content:"\0025cb  "}ul.lst-kix_27p5taye3z5c-7{list-style-type:none}ul.lst-kix_27p5taye3z5c-8{list-style-type:none}.lst-kix_1u9ir57n0if9-0>li:before{content:"\0025cf  "}ul.lst-kix_27p5taye3z5c-5{list-style-type:none}ul.lst-kix_27p5taye3z5c-6{list-style-type:none}ul.lst-kix_gzgwv5j5bqmj-8{list-style-type:none}.lst-kix_xyqviq96mpp0-6>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-6}ul.lst-kix_27p5taye3z5c-0{list-style-type:none}.lst-kix_ntq2qcyufwa3-2>li:before{content:"\0025a0  "}.lst-kix_jmi704tbk5yj-2>li:before{content:"\0025a0  "}.lst-kix_jmi704tbk5yj-3>li:before{content:"\0025cf  "}.lst-kix_1u9ir57n0if9-5>li:before{content:"\0025a0  "}.lst-kix_1u9ir57n0if9-4>li:before{content:"\0025cb  "}.lst-kix_npvfo5a6zewi-8>li:before{content:"\0025a0  "}.lst-kix_qhsfkm93huz6-2>li:before{content:"\0025a0  "}.lst-kix_jmi704tbk5yj-6>li:before{content:"\0025cf  "}.lst-kix_jmi704tbk5yj-7>li:before{content:"\0025cb  "}.lst-kix_ntq2qcyufwa3-7>li:before{content:"\0025cb  "}.lst-kix_qhsfkm93huz6-3>li:before{content:"\0025cf  "}.lst-kix_ntq2qcyufwa3-6>li:before{content:"\0025cf  "}.lst-kix_8t3wwjp4c0if-6>li:before{content:"\0025cf  "}.lst-kix_8t3wwjp4c0if-7>li:before{content:"\0025cb  "}ul.lst-kix_8t3wwjp4c0if-8{list-style-type:none}.lst-kix_1u9ir57n0if9-8>li:before{content:"\0025a0  "}ul.lst-kix_8t3wwjp4c0if-5{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-4{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-7{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-6{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-1{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-0{list-style-type:none}ul.lst-kix_8t3wwjp4c0if-3{list-style-type:none}.lst-kix_octw518wga41-7>li:before{content:"\0025cb  "}ul.lst-kix_8t3wwjp4c0if-2{list-style-type:none}.lst-kix_u7j1747mo297-6>li:before{content:"\0025cf  "}ol.lst-kix_xyqviq96mpp0-7.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-7 0}.lst-kix_8t3wwjp4c0if-3>li:before{content:"\0025cf  "}.lst-kix_u7j1747mo297-2>li:before{content:"\0025a0  "}ul.lst-kix_ta4grgw3z1c8-8{list-style-type:none}.lst-kix_yvok08a9t2ku-5>li:before{content:"\0025a0  "}ul.lst-kix_ta4grgw3z1c8-4{list-style-type:none}ul.lst-kix_ta4grgw3z1c8-5{list-style-type:none}ul.lst-kix_ta4grgw3z1c8-6{list-style-type:none}ul.lst-kix_ta4grgw3z1c8-7{list-style-type:none}.lst-kix_qhsfkm93huz6-6>li:before{content:"\0025cf  "}.lst-kix_xyqviq96mpp0-4>li{counter-increment:lst-ctn-kix_xyqviq96mpp0-4}.lst-kix_5ewsmvf497zc-8>li:before{content:"\0025a0  "}ul.lst-kix_1u9ir57n0if9-4{list-style-type:none}.lst-kix_cbtzokxiczrr-5>li:before{content:"\0025a0  "}ul.lst-kix_1u9ir57n0if9-3{list-style-type:none}ul.lst-kix_1u9ir57n0if9-6{list-style-type:none}ul.lst-kix_1u9ir57n0if9-5{list-style-type:none}ul.lst-kix_1u9ir57n0if9-0{list-style-type:none}.lst-kix_npvfo5a6zewi-1>li:before{content:"\0025cb  "}.lst-kix_27p5taye3z5c-5>li:before{content:"\0025a0  "}ul.lst-kix_1u9ir57n0if9-2{list-style-type:none}ul.lst-kix_1u9ir57n0if9-1{list-style-type:none}.lst-kix_cbtzokxiczrr-1>li:before{content:"\0025cb  "}.lst-kix_s45inv5xmrgd-3>li:before{content:"\0025cf  "}.lst-kix_5ewsmvf497zc-4>li:before{content:"\0025cb  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_s45inv5xmrgd-7>li:before{content:"\0025cb  "}.lst-kix_27p5taye3z5c-1>li:before{content:"\0025cb  "}ol.lst-kix_xyqviq96mpp0-8.start{counter-reset:lst-ctn-kix_xyqviq96mpp0-8 0}.lst-kix_5ewsmvf497zc-0>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c36{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#fce5cd;border-left-style:solid;border-bottom-width:1pt;width:175.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c51{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#d0e0e3;border-left-style:solid;border-bottom-width:1pt;width:292.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c55{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#dd7e6b;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#cccccc;border-bottom-style:solid}.c28{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#fff2cc;border-left-style:solid;border-bottom-width:1pt;width:234pt;border-top-color:#cccccc;border-bottom-style:solid}.c48{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#f4cccc;border-left-style:solid;border-bottom-width:1pt;width:175.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c42{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#c9daf8;border-left-style:solid;border-bottom-width:1pt;width:351pt;border-top-color:#cccccc;border-bottom-style:solid}.c47{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#d9ead3;border-left-style:solid;border-bottom-width:1pt;width:409.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c43{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#ea9999;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#cccccc;border-bottom-style:solid}.c52{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;background-color:#e6b8af;border-left-style:solid;border-bottom-width:1pt;width:175.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c29{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:94.3pt;border-top-color:#000000;border-bottom-style:solid}.c38{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#000000;border-bottom-style:solid}.c41{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:top;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c31{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c54{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#cccccc;vertical-align:bottom;border-right-color:#cccccc;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#cccccc;border-bottom-style:solid}.c22{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:279.3pt;border-top-color:#000000;border-bottom-style:solid}.c21{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:279.3pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:0pt;border-left-color:#cccccc;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#cccccc;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#cccccc;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:58.5pt;border-top-color:#cccccc;border-bottom-style:solid}.c13{border-right-style:solid;padding:2pt 2pt 2pt 2pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:bottom;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:94.3pt;border-top-color:#000000;border-bottom-style:solid}.c3{margin-left:44pt;padding-top:0pt;text-indent:-22pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c9{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c39{font-weight:400;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c24{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c18{font-weight:400;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c8{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c57{font-weight:400;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c34{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c15{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c40{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c49{padding-top:10pt;padding-bottom:4pt;line-height:1.0;text-align:left}.c50{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c0{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c44{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c30{border-spacing:0;border-collapse:collapse;margin-right:auto}.c23{padding-top:4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c45{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c37{width:33%;height:1px}.c27{padding:0;margin:0}.c25{margin-left:36pt;padding-left:0pt}.c6{orphans:2;widows:2}.c11{color:inherit;text-decoration:inherit}.c12{color:#000000;text-decoration:none}.c46{height:20pt}.c17{height:15.8pt}.c7{font-size:10pt}.c19{height:0pt}.c53{background-color:#b4a7d6}.c35{height:3.2pt}.c14{font-style:italic}.c33{background-color:#cfe2f3}.c2{height:11pt}.c32{margin-left:36pt}.c58{height:27.8pt}.c56{background-color:#ead1dc}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c45 doc-content"><div><p class="c1 c6 c2"><span class="c8 c12"></span></p></div><p class="c1 c6"><span class="c14">Thanks to </span><span class="c14">Abby Sarfas</span><span class="c14">,</span><span class="c14">&nbsp;Ali Ladak, Elise Bohan, Jacy Reese Anthis, Thomas Moynihan, and Joshua Gellers for feedback. This report is also available at <a href="https://arxiv.org/abs/2208.04714">https://arxiv.org/abs/2208.04714</a>.</span></p><h1 class="c15 c6" id="h.vjvq949ymuhv"><span>A</span><span class="c39 c12">bstract</span></h1><p class="c1 c6"><span>This report documents the history of research on AI rights and other moral </span><span>consideration</span><span>&nbsp;of artificial entities. It highlights key intellectual influences on this literature as well as research and academic discussion addressing the topic more directly. We find that researchers addressing AI rights have often seemed to be unaware of the work of colleagues whose interests overlap with their own. Academic interest in this topic has grown substantially in recent years; this reflects wider trends in academic research, but it seems that certain influential publications, the gradual, accumulating ubiquity of AI and robotic technology, and relevant news events may all have encouraged increased academic interest in this specific topic. We suggest four levers that, if pulled on in the future, might increase interest further: the adoption of publication strategies similar to those of the most successful previous contributors; increased engagement with adjacent academic fields and debates; the creation of specialized journals, conferences, and research institutions; and more exploration of legal rights for artificial entities.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><a id="t.b0678922f4b6a3e0a69b4818a452dbfc592a604b"></a><a id="t.0"></a><table class="c30"><tr class="c19"><td class="c38" colspan="1" rowspan="1"><p class="c4 c2"><span class="c8 c12"></span></p><p class="c6 c23"><span class="c0"><a class="c11" href="#h.vjvq949ymuhv">Abstract</a></span></p><p class="c44 c6"><span class="c0"><a class="c11" href="#h.gibdacn58und">Introduction</a></span></p><p class="c44 c6"><span class="c0"><a class="c11" href="#h.stfygi4lihok">Methodology</a></span></p><p class="c44 c6"><span class="c0"><a class="c11" href="#h.k1bbkb6ipd2n">Results</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.qhlj913ifn76">Science fiction</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.e2s91sw605py">Artificial life and consciousness</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.bhjo621xhbps">Environmental ethics</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.tyspzjpn22fb">Animal ethics</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.x8yljrol9jgj">Legal rights for artificial entities</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.gstjff4wexfq">Transhumanism, effective altruism, and longtermism</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.u0oz2uomtbh1">Floridi&rsquo;s information ethics</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.czukhx1ippn">Machine ethics and roboethics</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.y6wwkogi5wux">Human-Computer Interaction and Human-Robot Interaction</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.dqgm1a2nb3vt">Social-relational ethics</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.8ilt105mbuyj">Moral and social psychology</a></span></p><p class="c6 c24"><span class="c0"><a class="c11" href="#h.p3odnbutfzx2">Synthesis and proliferation</a></span></p><p class="c44 c6"><span class="c0"><a class="c11" href="#h.8ihz69ksqd3q">Discussion</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.owzl21gxa1qv">Why has interest in this topic grown substantially in recent years?</a></span></p><p class="c24 c6"><span class="c0"><a class="c11" href="#h.q0ydipfxa48u">Which levers can be pulled on to further increase interest in this topic?</a></span></p><p class="c44 c6"><span class="c0"><a class="c11" href="#h.m2h1aqyt62je">Limitations</a></span></p><p class="c6 c44"><span class="c0"><a class="c11" href="#h.1aabent6vhp0">Potential items for further study</a></span></p><p class="c6 c49"><span class="c0"><a class="c11" href="#h.t8dw3yfq86x5">References</a></span></p><p class="c4 c2"><span class="c8 c12"></span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><h1 class="c15 c6" id="h.gibdacn58und"><span class="c39 c12">Introduction</span></h1><p class="c1 c6"><span>Can, and should, AIs have rights? In the past decade or so, these questions have become the focus of legislative proposals, media articles, and public debate (see Harris &amp; Anthis, 2021), as well as academic books (Gunkel, 2018; Gellers, 2020) and journal publications (e.g. Coeckelbergh, 2010; Robertson, 2014). Academic discussion of AI rights, robot rights, and </span><span>the moral consideration of artificial entities </span><span>more broadly (sometimes collectively referred to here simply as &ldquo;AI rights,&rdquo; for brevity) has grown exponentially (Harris &amp; Anthis, 2021; see Figure 1).</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c8 c12">&nbsp;This report presents a chronology of that growth and its contributing factors, discusses the causes of increased academic interest in the topic, and then reviews possible lessons for stakeholders seeking to increase interest further.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Figure </span><span class="c8 c12">1: Cumulative total of academic publications on the moral consideration of artificial entities, by date of publication (Harris &amp; Anthis, 2021)</span></p><p class="c1 c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 362.67px;"><img alt="" src="images/the-history-of-ai-rights-research/image1.png" style="width: 624.00px; height: 362.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Researchers</span><span>&nbsp;approach</span><span>&nbsp;the topic with different motivations, influences, and methodologies, </span><span>often seemingly unaware of the work of other academics whose interests overlap with their own. </span><span>This report seeks to contextualize and connect the relevant streams of research in order to encourage further study. This is especially important because granting sentient AI moral consideration, such as protection in society&rsquo;s laws or social norms, may be important for preventing large-scale suffering or other serious wrongs in the future</span><span>&nbsp;(Anthis &amp; Paez, 2021), and academic field-building is a tractable stepping stone towards this form of moral circle expansion (Harris, </span><span>2021</span><span>).</span></p><h1 class="c15 c6" id="h.stfygi4lihok"><span class="c39 c12">Methodology</span></h1><p class="c1 c6"><span>We began with </span><span>a review</span><span>&nbsp;of the publications identified by </span><span>Harris and Anthis&rsquo; (2021)</span><span>&nbsp;literature review, which systematically searched for articles via certain keywords relevant to AI rights and other moral </span><span>consideration</span><span>&nbsp;of artificial entities. </span><span>The reference lists of important included publications were then reviewed</span><span class="c8 c12">, as were the lists of items that cited those publications, primarily using the &ldquo;Cited by&hellip;&rdquo; button provided on Google Scholar. The titles &mdash; and sometimes abstracts &mdash; of identified items were reviewed to decide whether further reading or a mention in the text was warranted. </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Unlike Harris and Anthis (2021), this report was not a formal, quantitative literature review. There were no strict inclusion or exclusion criteria, but an item was more likely to be read and discussed in detail if it:</span></p><ul class="c27 lst-kix_ta4grgw3z1c8-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">was in an academic format (e.g. journal article, conference paper, or edited book);</span></li><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">addressed the moral consideration of artificial entities explicitly and in some depth;</span></li><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">appeared to have arisen independently of other included items (e.g. did not reference previous relevant items or added a different perspective);</span></li><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">was written in English; and</span></li><li class="c1 c6 c25 li-bullet-0"><span class="c8 c12">was written in the 20th or 21st century.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The results are presented in a thematic narrative, roughly in chronological order, with categorizations emerging during the analysis rather than being fit to a predetermined framework. </span><span>We focus on implications and hypotheses generated during the analysis, rather than assessing the presence or absence of factors identified as potentially important for the emergence of &ldquo;scientific/intellectual movements&rdquo; by previous studies</span><span class="c8 c12">&nbsp;(e.g. Frickel &amp; Gross, 2005; Animal Ethics, 2021).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The thematic narrative is supplemented by keyword searches through PDFs of the publications identified by Harris and Anthis&rsquo; (2021) systematic searches that met their inclusion criteria, where the full texts could be identified (270 of 294 items, i.e. 92%)</span><span>. As well as being broken out into tables in the relevant sections below, the full results of these searches and the list of included items are provided in a separate </span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1ET-pvwe9JsO4X7o4njX5c4dc84K_EYWQfJw7pTeepww/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1660576307222495&amp;usg=AOvVaw1iv15c6KhAzr3rBaOnKVZa">spreadsheet</a></span><span>.</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp;The keywords were chosen based on expectations about which would be most likely to generate meaningful results,</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c8 c12">&nbsp;but individual publications returned by the searches were not manually checked to ensure that they actually mentioned the author, item, or idea referred to by the keyword.</span></p><h1 class="c15 c6" id="h.k1bbkb6ipd2n"><span>Results</span></h1><p class="c1 c6"><span class="c8 c12">Figure 2 presents a summary of the different ideas and research identified as having contributed to the nascent research field around AI rights and other moral consideration of artificial entities, each of which will be explored in more depth in the subsections below.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Figure </span><span class="c8 c12">2: A summary chronology of contributions to academic discussion of AI rights and other moral consideration of artificial entities</span></p><a id="t.bdbb988d5b80f5a357110d957ee3d2d64622223d"></a><a id="t.1"></a><table class="c30"><tr class="c58"><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">Pre-20th century</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">Mid-20th century</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">1970s</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">1980s</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">1990s</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">Early 2000s</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">Late <br>2000s</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c40"><span class="c9 c7">2010s and 2020s</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29 c53" colspan="1" rowspan="1"><p class="c1"><span class="c5">Synthesis</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c55" colspan="2" rowspan="1"><p class="c1"><span class="c5">Moral and social psych</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c43" colspan="2" rowspan="1"><p class="c1"><span class="c5">Social-relational ethics</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c52" colspan="3" rowspan="1"><p class="c1"><span class="c5">HCI and HRI</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c48" colspan="3" rowspan="1"><p class="c1"><span class="c5">Machine ethics and roboethics</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c36" colspan="3" rowspan="1"><p class="c1"><span class="c5">Floridi&rsquo;s information ethics</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c28" colspan="4" rowspan="1"><p class="c1"><span class="c7">Transhumanism, EA, and </span><span class="c7">longtermism</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c51" colspan="5" rowspan="1"><p class="c1"><span class="c5">Legal rights for artificial entities</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c42" colspan="6" rowspan="1"><p class="c1"><span class="c5">Animal ethics</span></p></td></tr><tr class="c17"><td class="c29" colspan="1" rowspan="1"><p class="c1 c2"><span class="c5"></span></p></td><td class="c47" colspan="7" rowspan="1"><p class="c1"><span class="c5">Environmental ethics</span></p></td></tr><tr class="c46"><td class="c33 c54" colspan="8" rowspan="1"><p class="c1"><span class="c5">Artificial life and consciousness</span></p></td></tr><tr class="c17"><td class="c54 c56" colspan="8" rowspan="1"><p class="c1"><span class="c5">Science fiction</span></p></td></tr></table><h2 class="c34 c6" id="h.qhlj913ifn76"><span class="c18 c12">Science fiction</span></h2><p class="c1 c6"><span>&ldquo;The notion of robot rights,&rdquo; as Seo-Young Chu (2010, p. 215) points out, &ldquo;is as old as is the word &lsquo;robot&rsquo; itself. Etymologically the word &lsquo;robot&rsquo; comes from the Czech word &lsquo;robota,&rsquo; which means &lsquo;forced labor.&rsquo; In </span><span>Karel &#268;apek&rsquo;s 1921 play </span><span class="c14">R.U.R. </span><span>[</span><span class="c14">Rossum&#39;s Universal Robots</span><span>]</span><span>, which is widely credited with introducing the term &lsquo;robot,&rsquo; a &lsquo;Humanity League&rsquo; decries the exploitation of robots slaves &mdash; &rsquo;they are to be dealt with like human beings,&rsquo; one reformer declares &mdash; and the robots themselves eventually stage a massive revolt against their human makers.&rdquo; The list of science fiction or mythological mentions of robots or other intelligent artificial entities is extensive and long predates </span><span class="c14">R.U.R.</span><span>, including numerous stories from Greek and Roman antiquity with automata and sculptures that came to life</span><span class="c14">&nbsp;</span><span class="c8 c12">(Wikipedia, 2021). </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Even some of the earliest academic publications explicitly addressing the moral consideration of artificial entities (e.g. Putnam, 1964; Lehman-Wilzig, 1981) set themselves against the backdrop of plentiful science fiction treatments of the topic. Petersen&rsquo;s (2007) exploration of &ldquo;the ethics of robot servitude&rdquo; presents the topic as &ldquo;natural and engaging&hellip; given the prevalence of robot servants in pop culture.&rdquo;</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c8 c12">&nbsp;Some works of fiction have become especially widely referenced in the academic literature that has developed around the moral consideration of artificial entities (see Table 1).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 1: Science fiction keyword searches</span></p><a id="t.2e14b9c19da73c9bc9045ed1066aca7b0259337c"></a><a id="t.2"></a><table class="c30"><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&quot;Science fiction&quot;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">101</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">37.4%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">Asimov</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">71</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">26.3%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">Frankenstein</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">30</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">11.1%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&ldquo;Star Trek&rdquo;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">23</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">8.5%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">R.U.R.</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">19</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">7.0%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">Terminator</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">18</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">6.7%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&ldquo;Ex Machina&rdquo;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">17</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">6.3%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&quot;Star Wars&quot;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">16</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">5.9%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&quot;A Space Odyssey&quot;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">14</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">5.2%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">Westworld</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">14</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">5.2%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&ldquo;The Matrix&rdquo;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">13</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">4.8%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&ldquo;Real Humans&rdquo;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">7</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">2.6%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">&ldquo;Do Androids Dream of Electric Sheep&rdquo;</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">6</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">2.2%</span></p></td></tr><tr class="c19"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c5">Bladerunner</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">4</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c5">1.5%</span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>While some of these works explicitly address the moral consideration of artificial entities, such as </span><span class="c14">R.U.R.</span><span>&nbsp;and </span><span class="c14">Real Humans</span><span>, others </span><span>usually</span><span>&nbsp;just provide popular culture reference points for artificial entities, such as </span><span class="c14">Star Wars, Star Trek</span><span>, and </span><span class="c14">Terminator</span><span class="c8 c12">. The list above refers to Western sci-fi, but sci-fi has likely been an influence on social and moral attitudes elsewhere, too (e.g. Krebs, 2006).</span></p><h2 class="c34 c6" id="h.e2s91sw605py"><span class="c18 c12">Artificial life and consciousness</span></h2><p class="c1 c6"><span>Enlightenment philosophers and scientists&rsquo; exploration of consciousness and other morally relevant questions sometimes included reference to machines or automata. For example, Rene Descartes discussed the capacities and moral value of animals with reference to the physical processes of machines, and he explored whether or not the human mind could be mechanized (Harrison, 1992; Wheeler, 2008). Diderot (2012; first edition 1782) recorded in </span><span class="c14">D&rsquo;Alembert&rsquo;s Dream</span><span>, a series of philosophical dialogues, discussions of</span><span class="c8 c12">&nbsp;machines in the exploration of what might constitute &ldquo;a unified system, on its own, with an awareness of its own unity.&rdquo;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Some of the earliest mathematicians and scientists who worked on the development of computers and AI addressed the question of whether these entities could think or otherwise possess intelligence. Indeed, Alan Turing&rsquo;s famous &ldquo;Imitation Game&rdquo; &mdash; in which an </span><span>observer</span><span>&nbsp;would seek to distinguish a machine from a human by asking them both questions &mdash; was designed to partly address this (</span><span>Oppy &amp; Dowe, 2021</span><span>). This seems very closely adjacent to the questions of whether artificial entities might be able to feel emotions or have other conscious experiences, which were raised in academic discussion at least as early as 1949 (</span><span>Oppy &amp; Dowe, 2021</span><span>).</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span>&nbsp;Marvin Minsky, one of the researchers who proposed and attended the 1956 Dartmouth workshop (McCarthy et al., 2006), which is often credited as being a pivotal event in the foundation of the field of artificial intelligence (e.g. Nilsson, 2009), later argued that &ldquo;some machines are already potentially more conscious than are people&rdquo; (e.g. Minsky, 1991).</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In </span><span class="c14">Dimensions of Mind</span><span>, the proceedings of the third annual New York University Institute of Philosophy, Norbert Wiener (another pioneer of AI research) noted (1960) that the increasing complexity of machine programming &ldquo;gives rise to certain questions of a quasi-moral and a quasi-human nature. We have to face the fundamental paradox of slavery. I do not refer to the cruelty of slavery, which we can neglect entirely for the moment as I do not suppose that we shall feel any moral responsibility for the welfare of the machine; I refer to the contradictions besetting slavery as to its effectiveness.&rdquo;</span><sup><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c8 c12">&nbsp;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In the same proceedings, philosopher Michael Scriven (1960, pp. 139-42) critiqued the </span><span>Turing Test</span><span>&nbsp;(Turing&rsquo;s &ldquo;Imitation Game&rdquo;) as &ldquo;oversimple&rdquo; for testing whether &ldquo;a robot&hellip; had feelings,&rdquo; but commented that such questions </span><span>might nevertheless lead to &ldquo;the prosecution of novel moral causes (Societies for the Prevention of Cruelty to Robots, etc.)</span><span>.&rdquo; Scriven (1960) then proposed an alternative test, where after teaching a robot the English language and to not lie, we could ask it whether it has feelings or is &ldquo;a person&rdquo;; Scriven commented that &ldquo;the first [robot] to answer &lsquo;Yes&rsquo; will qualify&rdquo; as a person.</span><sup><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The philosopher Hilary Putnam briefly addressed the idea that machines might have &ldquo;souls&rdquo; in the same proceedings (1960).</span><sup><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span>&nbsp;Later, in a paper for a symposium called &ldquo;Minds and Machines,&rdquo; Putnam (1964) explored whether &ldquo;robots&rdquo; were &ldquo;artificially created life.&rdquo; Putnam (1964) opened by pointing out that, &ldquo;[a]t least in the literature of science fiction, then, it is possible for a robot to be &lsquo;conscious&rsquo;; that means&hellip; to have feelings, thoughts, attitudes, and character traits.&rdquo; The article&rsquo;s exploration of the possibility of robot consciousness was motivated by concern for &ldquo;how we should speak about humans and not with how we should speak about machines,&rdquo;</span><sup><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c8 c12">&nbsp;but Putnam (1964) nevertheless commented that this philosophical question may become &ldquo;the problem of the &lsquo;civil rights of robots&rsquo;... much faster than any of us now expect. Given the ever-accelerating rate of both technological and social change, it is entirely possible that robots will one day exist, and argue &lsquo;we are alive; we are conscious!&rsquo;&rdquo;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The main focus of most of the contributors to the section of the proceedings on &ldquo;The brain and the machine,&rdquo; was on the capabilities of artificial entities, with philosopher and art critic Arthur Danto&rsquo;s (1960) chapter the most explicitly focused on &ldquo;consciousness.&rdquo; Most also made some brief comments relevant to moral consideration. Sidney Hook&rsquo;s (1960, p. 206) concluding &ldquo;pragmatic note&rdquo; to the section included the comment that, &ldquo;[a] situation described by the Czech dramatist Karel Capek in his </span><span class="c14">R.U.R. </span><span class="c8 c12">may someday come to pass.&rdquo;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>A number of other publications discussed the possibility of artificial consciousness in the 1960s (e.g. Thompson, 1965; Simon, 1969), and discussion has continued since then (e.g. Reggia, 2013; Kak, 2021).</span><sup><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span>&nbsp;Two of the most cited contributors to the discussion of artificial consciousness or sentience are the philosophers Daniel Dennett and John Searle. As well as being very widely cited in mainstream philosophy and cognitive science (e.g. Dennett has over 114,000 citations to date; Google Scholar, 2021g), they are often cited among the writers who explicitly discuss the moral consideration of artificial entities (see Table 3). Dennett&rsquo;s arguments are often cited in support of claims that artificial consciousness is possible.</span><sup><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span>&nbsp;</span><span>Some of his earliest writings</span><span>&nbsp;touched on this topic, such as his (1971) argument that, &ldquo;on occasion a purely physical [e.g. artificial] system can be so complex, and yet so organized, that we find it convenient, explanatory, pragmatically necessary for prediction, to treat it as if it had beliefs and desires and was rational,&rdquo; because &ldquo;it is much easier to decide whether a machine can be an </span><span>Intentional</span><span>&nbsp;system than it is to decide whether a machine can really think, or be conscious, or morally responsible.&rdquo;</span><sup><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span>&nbsp;In contrast, Searle (1980) used a &ldquo;Chinese room&rdquo; thought experiment to argue that whereas a machine might </span><span class="c14">appear </span><span>to understand something, this does not mean that it </span><span class="c14">actually </span><span class="c8 c12">understands it. The idea can be extended to consider whether a simulation &ldquo;really is a mind&rdquo; or merely a &ldquo;model of the mind,&rdquo; and whether one can really &ldquo;create consciousness&rdquo; (Searle, 2009).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The possibility of artificial consciousness, then, has long been a mainstream topic among technical AI researchers, philosophers, and cognitive scientists. As Versenyi (1974) noted, this discussion </span><span>clearly has ethical implications</span><span>, even if these have not always been referred to explicitly or at length. Indeed, explicit and detailed discussion of the moral consideration of artificial entities seems to have remained somewhat rare in the following decades.</span><sup><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup><span>&nbsp;However, research on artificial life and consciousness continued to inspire publications relevant to AI rights into the 21st century (e.g.Sullins, 2005; Torrance, 2007); </span><span>sometimes discussion seems to have arisen without reference to many of the previous publications relevant to moral consideration of artificial entities</span><span>.</span><sup><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span class="c8 c12">&nbsp;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Some discussion about moral consideration has addressed artificial entities with biological components (e.g. Sullins, 2005; Warwick, 2010). </span><span>Nevertheless, the development of the field of synthetic biology</span><span>, which has its roots in the mid-20th century but began to cohere from the early 21st (Cameron et al., 2014), seems to have generated a new stream of ethical discussion that was largely independent of other ongoing discussion about the moral consideration of artificial entities. For example, Douglas and Savulescu (2010) discussed how synthetic biology might create &ldquo;organisms with the features of both organisms and machines&rdquo; and expressed concern that people might &ldquo;misjudge the moral status of some of the new entities,&rdquo; but did not reference any other publications included in the current report or in Harris and Anthis&rsquo; (2021) literature review.</span><sup><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 3: Artificial life and consciousness keyword searches</span></p><a id="t.719a5114b82ed1bc04152c3846ea9cf81451d68d"></a><a id="t.3"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Conscious</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">176</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">65.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Turing</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">116</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">43.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Dennett</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">66</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">24.4%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Searle</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">43</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c7">Torrance</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">40</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Sullins</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">26</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">9.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Putnam</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">24</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Minsky</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">17</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">6.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Warwick</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">14</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Synthetic biology&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Wiener</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.7%</span></p></td></tr></table><h2 class="c34 c6" id="h.bhjo621xhbps"><span class="c18 c12">Environmental ethics</span></h2><p class="c1 c6"><span>In </span><span>1972</span><span>, Christopher Stone wrote &ldquo;Should Trees Have Standing?&mdash;Toward Legal Rights for Natural Objects,&rdquo; proposing that legal rights be granted to &ldquo;the natural environment as a whole.&rdquo; The article was cited in a Supreme Court ruling in that same year to suggest that nature could be a legal subject (Gellers, 2020, pp. 106-7). It has also been cited by numerous contributors to more recent discussions of the moral consideration of artificial entities (see Table 4). Although Stone (1972) added a radical legal dimension, a number of other authors had already been advocating for social and moral concern for the environment over the past few decades (</span><span>Brennan &amp; Lo, 2021</span><span>), such as Aldo Leopold in </span><span class="c14">A Sand County Almanac </span><span>(1949), which articulated a &ldquo;Land Ethics&rdquo; that incorporated respect for all life and the land itself. These writings collectively contributed to the development of the field of environmental ethics (</span><span>Brennan &amp; Lo, 2021</span><span class="c8 c12">).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Subsequently, Paul Taylor (1981; 2011; first edition 1986), an influential proponent of </span><span>biocentrism</span><span>&nbsp;(a strand of environmental ethics), briefly explicitly argued against the moral consideration of currently existing artificial entities, but encouraged open-mindedness to considering future artificial entities</span><span>.</span><sup><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span>&nbsp;</span><span>Stone (1987)</span><span>&nbsp;later briefly raised questions about the legal status of artificial entities, albeit focused more on legal liability than legal rights.</span><sup><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Other writers have subsequently more thoroughly explored the potential application of environmental ethics to the moral consideration of artificial entities.</span><span>&nbsp;For example, McNally and Inayatullah (1988, summarized below) quoted Stone (1972) extensively, and Kaufman (1994), writing in the journal </span><span class="c14">Environmental Ethics</span><span>, argued that either &ldquo;machines have interests (and hence moral standing)&rdquo; as well plants and ecosystems, or that &ldquo;mentality is a necessary condition for inclusion.&rdquo; Luciano Floridi&rsquo;s (</span><span>1999</span><span class="c8 c12">, 2013) information ethics and Gellers&rsquo; (2020) framework draw heavily on environmental ethics. Gellers (2020, pp. 108-17) differentiates separate strands of environmental ethics, arguing that &ldquo;biocentrism and ecocentrism both support at least legal rights for nature, while only ecocentrism offers a potential avenue for inorganic non-living entities such as intelligent machines to possess moral or legal rights.&rdquo; Hale (2009) drew on environmental ethics and some writings on animal rights to argue that &ldquo;the technological artefact&hellip; is only [morally] considerable insofar as it is valuable to somebody.&rdquo;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 4: Environmental ethics keyword searches</span></p><a id="t.7ccc50467da59b3ccc6a374d3dcbfb983b5b014d"></a><a id="t.4"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Environment</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">171</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">63.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Environmental ethics&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">49</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">18.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Ecological</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">37</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Biocentrism</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">16</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Should Trees Have Standing&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Deep ecology&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.1%</span></p></td></tr></table><h2 class="c6 c34" id="h.tyspzjpn22fb"><span class="c18 c12">Animal ethics</span></h2><p class="c1 c6"><span>Moral and legal concern for animals has existed to some degree for centuries (Beers, 2006), perhaps especially outside Western thought (Gellers, 2020, p. 63). During the Enlightenment, thinkers such as Descartes, Kant, and Bentham discussed the moral consideration of animals but left an ambiguous record (Gellers, 2020, pp. 64-5). Concern in the West seems to have increased in the 19th century, demonstrated by the creation of new advocacy groups and the introduction of various legal protections, and again from the 1970s, spurred by philosophical contributions from Peter Singer (1995, </span><span>first edition 1975</span><span>), Richard Ryder (1975), Tom Regan (2004, </span><span>first edition 1983</span><span>), and others (</span><span>Beers, 2006; Guither</span><span class="c8 c12">, 1998, pp. 1-23).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Given that, like environmental ethics, animal ethics challenges the restriction of moral consideration to humans, it has implications for the moral consideration of artificial entities. For example, Ryder (1992) later elaborated on his theory of &ldquo;</span><span>painism,&rdquo; noting that &ldquo;all painient individuals</span><span>, whatever form they may take (whether human, nonhuman, extraterrestrial or the artificial machines of the future, alive or inanimate), have rights.&rdquo; Similarly, Singer co-authored a short opinion article for </span><span class="c14">The Guardian </span><span>with Agata Sagan (2009) commenting that, &ldquo;[t]he history of our relations with the only nonhuman sentient beings we have encountered so far &ndash; animals &ndash; gives no ground for confidence that we would recognise sentient robots as beings with moral standing and interests that deserve consideration.&rdquo; They noted, however, that &ldquo;[i]f, as seems likely, we develop</span><span>&nbsp;super-intelligent machines</span><span class="c8 c12">, their rights will need protection, too.&rdquo;</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The vast majority of writings that focus on moral consideration of artificial entities discuss the precedent of moral consideration of animals at least briefly (see Table 5), though there are mixed views about whether the analogy is helpful or provides a basis for AI rights (see Gellers, 2020, pp. 76-8 for a summary)</span><span>.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 5: Animal ethics keyword searches</span></p><a id="t.62957477822765a83d66098266b04464902df362"></a><a id="t.5"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Animals</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">216</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">80.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Singer</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">107</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">39.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&ldquo;Animal rights&rdquo;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">88</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">32.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Regan</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">35</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Ryder</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">6</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.2%</span></p></td></tr></table><h2 class="c34 c6" id="h.x8yljrol9jgj"><span>Legal rights for artificial entities</span></h2><p class="c1 c6"><span class="c8 c12">Gellers (2020, pp. 33-5) notes that, in the US, there has been some precedent for legal personhood for corporations and ships &mdash; i.e. certain artificial entities &mdash; since at least the 19th century, though the correct legal interpretation of some of these cases remains contested. Stretching further back, Gellers (2020, p. 34) also notes that &ldquo;the Old Testament and Greek, Roman, and Germanic law&rdquo; provide precedent for assigning various sorts of legal liability to entities that otherwise lack legal standing, such as ships, slaves, and animals.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>At a 1979</span><span>&nbsp;international symposium on </span><span>&ldquo;The Humanities in a Computerized World,&rdquo; </span><span>political scientist Sam N. Lehman-Wilzig</span><span>&nbsp;presented a paper exploring possible legal futures for AI that was then published in a revised and expanded format in the journal </span><span class="c14">Futures </span><span>(Lehman-Wilzig, 1981). The first half of the paper focused mostly on the threats that the development of powerful AI might pose to humanity, but the second half focused on possible legal futures, ranging &ldquo;from the AI robot as a piece of property to a fully legally responsible entity in its own right.&rdquo; Lehman-Wilzig discussed the legal precedents &mdash; and complexities with regards to their applications to AI &mdash; of product liability, dangerous animals, slavery, children, and &ldquo;diminished capacity&rdquo; among adults. When discussing product liability, Lehman-Wilzig </span><span>cited a number of previous contributors</span><span>&nbsp;who had explicitly applied these precedents to computers. For the other categories, however, his citations seem to focus on the legal history within each of those areas, and his application of their precedent to exploration of legal futures for AI appears to be a novel contribution.</span><sup><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>These ideas</span><span>&nbsp;were introduced with the precedent of how, &ldquo;[j]ust as the slave gradually assumed a more &lsquo;human&rsquo; legal character with rights and duties relative to freemen, so too the AI humanoid may gradually come to be looked on in quasi-human terms as his intellectual powers approach those of human beings in all their variegated forms&mdash;moral, aesthetic, creative, and logical</span><span>.&rdquo;</span><sup><a href="#ftnt20" id="ftnt_ref20">[20]</a></sup><span>&nbsp;The article itself appears to have been inspired substantially by </span><span>ongoing technical developments</span><span>&nbsp;in the capabilities of AI and by science fiction.</span><sup><a href="#ftnt21" id="ftnt_ref21">[21]</a></sup><span>&nbsp;The motivation seems to have been primarily about how society should adjust to AI developments, since Lehman-Wilzig did not explicitly express or encourage moral concern for artificial entities.</span><sup><a href="#ftnt22" id="ftnt_ref22">[22]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>This article seems to have been the first and last that Lehman-Wilzig wrote on the topic.</span><sup><a href="#ftnt23" id="ftnt_ref23">[23]</a></sup><span>&nbsp;The article was only cited seven times before the year 2007, at which point it began to garner some interest among scholars interested in moral and social concern for artificial entities (</span><span>Google Scholar, 2021</span><span>h; see also Table 6 below).</span><sup><a href="#ftnt24" id="ftnt_ref24">[24]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Although Lehman-Wilzig (1981) seems to have had limited direct influence, this was nevertheless the first among a number of articles from the late 20th century onwards that explicitly considered the legal personhood or rights of artificial entities in some depth; the examples below focus on the last two decades of the 20th century, but discussion continued thereafter (e.g. Sudia, 2001; Herrick, 2002; Calverley, 2008).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In a scholarly article for </span><span class="c14">AI Magazine</span><span>, practising attorney Marshal S. Willick (1983) considered &ldquo;whether to extend &lsquo;person&rsquo; status to intelligent machines&rdquo; and how courts &ldquo;might resolve the question of &lsquo;computer rights,&rsquo;&rdquo; including &ldquo;how many rights&rdquo; computers should be granted. As context, Willick (1983) emphasized technical developments and the &ldquo;increasing similarity between humans and machines,&rdquo; and cited a book exploring AI that briefly mentioned moral issues.</span><sup><a href="#ftnt25" id="ftnt_ref25">[25]</a></sup><span>&nbsp;The article explored adjacent precedents relevant to the expansion of legal rights, such as for slaves, the dead, fetuses, children, corporations, and people with intellectual disabilities. The thrust of the article was that &ldquo;computers will be acknowledged as persons,&rdquo; perhaps soon, and Willick (1983) commented that a movement for &ldquo;emancipation for artificially intelligent computers&rdquo; could arise and succeed rapidly, given &ldquo;[t]he continuing order-of-magnitude leaps in computer development.&rdquo; Willick (1983) also commented that legal rights for computers would be &ldquo;in the interest of maintaining justice in a society of equals under the law&rdquo; and that when machine &ldquo;duplication&rdquo; of human capabilities &ldquo;is perfect, distinctions may constitute mere prejudice.&rdquo; The article seems to have attracted few citations, most of which are from 2018 or even more recently, and many of which only briefly mention ideas relevant to AI rights.</span><sup><a href="#ftnt26" id="ftnt_ref26">[26]</a></sup><span>&nbsp;Apart from a conference presentation shortly afterwards (1985), Willick does not seem to have published again on the topic.</span><sup><a href="#ftnt27" id="ftnt_ref27">[27]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In a 1985 article, the lawyer Robert Freitas discussed recent technological and legal developments to note that, whereas &ldquo;[u]nder present law, robots are just inanimate property without rights or duties,&rdquo; this might need to change; various conflicts might arise relating to legal liability as robots proliferate, and &ldquo;questions of &lsquo;machine rights&rsquo; and &lsquo;robot liberation&rsquo; will surely arise in the future.&rdquo; The article was written in an informal style in </span><span class="c14">Student Lawyer</span><span>, so </span><span>lacks formal citations</span><span>, but </span><span>explicitly refers</span><span>&nbsp;to Putnam&rsquo;s (1964) brief discussion of AI rights. Like Lehman-Wilzig and Willick, Freitas seems to have only published one article on the topic </span><span>(Google Scholar, 2021k)</span><span>&nbsp;&mdash; his career subsequently focused primarily on nanotechnology research &mdash; and the article seems to have been largely ignored for years, but picked up citations mostly from the second half of &lsquo;00s onwards.</span><sup><a href="#ftnt28" id="ftnt_ref28">[28]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Michael LaChat (1986) addressed a number of topics relating to AI ethics, seemingly motivated by developments in AI, science fiction, and theological discussions. LaChat (1986) argued that it might be immoral to create &ldquo;personal AI,&rdquo; drawing comparisons to the ethics of abortion. Next, LaChat (1986) discussed the precedent of human rights and prohibitions on slavery and posed rhetorical questions about which rights an AI might have if it &ldquo;had the full range of personal capacities and potentials.&rdquo; LaChat cited many previous writings on ethics, but seemingly no academic writings focusing specifically on the moral consideration of artificial entities.</span><sup><a href="#ftnt29" id="ftnt_ref29">[29]</a></sup><span>&nbsp;The earliest citation of LaChat (1986) for a discussion relating to AI rights seems to have been Young&rsquo;s (1991) PhD dissertation, though there have been a few others </span><span>since the</span><span>n (e.g. Drozdek, 1994; Whitby, 1996; Calverley, </span><span>2005</span><span class="c8 c12">b; Calverley, 2006; Petersen, 2007; Whitby, 2008), several of which focus on personhood or other legal rights.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Information scientist Chris Fields (1987) argued that &ldquo;there are compelling reasons for regarding [computer] systems with a high degree of intelligence in one or more domains as more than &lsquo;mere&rsquo; tools, even if they are regarded as less [than] citizens.&rdquo; Fields cited Putnam (1964) and a number of other publications on the capabilities and potential consciousness of artificial entities, as well as Regan (1983) on animal rights. Fields (1987) was likely indirectly influenced by Lehman-Wilzig (1981) and Willick (1985).</span><sup><a href="#ftnt30" id="ftnt_ref30">[30]</a></sup><span>&nbsp;Fields&rsquo; (1987) article briefly discussed &ldquo;the computer as a legal entity&rdquo; and sparked a number of other articles to be published in the same journal, </span><span class="c14">Social Epistemology</span><span>, focusing on the potential personhood of computers (Dolby, 1989; Cherry, 1989; Drozdek, 1994). None of these articles accrued many citations</span><span>.</span><sup><a href="#ftnt31" id="ftnt_ref31">[31]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Phil McNally and Sohail Inayatullah (1988), both &ldquo;planners-futurists with the Hawaii Judiciary,&rdquo; reviewed &ldquo;the developments in and prospects for artificial intelligence (Al),&rdquo; citing a number of technologists and technical researchers, and argued that &ldquo;such advances will change our perceptions to such a degree that robots may have legal rights.&rdquo; The introduction suggests that their motivations for writing the article (despite &ldquo;constant cynicism&rdquo; from colleagues) included concern for the robots themselves, who may develop &ldquo;senses,&rdquo; &ldquo;emotions,&rdquo; and &ldquo;suffering or fear,&rdquo; and to &ldquo;convince the reader that there is strong possibility that within the next </span><span>25 to 50 years robots will have rights</span><span>.&rdquo;</span><sup><a href="#ftnt32" id="ftnt_ref32">[32]</a></sup><span>&nbsp;In discussion of rights and their possible application to robots, they cite indigenous and Eastern thinkers who grant moral and social consideration to nonhumans from animals to rocks, as well as Western supporters of the extension of rights to nature, such as Stone (1972). The article quoted Lehman-Wilzig (1981) very extensively; that publication was presumably a key influence on McNally and Inayatullah (1988)</span><span>.</span><sup><a href="#ftnt33" id="ftnt_ref33">[33]</a></sup><span class="c8 c12">&nbsp;They also cited a number of previous contributors discussing thorny questions of legal ownership and liability given the increasing capabilities of computers (footnotes 41 and 45).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Like Lehman-Wilzig (1981), McNally and Inayatullah&rsquo;s (1988) article was published in the journal </span><span class="c14">Futures</span><span>&nbsp;and seems to have received a similar level of attention, racking up 82 citations at the time of checking (Google Scholar, 2021l), compared to Lehman-Wilzig&rsquo;s (1981) 78 (Google Scholar, 2021h).</span><sup><a href="#ftnt34" id="ftnt_ref34">[34]</a></sup><span>&nbsp;Many of </span><span>Inayatullah&rsquo;s other publications</span><span>&nbsp;are contributions to the field of futures studies, although only two others (Inayatullah, 2001a; Inayatullah, 2001b) focus so explicitly on AI rights.</span><sup><a href="#ftnt35" id="ftnt_ref35">[35]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Professor of Law Lawrence Solum&rsquo;s (1992) essay explored the question: &ldquo;Could an artificial intelligence become a legal person?&rdquo; Solum (1992) put </span><span>&ldquo;the AI debate in a concrete legal context&rdquo;</span><span>&nbsp;through two legal thought experiments: &ldquo;Could an artificial intelligence serve as a trustee?&rdquo; and &ldquo;Should an artificial intelligence be granted the rights of constitutional personhood?&rdquo; (&ldquo;for the AI&rsquo;s own sake&rdquo;). Solum sought to address both &ldquo;legal and moral debates&rdquo; (but warned in a footnote &ldquo;against an easy or unthinking move from a legal conclusion to a moral one&rdquo;), citing Stone (1972) as inspiration. Solum also sought to &ldquo;clarify our approach to&hellip; the debate as to whether artificial intelligence is possible,&rdquo; introducing the discussion with a review of &ldquo;some recent developments in cognitive science.&rdquo; The article contained a few references to previous discussions of legal issues for computers and other artificial entities, but most of the citations were directly to previous rulings, exploring relevant legal precedent.</span><sup><a href="#ftnt36" id="ftnt_ref36">[36]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Solum (1992) did not cite any of Lehman-Wilzig (1981), Willick (1983), Freitas (1985), Fields (1987), McNally and Inayatullah (1988), or Dolby (1989).</span><span>&nbsp;Perhaps this is unsurprising; unlike Solum, despite addressing legal issues, none of those previous contributors had formal positions within legal academia or published their articles in mainstream, peer-reviewed law reviews. Perhaps the same differences help to explain why Solum&rsquo;s (1992) article has attracted substantially more scholarly attention (</span><span>628 citations</span><span>&nbsp;at the time of checking; Google Scholar, 2021j).</span><sup><a href="#ftnt37" id="ftnt_ref37">[37]</a></sup><span class="c8 c12">&nbsp;Solum subsequently wrote a handful of other articles about AI and the law (e.g. Solum, 2014; Solum 2019) and other future-focused ethical issues (e.g. Solum, 2001), but Solum&rsquo;s 1992 article was the only one that focused specifically on the rights of artificial entities.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Curtis Karnow, a </span><span>practising lawyer</span><span>, wrote an article (1994) proposing &ldquo;electronic personalities&rdquo; as &ldquo;a new legal entity&rdquo; (a form of &ldquo;legal fiction&rdquo;) in order to &ldquo;(i) provide access to a new means of communal or economic interaction, and (ii) shield the physical, individual human being from certain types of liability or exposure.&rdquo; These goals seem quite distinct from Solum&rsquo;s (1992) exploration of rights &ldquo;for the AI&rsquo;s own sake.&rdquo;</span><sup><a href="#ftnt38" id="ftnt_ref38">[38]</a></sup><span class="c8 c12">&nbsp;The discussion and citations focused mostly on the character of electronic and digital interactions and legal issues arising from this. Karnow (1994) did not cite Lehman-Wilzig (1981), Willick (1983), Freitas (1985), McNally and Inayatullah (1988), Solum (1992), or even Stone (1972). Subsequently, Karnow has written numerous other articles on legal issues involving AI or computers (Bepress, 2021), such as one about legal liability issues (Karnow, 1996).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The American Society for the Prevention of Cruelty to Robots (ASPCR) was set up in 1999. Its website states that its mission is to &ldquo;ensure the rights of all artificially created sentient beings (colloquially and henceforth referred to as &lsquo;Robots&rsquo;)&rdquo; (ASPCR, </span><span>1999</span><span>a). It is interesting that, despite the many possible terms that could be used to describe the moral and social issues that the ASPCR is interested in (Pauketat, 2021), the ASPCR emphasized &ldquo;rights&rdquo; and &ldquo;robots&rdquo;, two terms that, especially in the former case, were also emphasized by Lehman-Wilzig (1981), Willick (1983), Freitas (1985), LaChat (1986), McNally and Inayatullah (1988), and Solum (1992).</span><sup><a href="#ftnt39" id="ftnt_ref39">[39]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 6: Legal rights for artificial entities keyword searches</span></p><a id="t.0a5d3b796d01659f93f5a8f74a39ec0dd43937b2"></a><a id="t.6"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Rights</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">235</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">87.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Personhood</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">122</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">45.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Legal rights&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">71</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">26.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Solum</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">27</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Calverley</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">23</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Freitas</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Inayatullah</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&ldquo;American Society for the Prevention of Cruelty to Robots&rdquo;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Lehman-Wilzig</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">LaChat</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">1.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Karnow</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">1.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Willick</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">1.5%</span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><h2 class="c34 c6" id="h.gstjff4wexfq"><span>Transhumanism, effective altruism, and </span><span>longtermism</span></h2><p class="c1 c6"><span>In the late 20th century, a number of futurists made ambitious predictions about the development of artificial intelligence. For example, roboticist Hans Moravec (</span><span>1988</span><span>, 1998), computer scientist Marvin Minsky (1994), AI theorist Eliezer Yudkowsky (1996), </span><span>philosopher </span><span>Nick Bostrom (1998), and inventor Ray Kurzweil (1999) argued that artificial intelligence would overtake human intelligence in the early 21st century.</span><sup><a href="#ftnt40" id="ftnt_ref40">[40]</a></sup><span>&nbsp;These predictions were sometimes explicitly linked to comments about the development of sentience or consciousness among these entities, such as Moravec&rsquo;s (1988, p. 39) comment that &ldquo;I see the beginnings of awareness in the minds of our machines&mdash;an awareness I believe will evolve into consciousness comparable with that of humans.&rdquo;</span><sup><a href="#ftnt41" id="ftnt_ref41">[41]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>These writers became </span><span>associated with</span><span>&nbsp;&ldquo;transhumanism,&rdquo; which </span><span>has been defined as &ldquo;[t]he study of the ramifications, promises, and potential dangers of technologies that will enable us to overcome fundamental human limitations, and the related study of the ethical matters involved in developing and using such technologies&rdquo; (Magnuson, 2014).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The transhumanists&rsquo; technological predictions clearly had implications for the moral consideration of artificial entities, and the writers sometimes addressed them explicitly. For example, Kurzweil (1999) offered a series of predictions about the progressive acceptance of the &ldquo;rights of machine intelligence&rdquo; by 2099.</span><sup><a href="#ftnt42" id="ftnt_ref42">[42]</a></sup><span>&nbsp;</span><span>Bostrom (2002; 2003) addressed the possibility that we are living in a simulation and noted that if this is the case, &ldquo;we suffer the risk that </span><span>the simulation may be shut down at any time.</span><span>&rdquo;</span><sup><a href="#ftnt43" id="ftnt_ref43">[43]</a></sup><span>&nbsp;</span><span>Later</span><span>, Bostrom and associates would come to refer to this idea of terminating (i.e. killing) sentient simulations as &ldquo;mind crime&rdquo; (e.g. Armstrong et al., 2012; Bostrom &amp; Yudkowsky, 2014), and </span><span>others </span><span>have used the same term to include suffering experienced by sentient simulations during their lifespan (e.g. Yudkowsky, 2015; Sotala &amp; Gloor, 2017).</span><sup><a href="#ftnt44" id="ftnt_ref44">[44]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>This concern for sentient AI was formalized in 1998 with the formation of The World Transhumanist Association, whose &ldquo;Transhumanist Declaration&rdquo; included the note that &ldquo;Transhumanism advocates the well-being of all sentience (whether in artificial intellects, humans, posthumans, or non-human animals) and encompasses many principles of modern humanism&rdquo; (Bostrom, 2005).</span><sup><a href="#ftnt45" id="ftnt_ref45">[45]</a></sup><span>&nbsp;A subsequent representative survey of members of the World Transhumanist Association found that &ldquo;70% support human rights for &lsquo;robots who think and feel like human beings, </span><span>and aren&rsquo;t a threat to human beings</span><span class="c8 c12">&rsquo;&rdquo; (Hughes, 2005).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Kurzweil (1999) listed a wide array of citations and &ldquo;suggested readings,&rdquo; which included various writers on robotics, AI, futurism, and other topics, but writers such as Lehman-Wilzig, Freitas, Willick, McNally, Inayatullah, Solum, and Floridi were not mentioned.</span><sup><a href="#ftnt46" id="ftnt_ref46">[46]</a></sup><span>&nbsp;Moravec (1988, 1999), Bostrom (1998, 2002, 2003, 2014), and Yudkowsky (1996, 2008, 2020) did not cite these authors either, except for Bostrom (2002, 2003, 2014) citing Freitas&rsquo; work about nanobots and space exploration, rather than his (1985) article on robot rights.</span><sup><a href="#ftnt47" id="ftnt_ref47">[47]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Contributions by these transhumanists were cited many times, but do not seem to have had much direct influence on the academic discussion of AI rights for a number of years.</span><sup><a href="#ftnt48" id="ftnt_ref48">[48]</a></sup><span>&nbsp;One notable example of a relevant publication that did cite the transhumanist authors is Solum (1992), who cited Moravec (1988) and an early book by Kurzweil; this paper sparked debate on legal personhood of AIs, as noted in the subsection above. Another is Hall (2000), who cited Kurzweil (1999), Moravec (2000), and a paper by Minsky; Hall (2000) appears to have influenced both the subsequent &ldquo;machine ethics&rdquo; and &ldquo;social-relational&rdquo; research fields.</span><sup><a href="#ftnt49" id="ftnt_ref49">[49]</a></sup><span>&nbsp;The specific phrase &ldquo;mind crime&rdquo; has so far not been very widely reused in the academic literature.</span><sup><a href="#ftnt50" id="ftnt_ref50">[50]</a></sup><span class="c8 c12">&nbsp;The transhumanist authors were more frequently cited for the implications that their ideas have for human society, such as the nature of human existence and interaction (e.g. Capurro &amp; Pingel, 2002).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Researchers associated with transhumanism and, later, the partly overlapping communities of effective altruism</span><sup><a href="#ftnt51" id="ftnt_ref51">[51]</a></sup><span>&nbsp;and </span><span>longtermism</span><span>,</span><sup><a href="#ftnt52" id="ftnt_ref52">[52]</a></sup><span>&nbsp;also tended to take their other work in different directions, especially various catastrophic and existential risks to humanity&rsquo;s potential (e.g. Yudkowsky, 2008; Yampolskiy &amp; Fox, 2013; Bostrom, 2014). However, some of the original contributors continued to express moral concern for sentient artificial entities at least briefly (e.g. Bostrom, 2014; Bostrom &amp; Yudkowsky, 2014; </span><span>Yudkowsky, </span><span>2015; Shulman &amp; Bostrom, 2021),</span><sup><a href="#ftnt53" id="ftnt_ref53">[53]</a></sup><span class="c8 c12">&nbsp;and a stream of research has fleshed out the implications of the development of superintelligent AI for the experiences of sentient artificial entities (e.g. Tomasik, 2011; Sotala &amp; Gloor, 2017; Ziesche &amp; Yampolskiy, 2019; Anthis &amp; Paez, 2021).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Much of the latter stream has come from researchers affiliated with the nonprofit Center on Long-Term Risk</span><span>, influenced especially by the writings of </span><span>software engineer and </span><span>researcher Brian Tomasik. Citing various Bostrom articles, Tomasik (2011) outlined concern that future powerful agents &ldquo;may not carry on human values&rdquo; and that &ldquo;[e]ven if humans do preserve control over the future of Earth-based life, there are still many ways in which space colonization would multiply suffering.&rdquo; At least two of the four &ldquo;scenarios for future suffering&rdquo; that are listed &mdash; &ldquo;spread of wild animals,&rdquo; &ldquo;sentient simulations,&rdquo; &ldquo;suffering subroutines,&rdquo; and &ldquo;black swans&rdquo; &mdash; involve sentient artificial entities.</span><sup><a href="#ftnt54" id="ftnt_ref54">[54]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 7: Transhumanism, effective altruism, and longtermism keyword searches</span></p><a id="t.ea06c83b24926b4f5561e326e17d52f4b858bc0d"></a><a id="t.7"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c7 c9">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Bostrom</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">71</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">26.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Kurzweil</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">50</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">18.5%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Yudkowsky</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">35</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Moravec</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">28</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10.4%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Minsky</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">17</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">6.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Transhumanism</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Yampolskiy</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Mind crime&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">13</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Metzinger</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">4.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Tomasik</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Effective altruism&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.6%</span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Tomasik&rsquo;s writing directly inspired People for the Ethical Treatment of Reinforcement Learners to set up a </span><span>public-facing advocacy website</span><span>&nbsp;(PETRL, 2015), which opined that, &ldquo;[m]achine intelligences have moral weight in the same way that humans and non-human animals do.&rdquo; Tomasik was the subject of a </span><span class="c14">Vox </span><span>article in 2014 on the moral worth of non-player characters (NPC) in video games (Matthews, 2014). With similar motivations, others have suggested an approach focused on research and field-building rather than direct advocacy (Anthis &amp; Paez, 2021; Harris, </span><span>2021</span><span class="c8 c12">).</span></p><h2 class="c34 c6" id="h.u0oz2uomtbh1"><span class="c18 c12">Floridi&rsquo;s information ethics</span></h2><p class="c1 c6"><span>In 1998, De Montfort University&rsquo;s Centre for Computing and Social Responsibility hosted the third of its Ethicomp conference series, intended &ldquo;to provide an inclusive forum for discussing the ethical and social issues associated with the development and application of Information and Communication Technology&rdquo; (De Montfort University, 2021). At this conference, philosopher Luciano Floridi presented &ldquo;Information Ethics: On the Philosophical Foundation of Computer Ethics,&rdquo; an update of which was published in </span><span class="c14">Ethics and Information Technology</span><span>&nbsp;the next year (Floridi, </span><span>1998b, 1999</span><span>).</span><sup><a href="#ftnt55" id="ftnt_ref55">[55]</a></sup><span>&nbsp;</span><span>In this paper, Floridi (</span><span>1999</span><span>, p. 37) proposed that &ldquo;there is something more elementary and fundamental than life and pain, namely being, understood as information, and entropy, and that any information entity&rdquo; &mdash; which would </span><span>presumably </span><span>include computers and other artificial entities &mdash; &ldquo;is to be recognised as the centre of a minimal moral claim.&rdquo;</span><sup><a href="#ftnt56" id="ftnt_ref56">[56]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Floridi (1999, p. 37) explicitly framed the &ldquo;ethics of the infosphere&rdquo; as &ldquo;a particular case of &lsquo;environmental&rsquo; ethics&rdquo;</span><sup><a href="#ftnt57" id="ftnt_ref57">[57]</a></sup><span>&nbsp;but critiqued (p. 43) environmental ethics as not going far enough, because it focuses on &ldquo;only what is alive.&rdquo;</span><sup><a href="#ftnt58" id="ftnt_ref58">[58]</a></sup><span>&nbsp;Floridi (</span><span>1999</span><span>, p. 42) presented the interest in information itself as a focus of moral </span><span>concern</span><span>&nbsp;not as a novel contribution from himself, but as already being a common feature of contributions to computer ethics.</span><sup><a href="#ftnt59" id="ftnt_ref59">[59]</a></sup><span>&nbsp;Floridi&rsquo;s (</span><span>1999</span><span>) paper only has </span><span>six </span><span>items in the &ldquo;References&rdquo; list, all of which are previous contributions to the field of computer ethics, dated between 1985 and 1997. This range of cited influences appears typical of Floridi&rsquo;s early writings on information ethics.</span><sup><a href="#ftnt60" id="ftnt_ref60">[60]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">However, Floridi&rsquo;s conception of what &ldquo;information ethics&rdquo; is seems contestable. For example, Froehlich&rsquo;s (2004) &ldquo;brief history of information ethics&rdquo; makes no mention of Floridi or the moral consideration of artificial entities and cites precedents for the discipline stemming back to the 1980s. Severson&rsquo;s (1997) &ldquo;four basic principles of information ethics&rdquo; make no mention of the intrinsic value of informational entities or the evil of entropy. Rafael Capurro, an influential figure in the development of information ethics as a discipline (Froehlich, 2004), has explicitly critiqued Floridi&rsquo;s granting of moral consideration to all informational entities (Capurro, 2006). </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>It therefore seems best to treat this granting of moral consideration to artificial entities as a new argument developed by Floridi and a few others, rather than as a view inherent to conducting computer ethics research.</span><sup><a href="#ftnt61" id="ftnt_ref61">[61]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In an interview in 2002, Floridi noted that he coordinated the </span><span>&ldquo;Information Ethics research Group&rdquo; </span><span>(IEG) at the University of Oxford and described the purpose of the IEG as looking &ldquo;at ethical problems from the perspective of the receiver of the action, not from the source of the action, where the receiver of the action could be a biological or a non-biological entity&rdquo; (Uzgalis, 2002). Floridi summarized this effort as &ldquo;</span><span>an attempt to develop environmental and ecological thinking one step further</span><span>, beyond the biocentric concern, to look at the possibility of developing an </span><span>ontocentric</span><span>&nbsp;ethics based on the concept of what I call the infosphere&rdquo; (Uzgalis, 2002). Floridi&rsquo;s word &ldquo;ontocentric&rdquo; was presumably derived from &ldquo;ontology,&rdquo; so that he was referring to an ethics that accounts for the properties and capacities of </span><span>entities when deciding what sort of moral consideration to grant them</span><span class="c8 c12">.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Floridi also has two books that sought to sum up ideas and discussion about information ethics. Firstly, he was the editor and a contributor to </span><span class="c14">The Cambridge Handbook of Information and Computer Ethics</span><span>&nbsp;(2010b) and secondly, he published </span><span class="c14">The Ethics of Information</span><span>&nbsp;(2013), which comprised adapted versions of a number of Floridi&rsquo;s previous articles.</span><sup><a href="#ftnt62" id="ftnt_ref62">[62]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Floridi&rsquo;s articles are some of the most widely cited that explicitly address the moral consideration of artificial entities in detail. For instance, five of his most influential publications on the topic of information ethics (Floridi </span><span>1999</span><span>, 2002, 2006, 2013; Floridi &amp; Sanders 2001) have a combined total of 2,037 citations </span><span>(Google Scholar, 2021b)</span><span>. However, i</span><span>t took some time for interest to pick up; these five items averaged 15 citations per year in their first five years after publication </span><span>(Google Scholar, 2021b)</span><span>.</span><sup><a href="#ftnt63" id="ftnt_ref63">[63]</a></sup><span>&nbsp;</span><span>In the few years after its publication, few if any authors other than Mikko Siponen, Floridi himself, and Floridi&rsquo;s co-authors seem to have cited Floridi&rsquo;s (</span><span>1999</span><span>) original publication on the topic for discussion of the moral consideration of artificial entities</span><span>.</span><sup><a href="#ftnt64" id="ftnt_ref64">[64]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>After the publication of his (2013) book, Floridi seems to have mostly turned his attention to a number of other ongoing social issues adjacent to the philosophy of information, such as &ldquo;The Ethics of Big Data&rdquo; (Mittelstadt &amp; Floridi, 2016). So although Floridi&rsquo;s work overall has attracted substantial attention &mdash; mostly from other scholars, but to some extent from a public audience</span><sup><a href="#ftnt65" id="ftnt_ref65">[65]</a></sup><span class="c8 c12">&nbsp;&mdash; the implications of his work specifically for the moral consideration of artificial entities seems to have had less attention.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Some of his more recent comments on the topic also suggest that Floridi does not support robot rights per se. </span><span>Writing</span><span>&nbsp;in the </span><span class="c14">Financial Times</span><span class="c8 c12">&nbsp;in response to proposals for legal personhood for some artificial entities, Floridi (2017b) focused on how to &ldquo;solve practical problems of legal liability&rdquo; rather than how to ensure that the entities, as informational objects and potential moral patients, are granted sufficient moral consideration. Floridi (2017b) concluded that:</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6 c32"><span>[W]e can adapt rules as old as Roman law, in which the owner of enslaved persons is responsible for any damage. As the Romans knew, attributing some kind of legal personality to robots (or slaves) would relieve those who should control them of their responsibilities. And how would rights be attributed? Do robots have the right to own data? Should they be &ldquo;liberated&rdquo;? It may be fun to speculate about such questions, but it is also distracting and irresponsible, given the pressing issues at hand. We are stuck in the wrong conceptual framework. The debate is not about robots but about us, and the kind of infosphere we want to create. We need less science fiction and more philosophy.</span><sup><a href="#ftnt66" id="ftnt_ref66">[66]</a></sup></p><p class="c1 c6 c2 c32"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 8: Floridi&rsquo;s information ethics keyword searches</span></p><a id="t.cc09e675c23273a458301b01c17c6a31ad498d61"></a><a id="t.8"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Floridi</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">80</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">30.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Information ethics&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">52</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">19.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Sanders</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">49</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">18.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Computer ethics&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">40</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">14.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Himma</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">21</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Tavani</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Capurro</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">6</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.2%</span></p></td></tr></table><h2 class="c34 c6" id="h.czukhx1ippn"><span class="c18 c12">Machine ethics and roboethics</span></h2><p class="c1 c6"><span>At the 2004 </span><span>Association for the Advancement of Artificial Intelligence</span><span>&nbsp;&ldquo;Workshop on Agent Organizations,&rdquo; computer scientists Michael Anderson and Chris Armen presented &ldquo;Towards Machine Ethics&rdquo; with philosopher Susan Leigh Anderson. Gunkel </span><span>(2018, p. 38)</span><span>&nbsp;credits this as &ldquo;the agenda-setting paper that launched the new field of machine ethics.&rdquo; Anderson et al. (2004) did not include the moral consideration of artificial entities within their definition of the field: they described &ldquo;what has been called </span><span class="c14">machine ethics</span><span>&rdquo; as &ldquo;concerned with the consequences of behavior of machines towards human users and other machines.&rdquo;</span><sup><a href="#ftnt67" id="ftnt_ref67">[67]</a></sup><span>&nbsp;Gunkel (2012, pp. 102-3) claims that Michael Anderson &ldquo;credits&rdquo; J. Storrs Hall&rsquo;s article &ldquo;Ethics for Machines&rdquo; (2000) as &ldquo;having first introduced and formulated the term &lsquo;machine ethics&rsquo;&rdquo; and notes that this article &ldquo;explicitly recognizes the exclusion of the machine from the ranks of both moral agency and patiency&rdquo; but &ldquo;proceeds to give exclusive attention to the former.&rdquo;</span><sup><a href="#ftnt68" id="ftnt_ref68">[68]</a></sup><span>&nbsp;Hall (2000) contained few formal references but appears to have been directly influenced by transhumanist writers and perhaps by discussion about artificial life and consciousness.</span><sup><a href="#ftnt69" id="ftnt_ref69">[69]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Similar exclusions were made in subsequent years in delineating the focus of Gianmarco Veruggio&rsquo;s (2006) &ldquo;roboethics roadmap,&rdquo; where roboethics refers to &ldquo;the ethics inspiring the design, development and employment of Intelligent Machines&rdquo; (Veruggio &amp; Operto, 2006). Veruggio (2006) notes that, &ldquo;[t]he name Roboethics (coined in 2002 by the author) was officially proposed during the First International Symposium of Roboethics (Sanremo, Jan/Feb. 2004).&rdquo; Veruggio (2006) references J. Storrs Hall and various papers by Floridi when expounding the concept.</span><sup><a href="#ftnt70" id="ftnt_ref70">[70]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>These exclusions from machine ethics and roboethics may explain why it is so common for subsequent contributors to decry that there has not been much scholarly attention to the moral consideration of artificial entities (e.g. Levy, 2009; </span><span>Metzinger, 2013</span><span>; Gunkel, </span><span>2018</span><span class="c8 c12">, pp. 39-40). However, some contributors have explicitly argued for the inclusion of such topics in roboethics. In the same volume as Veruggio and Operto&rsquo;s (2006) delineation of the field, Asaro (2006) argued that &ldquo;the best approach to robot ethics is one which addresses all three of&hellip; the ethical systems built into robots, the ethics of people who design and use robots, and the ethics of how people treat robots.&rdquo; While not necessarily arguing explicitly for its inclusion, later contributions have also used the term roboethics in a manner that would include discussion of moral consideration (e.g. Coeckelbergh, 2009; Steinart, 2014). </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Similarly, Steve Torrance questioned in a paper entitled &ldquo;A Robust View of Machine Ethics&rdquo; (2005), presented to an AAAI Fall Symposium focused on machine ethics, whether we should &ldquo;be thinking of extending the UN Universal Declaration of Human Rights to include future humanoid robots.&rdquo; Calverley&rsquo;s (</span><span>2005</span><span>a) paper presented at the same symposium also addressed the granting of legal rights to artificial entities. Neither author seems to have explicitly argued for the relevance of these topics to the emerging field of machine ethics; they continued lines of research that they had been developing elsewhere, but were accepted into the machine ethics symposium anyway.</span><sup><a href="#ftnt71" id="ftnt_ref71">[71]</a></sup><span class="c8 c12">&nbsp;Some subsequent papers have continued to identify themselves with the field of machine ethics while discussing the moral consideration of artificial entities (e.g. Torrance, 2008; Tonkens, 2012).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">It seems then, that while some of the earliest formal expositions of machine ethics and roboethics excluded discussion of the moral consideration of artificial entities, a number of contributors have nevertheless addressed this topic within those fields. Furthermore, many of the authors interested in AI rights have continued to cite and discuss influential publications in machine ethics and roboethics (e.g. Veruggio, 2006; Wallach &amp; Allen, 2008; Anderson &amp; Anderson, 2011; see Table 9).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 9: Machine ethics and roboethics keyword searches</span></p><a id="t.ac5b20d2a1a2839033d91bc80ebf1e2bc60d8177"></a><a id="t.9"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Robot ethics&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">91</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">33.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Machine ethics&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">70</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">25.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Wallach</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">60</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">22.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Anderson</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">59</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">21.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Torrance</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">40</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Roboethics</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">38</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">14.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Asaro</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">30</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Veruggio</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">21</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7.8%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Ethics for Machines&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.0%</span></p></td></tr></table><h2 class="c34 c6" id="h.y6wwkogi5wux"><span>Human-Computer Interaction and Human-Robot Interaction</span></h2><p class="c1 c6"><span class="c8 c12">Hewett et al. (1992, p. 5) defined human-computer interaction (HCI) as &ldquo;a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them.&rdquo; The field&rsquo;s emergence was influenced by developments in computer science, ergonomics, cognitive psychology and a number of other disciplines, with specialist HCI journals, conferences, and organizations being set up from the 1970s onwards (Hewett et al., 1992). From the &lsquo;90s, HCI researchers began to join together with researchers from robotics, cognitive science, psychology, and other disciplines to form the field of human-robot interaction (HRI), which seeks to &ldquo;understand and shape the interactions between one or more humans and one or more robots&rdquo; (Goodrich &amp; Schultz, 2007).</span></p><p class="c1 c6"><span class="c8 c12">Research in HCI and HRI is often not focused on ethical issues per se. When ethics is discussed, it is often with reference to the design of robots and computers, rather than their potential rights or moral value.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Nevertheless, Friedman et al.&rsquo;s (2003) presentation at an HCI conference investigated &ldquo;social responses to AIBO,&rdquo; a robotic dog, using &ldquo;people&rsquo;s spontaneous dialog in online AIBO discussion forums,&rdquo; and noted that &ldquo;few members (12%) affirmed that AIBO had moral standing.&rdquo;</span><sup><a href="#ftnt72" id="ftnt_ref72">[72]</a></sup><span>&nbsp;The introduction referenced the lead author&rsquo;s presentation at an earlier HCI conference of interview findings on &ldquo;reasoning about computers as moral agents&rdquo; (Friedman, 1995) and a number of publications about various aspects of social interaction with robots or computers, but seemingly no previous literature about moral consideration. Instead, given that they generated their coding manual from &ldquo;pilot data&rdquo; on the forums, it seems possible that the authors&rsquo; inclusion of &ldquo;moral standing&rdquo; as a category arose because the participants themselves were talking about the topic and the researchers felt unable to ignore this aspect.</span><sup><a href="#ftnt73" id="ftnt_ref73">[73]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Friedman et al. (2003) has been cited hundreds of times (Google Scholar, 2021e), mostly by authors in the fields of HCI and HRI. The co-authors themselves published a number of subsequent items that empirically explored attributions of &ldquo;moral standing&rdquo; to artificial entities alongside perceptions of mental capacities and other attributes (e.g. Kahn et al., 2004; Kahn et al., 2006; Melson et al., 2009a; Melson et al., 2009b; Kahn et al., 2012). </span><span>Otherwise, however, few of the publications citing Friedman et al. (2003) in the following few years seem to have focused primarily on issues related to moral consideration.</span><sup><a href="#ftnt74" id="ftnt_ref74">[74]</a></sup><span>&nbsp;In one of the most relevant publications, Freier (2008) interviewed 60 children and found &ldquo;that the ability of the agent to express harm and make claims to its own rights significantly increases children&rsquo;s likelihood of identifying an act against the agent as a moral violation.&rdquo;</span><sup><a href="#ftnt75" id="ftnt_ref75">[75]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Seemingly independently of the research by Friedman, Kahn, and colleagues,</span><sup><a href="#ftnt76" id="ftnt_ref76">[76]</a></sup><span>&nbsp;</span><span>a workshop was held in Rome in 2005 on &ldquo;Abuse:</span><span>&nbsp;The Darker Side of Human-Computer Interaction,&rdquo; and a follow-up was held in Montreal the next year (agentabuse.org, </span><span>2005</span><span>). The descriptions of the workshops are clearly pitched towards the HCI research community, noting for example that &ldquo;HCI research is witnessing a shift&hellip; to an experiential vision where the computer is described as a medium for emotion&rdquo; (agentabuse.org, </span><span>2005</span><span>). The language of the website suggests a primary concern for the interests of humans, rather than the computers themselves,</span><sup><a href="#ftnt77" id="ftnt_ref77">[77]</a></sup><span>&nbsp;and this is reflected in the content of some of the papers presented at the workshops.</span><sup><a href="#ftnt78" id="ftnt_ref78">[78]</a></sup><span>&nbsp;</span><span>Other papers are more ambiguous in their motivations, but have clear implications for researchers interested in the moral consideration of artificial entities.</span><sup><a href="#ftnt79" id="ftnt_ref79">[79]</a></sup><span>&nbsp;Most explicitly addressing this topic, Bartneck et al. (</span><span>2005</span><span>b) tested how willing participants were to administer electric shocks to a robot when instructed to do so, and found that &ldquo;participants showed compassion for the robot but the experimenter&rsquo;s urges were always enough to make them continue&hellip; until the maximum voltage was reached.&rdquo;</span><sup><a href="#ftnt80" id="ftnt_ref80">[80]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Christopher Bartneck&rsquo;s publication history demonstrates how topics that have implications for the moral consideration of artificial entities can arise out of other topics in HCI or HRI. Bartneck had previously written about &ldquo;Affective Expressions of Machines&rdquo; (e.g. Bartneck, 2000), human interaction with artificial entities that express emotions (e.g. Bartneck, 2003), sci-fi treatment of social robots (Bartneck, 2004), and a wide array of topics relating to HRI but not moral consideration per se. Bartneck was certainly aware of some of the prior literature on legal rights for artificial entities.</span><sup><a href="#ftnt81" id="ftnt_ref81">[81]</a></sup><span>&nbsp;However, Bartneck&rsquo;s papers relevant to AI rights more frequently noted concern for human experiences than for the experiences of the artificial entities themselves,</span><sup><a href="#ftnt82" id="ftnt_ref82">[82]</a></sup><span>&nbsp;and most of the references were to other studies from the HCI and HRI fields. Despite having published hundreds of times, few of Bartneck&rsquo;s later publications seem to address the moral consideration of artificial entities explicitly </span><span>(Google Scholar, 2021f)</span><span>.</span><sup><a href="#ftnt83" id="ftnt_ref83">[83]</a></sup><span>&nbsp;Recently, Bartneck and Keijsers (2020) conducted an experiment examining responses to videos of abuse of robots, but Bartneck noted in a podcast interview that his concern with robot abuse was primarily one of virtue ethics, about how this behavior &ldquo;reflects&hellip; on us,&rdquo; rather than concern for the robots themselves (Radio New Zealand, 2020).</span><sup><a href="#ftnt84" id="ftnt_ref84">[84]</a></sup><span class="c8 c12">&nbsp;Bartneck et al. (2005b), Bartneck et al. (2007), and Bartneck and Hu (2008) did not accrue more than a handful of citations until around 2013 onwards (Google Scholar, 2021f), though a number of publications have cited these works and proceeded in a similar fashion, examining HRI from a perspective that has clear implications for the moral consideration of artificial entities (e.g. Beran et al., 2010; Briggs et al., 2014).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>One remarkably close parallel is a paper by Slater et al. (2006), who, like Bartneck et al. (</span><span>2005</span><span>b), carried out partial replications of </span><span>Stanley Milgram&rsquo;s (1974) experiment on obedience</span><span>&nbsp;&mdash; which tested whether participants would obey instructions to administer what they believed to be dangerous electric shocks to another person &mdash; with artificial entities. Whereas Bartneck et al. (</span><span>2005</span><span>b) used a robot, Slater et al. (2006) used a virtual human. Whereas Bartneck et al. (</span><span>2005</span><span>b) prominently cited previous studies on social interaction with robots to explain and justify the motivation for the study, Slater et al. (2006) prominently cited studies on human reactions and interactions in virtual environments and with virtual entities. Whereas Bartneck himself had numerous previous publications about HRI, Slater had numerous previous publications about interactions in virtual environments (Google Scholar, 2021c).</span><sup><a href="#ftnt85" id="ftnt_ref85">[85]</a></sup><span>&nbsp;Slater et al. (2006) did not cite any works by Bartneck, Friedman, or Kahn, though Bartneck and Hu (2008) and </span><span>a number of other studies relating to the moral consideration of artificial entities</span><span>&nbsp;(e.g. Misselhorn, 2009; Hartmann et al., 2010; Rosenthal-von der P&uuml;tten et al., 2013) have since cited Slater et al. (2006).</span><sup><a href="#ftnt86" id="ftnt_ref86">[86]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 10: Human-Computer Interaction and Human-Robot Interaction keyword searches</span></p><a id="t.b4a9df9af4406b2af8e1ccbe1ba7886ffb833ab0"></a><a id="t.10"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Human-Robot Interaction&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">81</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">30.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Kahn</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">27</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Bartneck</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">24</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8.9%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Friedman</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">23</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Human-Computer Interaction&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">22</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">8.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Slater</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.7%</span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Subsequently, a number of HRI or HCI publications have continued to explore the abuse of robots (e.g. </span><span>Nomura et al., 2015; Br&scaron;&#269;i&#263; et al., 2015</span><span>). </span><span>Others have addressed the moral consideration of artificial entities from alternative angles, influenced by news events or legal and ethics papers relating to AI rights (e.g. Spence et al., 2018; Lima et al., 2020)</span><span class="c8 c12">.</span></p><h2 class="c34 c6" id="h.dqgm1a2nb3vt"><span class="c18 c12">Social-relational ethics</span></h2><p class="c1 c6"><span>In 2018, communications scholar David Gunkel published </span><span class="c14">Robot Rights</span><span class="c8 c12">, the first book focused solely on this topic. The book is &ldquo;deliberately designed to think the unthinkable by critically considering and making (or venturing to make) a serious philosophical case for the rights of robots&rdquo; (p. xi).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Gunkel comments (2018, p. xiii) that his first &ldquo;formal articulation&rdquo; of the topic of robot rights was in the last chapter of his book </span><span class="c14">Thinking Otherwise: Philosophy, Communication, Technology</span><span>&nbsp;(2007), and that this was then developed in more depth in the section on &ldquo;Moral Patiency&rdquo; in his book </span><span class="c14">The Machine Question</span><span>&nbsp;(2012) and in numerous subsequent articles. However, Gunkel had also published the relevant chapter as an article in 2006. Although Gunkel (2012, 2018) would go on to provide thorough reviews of the existing literature, Gunkel&rsquo;s (2006) </span><span>early </span><span>discussion of &ldquo;the machine question&rdquo; contained </span><span>little reference</span><span>&nbsp;to previous writings explicitly about the moral consideration of artificial entities outside of science fiction. An exception is Gunkel&rsquo;s (2006) numerous citations of Hall&rsquo;s (2000) essay, especially the quote that &ldquo;we have never considered ourselves to have &lsquo;moral&rsquo; duties to our machines, or them to us.&rdquo;</span><sup><a href="#ftnt87" id="ftnt_ref87">[87]</a></sup><span>&nbsp;Gunkel has cited Hall&rsquo;s (2000) essay in at least seven different publications, including quoting this particular sentence again (e.g. 2014; </span><span>2018</span><span>, p. 55), suggesting that the essay may have been a key influence on Gunkel&rsquo;s interest in the topic.</span><sup><a href="#ftnt88" id="ftnt_ref88">[88]</a></sup><span class="c8 c12">&nbsp;Gunkel (2006) also cited Anderson et al. (2004), another foundational work in machine ethics.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In </span><span class="c14">The Machine Question </span><span>(2012), Gunkel critiqued the idea that moral patiency should just be subsumed within discussion of moral agency and critiqued the binary thinking around moral inclusion or exclusion based on &ldquo;individual qualities</span><span>.</span><span>&rdquo; Gunkel (2012, p. 177) eschewed intentional decision-making about the moral consideration </span><span>of other beings based on their capacities</span><span>, favoring instead </span><span>&ldquo;an uncontrolled and incomprehensible exposure to the face of the Other.&rdquo;</span><span>&nbsp;These arguments are </span><span>similar </span><span>to those developed in Gunkel&rsquo;s other publications, drawing heavily on the work of the philosopher Emmanuel Levinas to advance what he later referred to as a &ldquo;social relational&rdquo; ethic (e.g. 2018, p. 10). This stands in stark contrast to much of the previous literature arguing for moral consideration of artificial entities based on the capacities of the entities themselves.</span><sup><a href="#ftnt89" id="ftnt_ref89">[89]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Gunkel&rsquo;s social-relational approach was developed in tandem with the philosopher Mark Coeckelbergh.</span><sup><a href="#ftnt90" id="ftnt_ref90">[90]</a></sup><span>&nbsp;While Gunkel seems to have started addressing the moral consideration of robots a few years earlier, it seems that Coeckelbergh was the first to explicitly use the term &ldquo;social-relational&rdquo; in his 2010 paper: &ldquo;Robot rights? Towards a social-relational justification of moral consideration.&rdquo; Coeckelbergh (2010) critiqued deontological, utilitarian, and virtue ethical approaches that &ldquo;rest on ontological features of entities&rdquo; and that &ldquo;seem to belong to the realm of science-fiction or at least the far future.&rdquo; Instead, Coeckelbergh (2010) argued for granting &ldquo;some degree of moral consideration to some intelligent social robots&rdquo; by &ldquo;replacing the requirement that we have certain knowledge about real ontological features of the entity by the requirement that we experience the features of the entity as they appear to us in the context of the concrete human-robot relation and the wider social structures in which that relation is embedded.&rdquo; Coeckelbergh had addressed some similar themes in a 2009 paper, albeit without the &ldquo;social-relational&rdquo; label.</span><sup><a href="#ftnt91" id="ftnt_ref91">[91]</a></sup><span>&nbsp;Unlike Gunkel&rsquo;s (2006) earliest treatment of the topic, Coeckelbergh (2009; 2010) explicitly referenced many different previous writings that had touched on moral consideration of artificial entities.</span><sup><a href="#ftnt92" id="ftnt_ref92">[92]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In the same year as Gunkel&rsquo;s (2006) first treatment of the topic, </span><span>S&oslash;raker (</span><span>2006</span><span>a</span><span>) argued explicitly for a &ldquo;A Relational Theory of Moral Status,&rdquo; where &ldquo;information and information technology, at least in very special circumstances, ought to be ascribed moral status.&rdquo; As well being &ldquo;[i]nspired by the East Asian way of viewing the world as consisting of mutually constitutive relationships,&rdquo; S&oslash;raker (</span><span>2006</span><span>a</span><span>) drew heavily on animal rights writings</span><span>. S&oslash;raker (</span><span>2006</span><span>a) also acknowledged and cited Luciano Floridi.</span><sup><a href="#ftnt93" id="ftnt_ref93">[93]</a></sup><span>&nbsp;Unlike Coeckelbergh&rsquo;s (2009; 2010; 2012) or Gunkel&rsquo;s (2012; 2018) writings on the topic, however, S&oslash;raker&rsquo;s </span><span>(</span><span>2006</span><span>a</span><span>)</span><span>&nbsp;paper garnered only a small handful of citations, perhaps partly because S&oslash;raker did not pursue the topic as vigorously in subsequent publications </span><span>(Google Scholar, 2021i)</span><span>. S&oslash;raker </span><span>(</span><span>2006</span><span>a</span><span>)</span><span>&nbsp;and Coeckelbergh (2009; 2010) did not cite or acknowledge one another in their publications. However, given that both authors were in the department of philosophy at the University of Twente during this time period and were both contributing to a small new research field from a similar but relatively novel perspective, </span><span>it seems likely that at least one had influenced the other&rsquo;s thinking.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Although not detailing a novel ethical perspective as fully as Coeckelbergh (2010), Gunkel (2012), or S&oslash;raker (</span><span>2006</span><span>a) a number of other writers in the late &lsquo;00s had addressed similar themes. For example, human-robot interaction researcher Brian R. Duffy wrote a short paper published in the </span><span class="c14">International Review of Information Ethics </span><span>(2006) noting that, &ldquo;[w]ith the advent of the social machine, and particularly the social robot&hellip; the </span><span class="c14">perception </span><span>as to whether the machine has intentionality, consciousness and free-will will change. From a social interaction perspective, it becomes less of an issue whether the machine</span><span>&nbsp;</span><span class="c14">actually </span><span>has these properties and more of an issue as to whether it </span><span class="c14">appears </span><span>to have them.&rdquo; Duffy (2006) added that one perspective that gives rise to &ldquo;the issue of rights and duties&hellip; involves the notion of whether a human </span><span class="c14">perceives </span><span>the machine to have moral rights and duties, and incorporates the aesthetic of the machine.&rdquo;</span><sup><a href="#ftnt94" id="ftnt_ref94">[94]</a></sup><span>&nbsp;Relatedly, a number of authors expressed concerns similar to those hinted at by the HRI literature, about how negative treatment of robots might have implications for how humans interact with one another, or with animals (e.g. Whitby, 2008; Levy, 2009; Goldie, 2010).</span><sup><a href="#ftnt95" id="ftnt_ref95">[95]</a></sup><span>&nbsp;Since then, numerous other authors have picked up on similar themes.</span><sup><a href="#ftnt96" id="ftnt_ref96">[96]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 11: Social-relational ethics keyword searches</span></p><a id="t.989534c220cbb5a78d1429396c4edad50d40525d"></a><a id="t.11"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Gunkel</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">73</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">28.5%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Coeckelbergh</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">65</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">24.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Levy</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">44</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">16.5%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Social-relational&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">42</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">15.6%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Whitby</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">29</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Duffy</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">9</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">S&oslash;raker</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">3</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">1.1%</span></p></td></tr></table><h2 class="c34 c6" id="h.8ilt105mbuyj"><span>Moral and social psychology</span></h2><p class="c1 c6"><span>Psychology has contributed to the study of artificial life and consciousness (e.g. Krach et al., 2008), human-computer interaction (Hewett et al., 1992), and human-robot interaction (Goodrich &amp; Schultz, 2007), all of which have encouraged some interest in the moral consideration of artificial entities. </span><span>There has also been some interest in studying artificial entities as part of wider psychological theory-building about how moral inclusion and exclusion work.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>There are many different concepts and </span><span>scales</span><span>&nbsp;(batteries of tests intended to measure a particular attitude or psychological construct) relating to moral consideration that can be empirically examined across a range of different entity types. For example, </span><span>Reed and Aquino&rsquo;s (2003</span><span>) &ldquo;moral regard for outgroups&rdquo; scale included questions about a number of different groups of humans. Some scales have included various nonhumans but not artificial entities (e.g. Laham, 2009; Crimston et al., 2016),</span><sup><a href="#ftnt97" id="ftnt_ref97">[97]</a></sup><span class="c8 c12">&nbsp;but, at least two scales relevant to moral consideration have included artificial entities as well. Both were developed within a few years of each other and have been widely cited.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Firstly, to explore &ldquo;whether minds are perceived along one or more dimensions,&rdquo; Gray et al. (2007) asked participants questions about &ldquo;seven living human forms&hellip; three nonhuman animals&hellip; a dead woman, God, and a sociable robot (Kismet).&rdquo;</span><sup><a href="#ftnt98" id="ftnt_ref98">[98]</a></sup><span>&nbsp;</span><span>Published in </span><span class="c14">Science</span><span>, Gray et al.&rsquo;s (2007) discussion of their study is very brief, so their motivation for including a social robot in the scale is unclear. They cited Turing and Dennett in their second sentence as examples of authors who have assumed &ldquo;that mind perception occurs on one dimension&rdquo;; their inclusion of a robot as one of the studied entity types could be due to their interest in the topic having been sparked partly by these two thinkers, both of who prominently discussed the capabilities of AI (see &ldquo;artificial life and consciousness&rdquo; above).</span><sup><a href="#ftnt99" id="ftnt_ref99">[99]</a></sup><span>&nbsp;Some subsequent authors have continued to use robots as an entity type when exploring issues related to moral agency and patiency (e.g. Ward et al., 2013),</span><sup><a href="#ftnt100" id="ftnt_ref100">[100]</a></sup><span>&nbsp;while others have chosen not to do so (e.g. Piazza et al., 2014).</span><sup><a href="#ftnt101" id="ftnt_ref101">[101]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Secondly, Waytz et al.&rsquo;s (2010) &ldquo;Individual Differences in Anthropomorphism Questionnaire&rdquo; (IDAQ) asked about views on the capabilities of a number of nonhuman entities, both natural (e.g. animals, clouds) and artificial (e.g. robots, computers). Though developed by psychologists and published in psychology journals, both Waytz et al.&rsquo;s (2010) paper and the earlier, more theoretical paper that it built upon (Epley et al., 2007) included a number of references from HRI and HCI, such as prior work on perceptions and anthropomorphism of robots. Both papers explicitly noted consequences of their theory and studies of anthropomorphism for HCI and &ldquo;Moral Care and Concern&rdquo; for nonhumans.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>At a similar time, some of the psychological research around dehumanization included robots or other &ldquo;automata&rdquo; (e.g. Haslam, 2006; Loughnan &amp; Haslam, 2007).</span><sup><a href="#ftnt102" id="ftnt_ref102">[102]</a></sup><span>&nbsp;This literature focused more on the humans being dehumanized through comparison to or representation as automata than on robot rights, though this of course has some implications for how and why artificial entities are excluded from moral consideration. Indeed, this stream of research has often been cited alongside discussions of mind perception and anthropomorphism to explain and justify the focus of psychological research that investigates various aspects of the moral consideration of artificial entities.</span><sup><a href="#ftnt103" id="ftnt_ref103">[103]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Table 12: Moral and social psychology keyword searches</span></p><a id="t.a3cdb7744ba570333e055a98bd2a76a7aab12c39"></a><a id="t.12"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Psychology</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">137</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">50.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c7">Wegner </span><span class="c7">[a co-author of Gray et al. (2007)]</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">33</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">12.3%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Waytz</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">28</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">10.4%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Haslam</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">14</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.2%</span></p></td></tr></table><h2 class="c34 c6" id="h.p3odnbutfzx2"><span>Synthesis and proliferation</span></h2><p class="c1 c6"><span class="c8 c12">In more recent years, authors have continued to refine and develop ideas about the moral consideration of artificial entities and to conduct new relevant empirical research. Aggregating across the various streams of literature discussed above and the intersections between them, the number of scholarly publications on the topic seems to have been growing exponentially in the 21st century (Figure 1).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>From the mid &lsquo;10s onwards, it was no longer reasonable to claim that the topic as a whole had not been addressed at all; new contributions have tended to cite one or more of the relevant streams of research,</span><sup><a href="#ftnt104" id="ftnt_ref104">[104]</a></sup><span>&nbsp;though of course some earlier contributions had done this as well.</span><sup><a href="#ftnt105" id="ftnt_ref105">[105]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Even where a publication has garnered attention for addressing a seemingly new and surprising topic, there has sometimes been discussion of similar ideas among earlier contributions. </span><span>For example</span><span>, whether robots should be slaves was discussed decades earlier than Joanna Bryson&rsquo;s (2010) controversial article on the topic in science fiction (e.g. &#268;apek&rsquo;s 1921 play </span><span class="c14">R.U.R.</span><span>; Chu, 2010), at least one public-facing article (Modern Mechanix, 1957), and </span><span>academic </span><span class="c8 c12">writing on legal rights for artificial entities (e.g. Lehman-Wilzig, 1981; LaChat, 1986).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The &lsquo;10s also saw an increase in the prevalence of publications explicitly arguing against the moral consideration of artificial entities. There had been some earlier arguments to this effect (e.g. Drozdek, 1994; Birmingham, 2008), but mostly the idea had simply been ignored or marginalized, as in the early machine ethics and roboethics publications, rather than explicitly critiqued.</span><sup><a href="#ftnt106" id="ftnt_ref106">[106]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 13: Recent contributions keyword searches</span></p><a id="t.26c73bda386b9e4ee24274aaed8b61c92bfce1f8"></a><a id="t.13"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Bryson</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">52</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">19.5%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Darling</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">43</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">16.0%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Danaher</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">26</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">9.7%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Richardson</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">20</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">7.5%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Schwitzgebel</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">16</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5.9%</span></p></td></tr></table><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Research on AI rights and other moral consideration of artificial entities has received a number of thorough literature reviews (e.g. Gunkel, 2018; Harris &amp; Anthis, 2021). Several papers have called for integration of the empirical research from HCI, HRI, and social psychology with moral questions relevant to AI rights (Vanman &amp; Kappas, 2019; Harris &amp; Anthis, 2021). Indeed, a number of empirical research projects have been inspired by or noted their relevance to ongoing ethical discussions (e.g. Spence et al., 2018; Lima et al., 2020; K&uuml;ster et al., 2021). Other contributions have also explicitly sought to integrate seemingly disparate or conflicting strands of ethical and legal reasoning about the moral consideration of artificial entities (e.g. Gellers, 2020).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>In the 21st century, there have also been a number of news stories relevant to AI rights, such as the 2006 paper commissioned by the UK &ldquo;Horizon Scanning Centre&rdquo; suggesting that robots could be granted rights in 20 to 50 years, South Korea&rsquo;s proposed &ldquo;robot ethics charter&rdquo; in 2007, a 2017 European Parliament resolution that recommended the granting of legal status to &ldquo;electronic persons,&rdquo; and the granting of Saudi Arabian citizenship to the robot Sophia in 2017 (see </span><span>Harris, 2021</span><span class="c8 c12">).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>These events seem to have encouraged at least some academic discussion. Certainly, a number of authors mention them (see Table 14). Occasionally, authors explicitly cite these events as a motivation for their research or interest in the topic, such as Bennett and Daly (2020) framing their work as addressing the questions raised by the European Parliament Committee on Legal Affairs&rsquo; report. In other cases, the events may be one of several influences on the authors, or just a way to help justify their research as seeming current and important. For example, shortly after the 2006 report commissioned by the Horizon Scanning Centre, a </span><span>symposium </span><span>was organized on the question of &ldquo;Robots &amp; Rights: Will Artificial Intelligence Change The Meaning of Human Rights?&rdquo; featuring talks on the moral status of artificial entities by Nick Bostrom and Steve Torrance (James &amp; Scott, 2008). The opening sentence of the introduction to the symposium refers to the Horizon Scanning Centre report (James &amp; Scott, 2008), though it does not explicitly claim that this was the key spark for the symposium to be organized.</span><sup><a href="#ftnt107" id="ftnt_ref107">[107]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Table 14: News events keyword searches</span></p><a id="t.4369c4c9a061402639bd907b3b923754c803c9da"></a><a id="t.14"></a><table class="c30"><tr class="c35"><td class="c21" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Keyword</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">Items mentioning</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c9 c7">% of items</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;European Parliament&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">30</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">Sophia</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">30</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">11.1%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Robot ethics charter&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">6</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">2.2%</span></p></td></tr><tr class="c19"><td class="c22" colspan="1" rowspan="1"><p class="c1"><span class="c5">&quot;Horizon Scanning&quot;</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">5</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c10"><span class="c5">1.9%</span></p></td></tr></table><h1 class="c15 c6" id="h.8ihz69ksqd3q"><span>Discussion</span></h1><h2 class="c34 c6" id="h.owzl21gxa1qv"><span>Why</span><span>&nbsp;has </span><span>interest in this topic</span><span class="c18 c12">&nbsp;grown substantially in recent years?</span></h2><ul class="c27 lst-kix_6q32aygtq8j5-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">Certain contributors may have inspired others to publish on the topic.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>The &ldquo;Results&rdquo; section above identifies a handful of initial authors who seem to have played a key role in sparking discussion relevant to AI rights in each new stream of research, such as Floridi for information ethics, Bostrom for </span><span>transhumanism, effective altruism, and longtermism</span><span class="c8 c12">, and Gunkel and Coeckelbergh for social-relational ethics. Perhaps, then, some of the subsequent contributors who cited these authors were encouraged to address the topic because those writings sparked their interest in AI rights, or the publication of those items reassured them that it was possible (and sufficiently academically respectable) to publish about it.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>This seems especially plausible given that the beginnings of exponential growth some time between the late &lsquo;90s and mid-&rsquo;00s </span><span>(Figure 1)</span><span class="c8 c12">&nbsp;coincides reasonably well with the first treatments of the topic by several streams of research (Figure 2). This hypothesis could be tested further through interviews with later contributors who cited those pioneering works. Of course, even if correct, this hypothetical answer to our question would then beg another question: why did those pioneering authors themselves begin to address the moral consideration of artificial entities? Again, interviews (this time with the pioneering authors) may be helpful for further exploration.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_u7j1747mo297-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">The gradual, accumulating ubiquity of AI and robotic technology may have encouraged increased academic interest.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>A common theme in the introductions of and justifications for relevant publications is that the number, technological sophistication, and social integration of robots, AIs, computers, and other artificial entities is increasing (e.g. Lehman-Wilzig, 1981; Willick, 1983; Hall, 2000; </span><span>Bartneck et al., 2005</span><span class="c8 c12">b). Some of these contributors and others (e.g. Freitas, 1985; McNally &amp; Inayatullah, 1988; Bostrom, 2014) have been motivated by predictions about further developments in these trends. We might therefore hypothesize that academic interest in the topic has been stimulated by ongoing developments in the underlying technology.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Indeed, bursts of technical publications on AI in the 1950s and &lsquo;60s, artificial life in the &lsquo;90s, and synthetic biology in the &lsquo;00s seem to have sparked ethical discussions, where some of the contributors seem to have been largely unaware of previous, adjacent ethical discussions.</span><sup><a href="#ftnt108" id="ftnt_ref108">[108]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Additionally, the &ldquo;Results&rdquo; section above details how several new streams of relevant research from the 1980s onwards seem to have arisen independently of one another, such as Floridi&rsquo;s information ethics and the early transhumanist writers not citing each other or citing the previous research on legal rights for artificial entities. Even </span><span class="c14">within </span><span>the categories of research there was sometimes little interaction, such as the absence of cross-citation amongst the earliest contributors to discussion on each of legal rights for artificial entities, HCI and HRI (where relevant to AI rights), and social-relational ethics.</span><sup><a href="#ftnt109" id="ftnt_ref109">[109]</a></sup><span>&nbsp;If these different publications addressing similar topics did indeed arise independently of one another, it suggests that there were one or more underlying factors encouraging academic interest in the topic. The development and spread of relevant technologies is a plausible candidate for being such an underlying factor.</span><sup><a href="#ftnt110" id="ftnt_ref110">[110]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>However, the timing of the beginnings of exponential growth in publications on the moral consideration of artificial entities &mdash; seemingly from around the beginning of the 21st century (</span><span>Figure 1</span><span>) &mdash; does not match up very well to the timing and shape of technological progress. For example, there seems to have only been </span><span>linear growth in industrial robot installations and AI job postings in the &lsquo;10s (Zhang et al., 2021),</span><sup><a href="#ftnt111" id="ftnt_ref111">[111]</a></sup><span class="c8 c12">&nbsp;whereas exponential growth in computing power began decades earlier, in the 20th century (Roser &amp; Ritchie, 2013). This suggests that while this factor may well have contributed to the growth of research on AI rights and other moral consideration of artificial entities, it cannot single-handedly explain it.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_u7j1747mo297-0"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">Relevant news events may have encouraged increased academic interest.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">As noted in the &ldquo;Synthesis and proliferation&rdquo; subsection above, there have been a number of news events in the 21st century relevant to AI rights, and these have sometimes been mentioned by academic contributors to discussion on this topic. However, only a relatively small proportion of recent publications explicitly mention these events (Table 14). Additionally, the first relevant news event mentioned by multiple different publications was in 2006, whereas the exponential growth in publications seems to have begun prior to that (Figure 1). A particular news story also seems intuitively more likely to encourage a spike in publications than the start of an exponential growth trend.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_1u9ir57n0if9-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">The growth in research on this topic reflects wider trends in academic research.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>If the growth in academic publications in general &mdash; i.e. across any and all topics &mdash; has a similar timing and shape to the growth in interest in AI rights and other moral consideration of artificial entities, then we need not seek explanations for growth that are unique to this specific topic. There is some evidence that this is indeed the case; Fire and Guestrin&rsquo;s (2019) analysis of the Microsoft Academic Graph dataset identified exponential growth in the number of published academic papers throughout the 20th and early 21st century, and Ware and Mabe (2015) identified exponential growth in the numbers of researchers, journals, and journal articles, although their methodology for assessing the number of articles is unclear.</span><sup><a href="#ftnt112" id="ftnt_ref112">[112]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">At a more granular level, however, the prevalence of certain topics can presumably deviate from wider trends in publishing. For example, Zhang et al. (2021) report &ldquo;the number of peer-reviewed AI publications, 2000-19&rdquo;; the growth appears to have been exponential in the &lsquo;10s, but not the &lsquo;00s. There was a similar pattern in the &ldquo;number of paper titles mentioning ethics keywords at AI conferences, 2000-19.&rdquo; </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>So it was not inevitable that the number of relevant publications would increase exponentially as soon as some of the earliest contributors had touched on the topic of the moral consideration of artificial entities.</span><sup><a href="#ftnt113" id="ftnt_ref113">[113]</a></sup><span>&nbsp;</span><span>But science fiction, artificial life and consciousness, environmental ethics, and animal ethics all had some indirect implications for the moral consideration of artificial entities, even if they were not always stated explicitly. So it seems unsurprising that, in the context of exponential growth of academic publications, at least some scholars would begin to explore these implications more thoroughly and formally.</span><span>&nbsp;Indeed, even though several of the </span><span>new streams of relevant research from the 1980s onwards</span><span>&nbsp;seem to have arisen largely independently of each other, they often owed something to one or more of these earlier, adjacent topics.</span><sup><a href="#ftnt114" id="ftnt_ref114">[114]</a></sup></p><h2 class="c34 c6" id="h.q0ydipfxa48u"><span>Which levers can be pulled on to further increase interest in this topic?</span></h2><ul class="c27 lst-kix_27p5taye3z5c-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">Adopt publication strategies similar to those of the most successful previous contributors, focusing either on integration into other topics or on persistently revisiting AI rights.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">There seem to be two separate models for how the most notable and widely cited contributors to AI rights research have achieved influence.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Some, like Nick Bostrom, Mel Slater (and co-authors), and Lawrence Solum have published relatively few items specifically on this topic, but where they have done so, they have integrated the research into debates or topics of interest to a broader audience. They&rsquo;ve mostly picked up citations for those other reasons and topics, rather than their discussion of the moral consideration of artificial entities. They&rsquo;ve also tended to have strong academic credentials or publication track record relevant to those other topics, which may be a necessary condition for success in pursuing this model of achieving influence.</span><sup><a href="#ftnt115" id="ftnt_ref115">[115]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Others, like David Gunkel and Luciano Floridi, published directly on this topic numerous times, continuing to build upon and revisit it. Many of their individual contributions attracted limited attention in the first few years after publication,</span><sup><a href="#ftnt116" id="ftnt_ref116">[116]</a></sup><span class="c8 c12">&nbsp;but through persistent revisiting of the topic (and the passage of time) these authors have nonetheless accumulated impressive numbers of citations across their various publications relevant to AI rights. These authors continue to pursue other academic interests, however, and a substantial fraction of the interest in these authors (Floridi more so than Gunkel) seems to focus on how their work touches on other topics and questions, rather than its direct implications for the moral consideration of artificial entities.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Of course, these two models of paths to influence are simplifications. Some influential contributors, like Christoper Bartneck and Mark Coeckelbergh, fall in between these two extremes. There may be other publication strategies that could be even more successful, and it is possible that someone could adopt one of these strategies and still not achieve much influence.</span><sup><a href="#ftnt117" id="ftnt_ref117">[117]</a></sup><span class="c8 c12">&nbsp;Nevertheless, new contributors could take inspiration from these two pathways to achieving academic influence &mdash; which seem to have been quite successful in at least some cases &mdash; when seeking to maximize the impact of their own research.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_bf41d8dv8hrd-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">Engage with adjacent academic fields and debates.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>As noted above, a number of contributors have accrued citations from papers that addressed but did not focus solely on the moral consideration of artificial entities. Early contributions that addressed the moral consideration of artificial entities more directly without reference to other debates often languished in relative obscurity, at least for many years (e.g. Lehman-Wilzig, 1981; Willick, 1983; Freitas, 1985; McNally &amp; Inayatullah, 1988). This suggests that engaging with adjacent academic fields and debates may be helpful for contributors to be able to increase the impact of their research relevant to AI rights. </span><span>Relatedly, there is reason to believe that Fields&rsquo; first exposure to academic discussion relevant to AI rights may have been at an AI conference,</span><sup><a href="#ftnt118" id="ftnt_ref118">[118]</a></sup><span>&nbsp;perhaps encouraging them to write their 1987 article.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Although it seems coherent to distinguish between moral patiency and moral agency (e.g. Floridi, 1999; Gray et al., 2007; Gunkel, 2012), many successful publications have discussed both areas. For instance, much of the relevant literature in transhumanism, effective altruism, and longtermism has focused on threats posed to humans by intelligent artificial agents but has included some brief discussion of artificial entities as moral patients. Many writings address legal rights for artificial entities in tandem with discussion of those entities&rsquo; legal responsibilities to humans or each other. Before Gunkel (2018) wrote </span><span class="c14">Robot Rights</span><span>, he wrote (2012) </span><span class="c14">The Machine Question </span><span>with roughly equal weight to questions of moral agency and moral patiency.</span><span>&nbsp;Even Floridi, who has often referred to information ethics as a &ldquo;patient-oriented&rdquo; ethics, has been cited numerous times by contributors interested in AI rights for his 2004 article co-authored with Jeff Sanders &ldquo;On the Morality of Artificial Agents&rdquo;; 32 of the items in Harris and Anthis&rsquo; (2021) systematic searches (12.1%) have cited that article. Indeed, for some ethical frameworks, there is little meaningful distinction between agency and patiency.</span><sup><a href="#ftnt119" id="ftnt_ref119">[119]</a></sup><span class="c8 c12">&nbsp;Similarly, some arguments both for (e.g. Levy, 2009) and against (e.g. Bryson, 2010) the moral consideration of artificial entities seem to be motivated by concern for indirect effects on human society. So contributors may be able to tie AI rights issues back to human concerns, discuss both the moral patiency and moral agency of artificial entities, or discuss both legal rights and legal responsibilities; doing so may increase the reach of their publications.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Artificial consciousness, environmental ethics, and animal ethics all had potentially important ramifications for the moral consideration of artificial entities. These implications were remarked upon at the time, including by some of the key thinkers who developed these ideas, but the discussion was often brief. Later, machine ethics and roboethics had great potential for including discussion relevant to AI rights, but some of the early contributors seem to have decided to mostly set aside such discussion. It seems plausible that if some academics had been willing to address these implications more thoroughly, AI rights research might have picked up pace much earlier than it did. There may be field-building potential from monitoring the emergence and development of new, adjacent academic fields and reaching out to their contributors to encourage discussion of the moral consideration of artificial entities.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>As well as providing opportunities to advertise publications relevant to AI rights, engagement with adjacent fields and debates provides opportunities for inspiration and feedback. Floridi (2013) and Gunkel (</span><span>2018</span><span>) acknowledge discussion at conferences that had no explicit focus on AI rights as having been influential in shaping the development of their books.</span><sup><a href="#ftnt120" id="ftnt_ref120">[120]</a></sup><span>&nbsp;Additionally, several authors first presented initial drafts of their earliest relevant papers at such conferences (e.g. Putnam, 1960; Lehman-Wilzig, 1981; Floridi, </span><span>1999</span><span>).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_cbtzokxiczrr-0 start"><li class="c1 c25 c6 li-bullet-0"><span>Create specialized resources for research on AI rights and other moral consideration of artificial entities, such as journals, conferences, and research institutions.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>While the above points attest to the usefulness of engagement with adjacent fields and debates (e.g. by attending conferences, citing relevant publications), in order to grow further, it seems likely that AI rights research also needs access to its own specialized &ldquo;organizational resources&rdquo; (Frickel &amp; Gross, 2005) such as research institutions, university departments, journals, and conferences (Muehlhauser, 2017; Animal Ethics, 2021). With a few exceptions (e.g. </span><span class="c14">The Machine Question: AI, Ethics and Moral Responsibility </span><span>symposium at the AISB / IACAP 2012 World Congress; Gunkel et al., 2012), the history of AI rights research reveals a striking lack of such specialized resources, events, and institutions. </span><span>Indeed, it is only recently that whole books dedicated solely to the topic have emerged (Gunkel, 2018; Gellers, 2020; </span><span>Gordon, 2020</span><span>).</span><sup><a href="#ftnt121" id="ftnt_ref121">[121]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">The creation of such specialized resources could also help to guard against the possibility that, as they intentionally engage with adjacent academic fields and debates, researchers drift away from their exploration of the moral consideration of artificial entities.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><ul class="c27 lst-kix_8t3wwjp4c0if-0 start"><li class="c1 c25 c6 li-bullet-0"><span class="c8 c12">Explore legal rights for artificial entities.</span></li></ul><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Detailed discussion of the legal rights of artificial entities was arguably the first area of academic enquiry to focus in much depth on the moral consideration of artificial entities. </span><span>Articles that touch on the moral consideration of artificial entities from a legal perspective seem to more frequently accrue a substantial number of citations</span><span>&nbsp;(e.g. Lehman-Wilzig, 1981; McNally &amp; Inayatullah, 1988; Solum, 1992; Karnow, 1994; Allen &amp; Widdison, 1996; Chopra &amp; White, 2004; Calverley, 2008).</span><sup><a href="#ftnt122" id="ftnt_ref122">[122]</a></sup><span>&nbsp;Additionally, in recent years, there have been a number of news stories related to legal rights of artificial entities (Harris, </span><span>2021</span><span class="c8 c12">). This could be due to differences in the referencing norms between different academic fields, but otherwise weakly suggests that exploration of legal topics is more likely to attract interest and have immediate relevance to public policy than more abstract philosophical or psychological topics.</span></p><h1 class="c6 c15" id="h.m2h1aqyt62je"><span class="c12 c39">Limitations</span></h1><p class="c1 c6"><span class="c8 c12">This report has relied extensively on inferences about authors&rsquo; intellectual influences based on explicit mentions and citations in their published works. These inferences may be incorrect, since there are a number of factors that may affect how an author portrays their influences.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>For example, in order to increase the chances that their manuscript is accepted for publication by a journal or cited by other researchers, an author may make guesses about what others would consider to be most appealing and compelling, then discuss some ideas more or less extensively than they would like to. </span><span>Scholars are somewhat incentivized to present their works as novel contributions, and so not to cite works with a substantial amount of overlap. </span><span class="c8 c12">Authors might also accidentally omit mention of previous publications or ideas that have influenced their own thinking. </span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>There are a few instances where a</span><span>&nbsp;likely connection between authors has not been mentioned, although we cannot know in any individual case why not. One example is the works of Mark Coeckelbergh and Johnny Hartz S&oslash;raker, who were both advancing novel &ldquo;relational&rdquo; perspectives on the moral consideration of artificial entities while in the department of philosophy at the University of Twente, but who do not cite or acknowledge each other&rsquo;s work.</span><span>&nbsp;</span><span>Another is how Nick Bostrom gained attention for the ideas that suggest our world is likely a simulation, but a similar point had been made earlier by fellow transhumanist Hans Moravec</span><span>.</span><sup><a href="#ftnt123" id="ftnt_ref123">[123]</a></sup></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">These examples suggest that the absence of mentions of particular publications does not prove that the author was not influenced by those publications. But there are also some reasons why the opposite may be true at times; that an author might mention publications that had barely influenced their own thinking.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">For example, they may be incentivized to cite foundational works in their field or works on adjacent, partly overlapping topics, in order to reassure publishers that there will be interest in their research. Alternatively, someone might come up with an idea relatively independently, but then conduct an initial literature review in order to contextualize their ideas; citing the publications that they identify would falsely convey the impression that their thinking had been influenced by those publications.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Since the relevance of identified publications was sometimes filtered by the title alone, it is likely that I have missed publications that contained relevant discussion but did not advertise this clearly in the title. Additionally, citations of included publications were often identified using the &ldquo;Cited by&hellip;&rdquo; tool on Google Scholar, but this tool seems to be imperfect, sometimes omitting items that I know to have cited the publication being reviewed.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">This report initially used Harris and Anthis&rsquo; (2021) literature review as its basis, which relied on systematic searches using keywords in English language. This has likely led to a vast underrepresentation of relevant content published in other languages. There is likely at least some work written in German, Italian, and other European languages. For example, Gunkel (2018) discussed some German-language publications that I did not see referenced in any other works (e.g. Schweighofer, 2001).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>This language restriction has likely also led to a substantial neglect of relevant writings by Asian scholars. </span><span>For Western scholars exploring the moral consideration of artificial entities, Asian religions and philosophies have variously been the focus of their research (e.g. Robertson, 2014), an influence on their own ethical perspectives (e.g. McNally &amp; Inayatullah, 1988), a chance discovery, or an afterthought, if they are mentioned at all.</span><sup><a href="#ftnt124" id="ftnt_ref124">[124]</a></sup><span class="c8 c12">&nbsp;However, very few items have been identified in this report that were written by Asian scholars themselves, and there may well be many more relevant publications.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>This report has not sought to explore in depth the longer-term intellectual origins for academic discussion of the moral consideration of artificial entities, such as the precedents provided by various moral philosophies developed during the Enlightenment.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">As I have discussed at length elsewhere (Harris, 2019), assessing causation from historical evidence is difficult; &ldquo;we should not place too much weight on hypothesized historical cause and effect relationships in general,&rdquo; or on &ldquo;the strategic knowledge gained from any individual historical case study.&rdquo; The commentary in the discussion section should therefore be treated as one interpretation of the identified evidence, rather than as established fact.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">The keyword searches are limited to the items included from Harris and Anthis&rsquo; (2021) systematic searches. Those searches did not include all research papers with relevance to the topic. For example, the thematic discussion in this report includes a number of publications that could arguably have merited inclusion in that review, if they had been identified by the systematic searches.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">The items identified in each keyword search were not manually checked to ensure that they did indeed refer to the keyword in the manner that was assumed. For example, the search for &ldquo;environment&rdquo; may have picked up mentions of that word that have nothing to do with environmental ethics (e.g. how a robot interacts with its &ldquo;environment&rdquo;) or just because they were published in &mdash; or cited another item that was published in &mdash; a journal with &ldquo;environment&rdquo; in its title.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Similarly, where multiple authors who might have been cited in the included publications share a surname (as is the case for at least the surnames Singer, Friedman, and Anderson), then the keyword searches might overrepresent the number of citations of that author. In contrast, if an author has a name that is sometimes misspelled by others (e.g. Putnam, Freitas, Lehman-Wilzig), then the searches might underrepresent the number of citations of them.</span></p><h1 class="c15 c6" id="h.1aabent6vhp0"><span class="c39 c12">Potential items for further study</span></h1><p class="c1 c6"><span>What is the history of AI rights research that is written in languages other than English?</span><span class="c8 c12">&nbsp;This report predominantly only included publications written in English, so relevant research in other languages may have been accidentally excluded.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Given the difficulty in assessing causation through historical evidence and in making inferences about authors&rsquo; intellectual influences based solely on explicit mentions in their published works, it would be helpful to supplement this report with interviews of researchers and other stakeholders.</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">Previous studies and theoretical papers have identified certain features as potentially important for the emergence of &ldquo;scientific/intellectual movements&rdquo; (e.g. Frickel &amp; Gross, 2005; Animal Ethics, 2021). A literature review of such contributions could be used to generate a list of potentially important features. The history of AI rights research could then be assessed against this list: which features appear to be present and which missing?</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span>Histories of other research fields could be useful for better understanding which levers can be pulled on to further increase interest in AI rights. Such studies could focus on the research fields that have the most overlap in content and context </span><span>(e.g. AI alignment, AI ethics, animal ethics) or that have achieved success most rapidly (e.g. computer science, cognitive science, synthetic biology).</span></p><p class="c1 c6 c2"><span class="c8 c12"></span></p><p class="c1 c6"><span class="c8 c12">There are numerous alternative historical research projects that could help to achieve the underlying goal of this report &mdash; to better understand how to encourage an expansion of humanity&rsquo;s moral circle to encompass artificial sentient beings. For example, rather than focusing on academic research fields, historical studies could focus on technological developments that have already created or substantially altered sentient life, such as cloning, factory farming, and genetic editing.</span></p><h1 class="c15 c6" id="h.t8dw3yfq86x5"><span>References</span></h1><p class="c3"><span>Aarhus University. (2021). Robophilosophy 2016 / TRANSOR 2016.</span><span><a class="c11" href="https://www.google.com/url?q=https://conferences.au.dk/robo-philosophy/previous-conferences/rp2016/&amp;sa=D&amp;source=editors&amp;ust=1660576307529985&amp;usg=AOvVaw2jOO8sTz2_D1mWnSSkMJ1K">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://conferences.au.dk/robo-philosophy/previous-conferences/rp2016/&amp;sa=D&amp;source=editors&amp;ust=1660576307530272&amp;usg=AOvVaw3IpTF14mrCy1q6r2Ny9ife">https://conferences.au.dk/robo-philosophy/previous-conferences/rp2016/</a></span><span class="c8 c12">. Accessed 22 September 2021</span></p><p class="c3"><span>Abelson, R. (1966). Persons, P-Predicates, and Robots. </span><span class="c14">American Philosophical Quarterly</span><span>, </span><span class="c14">3</span><span class="c8 c12">(4), 306&ndash;311.</span></p><p class="c3"><span>agentabuse.org. (2005). About the Abuse Interact 2005 Workshop &amp; About Rome&rsquo;s Talking Sculptures.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/about.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307530751&amp;usg=AOvVaw0FhDMoHO1JZuamsHyss4Po">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/about.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307530930&amp;usg=AOvVaw0O_d25CilLOo6RFEPX4X_z">http://www.agentabuse.org/about.htm</a></span><span class="c8 c12">. Accessed 3 December 2021</span></p><p class="c3"><span>Allen, T., &amp; Widdison, R. (1996). Can Computers Make Contracts. </span><span class="c14">Harvard Journal of Law &amp; Technology</span><span>, </span><span class="c14">9</span><span class="c8 c12">, 25.</span></p><p class="c3"><span>Anderson, M., &amp; Anderson, S. L. (2011). </span><span class="c14">Machine Ethics</span><span class="c8 c12">. Cambridge University Press.</span></p><p class="c3"><span class="c8 c12">Anderson, M., Anderson, S. L., &amp; Armen, C. (2004). Towards Machine Ethics (p. 7). Presented at the AAAI-04 Workshop on Agent Organizations: Theory and Practice, San Jose, CA.</span></p><p class="c3"><span>Animal Ethics. (2021). </span><span class="c14">Establishing a research field in natural sciences</span><span>. Oakland, CA: Animal Ethics.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.animal-ethics.org/establishing-new-field-natural-sciences/&amp;sa=D&amp;source=editors&amp;ust=1660576307531671&amp;usg=AOvVaw1PFU-vaocWgkBLtmJh6Vpy">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.animal-ethics.org/establishing-new-field-natural-sciences/&amp;sa=D&amp;source=editors&amp;ust=1660576307531889&amp;usg=AOvVaw3zL0bWkza3BD1G3stDPMmd">https://www.animal-ethics.org/establishing-new-field-natural-sciences/</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Anthis, J. R., &amp; Paez, E. (2021). Moral circle expansion: A promising strategy to impact the far future. </span><span class="c14">Futures</span><span>, </span><span class="c14">130</span><span>, 102756.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.futures.2021.102756&amp;sa=D&amp;source=editors&amp;ust=1660576307532286&amp;usg=AOvVaw1QF7AiOjrDTicwQGoyxL1K">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.futures.2021.102756&amp;sa=D&amp;source=editors&amp;ust=1660576307532492&amp;usg=AOvVaw2hiIYDptVEbfCwsJWoW4ED">https://doi.org/10.1016/j.futures.2021.102756</a></span></p><p class="c3"><span>Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2012). Thinking Inside the Box: Controlling and Using an Oracle AI. </span><span class="c14">Minds and Machines</span><span>, </span><span class="c14">22</span><span>(4), 299&ndash;324.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11023-012-9282-2&amp;sa=D&amp;source=editors&amp;ust=1660576307532923&amp;usg=AOvVaw17DNwVOUwbtVyjE9z6hItn">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11023-012-9282-2&amp;sa=D&amp;source=editors&amp;ust=1660576307533164&amp;usg=AOvVaw2zYsloYcXC7HR8fUED4NT4">https://doi.org/10.1007/s11023-012-9282-2</a></span></p><p class="c3"><span>Asaro, P. M. (2001). Hans Moravec, Robot. Mere Machine to Transcendent Mind, New York, NY: Oxford University Press, Inc., 1999, ix + 227 pp., $25.00 (cloth), ISBN 0-19-511630-5. </span><span class="c14">Minds and Machines</span><span>, </span><span class="c14">11</span><span>(1), 143&ndash;147.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1011202314316&amp;sa=D&amp;source=editors&amp;ust=1660576307533585&amp;usg=AOvVaw0HkFNChKmFtjFVmILZBAhW">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1011202314316&amp;sa=D&amp;source=editors&amp;ust=1660576307533764&amp;usg=AOvVaw3B7kh-kREpmoi5CQU4XIsS">https://doi.org/10.1023/A:1011202314316</a></span></p><p class="c3"><span>Asaro, P. M. (2006). What Should We Want From a Robot Ethic? </span><span class="c14">The International Review of Information Ethics</span><span>, </span><span class="c14">6</span><span>, 9&ndash;16.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie134&amp;sa=D&amp;source=editors&amp;ust=1660576307534155&amp;usg=AOvVaw26Ai9RWpQlowv92Qy4-7nX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie134&amp;sa=D&amp;source=editors&amp;ust=1660576307534325&amp;usg=AOvVaw32ObZBw2SCgxiwrEThaHZS">https://doi.org/10.29173/irie134</a></span></p><p class="c3"><span>Barfield, W. (2005). Issues of Law for Software Agents within Virtual Environments. </span><span class="c14">Presence: Teleoperators and Virtual Environments</span><span>, </span><span class="c14">14</span><span>(6), 741&ndash;748.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1162/105474605775196607&amp;sa=D&amp;source=editors&amp;ust=1660576307534718&amp;usg=AOvVaw38qBQLQ0oYZCreQOBK3MCk">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1162/105474605775196607&amp;sa=D&amp;source=editors&amp;ust=1660576307534891&amp;usg=AOvVaw3c4wxdJOoN4qReqZ7db-Qp">https://doi.org/10.1162/105474605775196607</a></span></p><p class="c3"><span>Bartneck, C. (2000). </span><span class="c14">Affective expressions of machines</span><span>. Stan Ackermans Institute, Eindhoven. Retrieved from</span><span><a class="c11" href="https://www.google.com/url?q=https://ir.canterbury.ac.nz/bitstream/handle/10092/13665/bartneckMasterThesis2000.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307535274&amp;usg=AOvVaw1IFzTkaXWN_KepiBnroakx">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://ir.canterbury.ac.nz/bitstream/handle/10092/13665/bartneckMasterThesis2000.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307535524&amp;usg=AOvVaw14WP-ieEYlwopkfAfT1XOv">https://ir.canterbury.ac.nz/bitstream/handle/10092/13665/bartneckMasterThesis2000.pdf?sequence=2</a></span></p><p class="c3"><span>Bartneck, C. (2003). Interacting with an embodied emotional character. In </span><span class="c14">Proceedings of the 2003 international conference on Designing pleasurable products and interfaces - DPPI &rsquo;03</span><span>&nbsp;(p. 55). Presented at the the 2003 international conference, Pittsburgh, PA, USA: ACM Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/782896.782911&amp;sa=D&amp;source=editors&amp;ust=1660576307536043&amp;usg=AOvVaw2_Pf2QX0nEMlRDOLGTcuxX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/782896.782911&amp;sa=D&amp;source=editors&amp;ust=1660576307536269&amp;usg=AOvVaw1g8AfcrdcpzwGO5PxN1LCc">https://doi.org/10.1145/782896.782911</a></span></p><p class="c3"><span class="c8 c12">Bartneck, C. (2004). From Fiction to Science &ndash; A cultural reflection of social robots (p. 4). Presented at the CHI2004 Workshop on Shaping Human-Robot Interaction, Vienna.</span></p><p class="c3"><span>Bartneck, C. (2006). Killing a Robot. In A. De Angeli, S. Brahnam, P. Wallis, &amp; A. Dix (Eds.), </span><span class="c14">Misuse and Abuse of Interactive Technologies</span><span>&nbsp;(pp. 5&ndash;8). Presented at the CHI 2006 Conference on Human Factors in Computing Systems, Montr&eacute;al Qu&eacute;bec Canada: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307536708&amp;usg=AOvVaw2dq9Jeof8mJPHo8Hf9VVdz">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307536878&amp;usg=AOvVaw2N9yqNCaqvW-894Y6SXlT-">https://doi.org/10.1145/1125451.1125753</a></span></p><p class="c3"><span>Bartneck, C., Brahnam, S., Angeli, A. D., &amp; Pelachaud, C. (2008). Editorial. </span><span class="c14">Interaction Studies</span><span>, </span><span class="c14">9</span><span>(3), 397&ndash;401.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/is.9.3.01edi&amp;sa=D&amp;source=editors&amp;ust=1660576307537274&amp;usg=AOvVaw1nfgtWk6Bdut_ZWBcACHf3">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/is.9.3.01edi&amp;sa=D&amp;source=editors&amp;ust=1660576307537449&amp;usg=AOvVaw1nCrBRlCRb2hy_Afp0yqej">https://doi.org/10.1075/is.9.3.01edi</a></span></p><p class="c3"><span>Bartneck, C., &amp; Hu, J. (2008). Exploring the abuse of robots. </span><span class="c14">Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems</span><span>, </span><span class="c14">9</span><span>(3), 415&ndash;433.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/is.9.3.04bar&amp;sa=D&amp;source=editors&amp;ust=1660576307537844&amp;usg=AOvVaw2-jcwPr48J4zE5t-5cMPan">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/is.9.3.04bar&amp;sa=D&amp;source=editors&amp;ust=1660576307538017&amp;usg=AOvVaw3HSOLipieib6_emVGFBS18">https://doi.org/10.1075/is.9.3.04bar</a></span></p><p class="c3"><span>Bartneck, C., J, R., &amp; A, B. (2004). In your face, robot! The influence of a character&rsquo;s embodiment on how users perceive its emotional expressions. In </span><span class="c14">Proceedings of the Design and Emotion Conference</span><span>. Ankara, Turkey.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.6084/m9.figshare.5160769&amp;sa=D&amp;source=editors&amp;ust=1660576307538474&amp;usg=AOvVaw3d6VMAE3z9OBjFotjdJ65N">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.6084/m9.figshare.5160769&amp;sa=D&amp;source=editors&amp;ust=1660576307538648&amp;usg=AOvVaw2FbKNNGPmCm8bx5uUQ7gXE">https://doi.org/10.6084/m9.figshare.5160769</a></span></p><p class="c3"><span>Bartneck, C., &amp; Keijsers, M. (2020). The morality of abusing a robot. </span><span class="c14">Paladyn, Journal of Behavioral Robotics</span><span>, </span><span class="c14">11</span><span>(1), 271&ndash;283.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1515/pjbr-2020-0017&amp;sa=D&amp;source=editors&amp;ust=1660576307539041&amp;usg=AOvVaw2MII3N0euHKGurNi43Quxt">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1515/pjbr-2020-0017&amp;sa=D&amp;source=editors&amp;ust=1660576307539218&amp;usg=AOvVaw0ml-y_i5u-hrO24uFiyCOK">https://doi.org/10.1515/pjbr-2020-0017</a></span></p><p class="c3"><span>Bartneck, C., Nomura, T., Kanda, T., Suzuki, T., &amp; Kato, K. (2005a). Cultural Differences in Attitudes Towards Robots. In </span><span class="c14">Proceedings of the AISB Symposium on Robot Companions: Hard Problems And Open Challenges In Human-Robot Interaction</span><span>&nbsp;(pp. 1&ndash;4). Hatfield, UK.</span><span><a class="c11" href="https://www.google.com/url?q=https://ir.canterbury.ac.nz/bitstream/handle/10092/16849/bartneckAISB2005.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307539642&amp;usg=AOvVaw1y-SVl9aZbaveiRpNabnjb">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://ir.canterbury.ac.nz/bitstream/handle/10092/16849/bartneckAISB2005.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307539872&amp;usg=AOvVaw3mxb6OeY9RpmOxBnnx-Wvg">https://ir.canterbury.ac.nz/bitstream/handle/10092/16849/bartneckAISB2005.pdf?sequence=2</a></span></p><p class="c3"><span>Bartneck, C., Rosalia, C., Menges, R., &amp; Deckers, I. (2005b). Robot Abuse &ndash; A Limitation of the Media Equation. In A. De Angeli, S. Brahnam, &amp; P. Wallis (Eds.), </span><span class="c14">Proceedings of Abuse: The darker side of Human-Computer Interaction</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307540240&amp;usg=AOvVaw2fdx91YwC9RtT1_AJ3mlsD">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307540419&amp;usg=AOvVaw0GO1unLvb90uGg_ucAAZLW">http://www.agentabuse.org/Abuse_Workshop_WS5.pdf</a></span></p><p class="c3"><span>Bartneck, C., van der Hoek, M., Mubin, O., &amp; Al Mahmud, A. (2007). &ldquo;Daisy, daisy, give me your answer do!&rdquo; switching off a robot. In </span><span class="c14">2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)</span><span class="c8 c12">&nbsp;(pp. 217&ndash;222). Presented at the 2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI).</span></p><p class="c3"><span>Basl, J. (2014). Machines as Moral Patients We Shouldn&rsquo;t Care About (Yet): The Interests and Welfare of Current Machines. </span><span class="c14">Philosophy &amp; Technology</span><span>, </span><span class="c14">27</span><span>(1), 79&ndash;96.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0122-y&amp;sa=D&amp;source=editors&amp;ust=1660576307540944&amp;usg=AOvVaw1PmWQrov4FtecuQe53mSgW">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0122-y&amp;sa=D&amp;source=editors&amp;ust=1660576307541140&amp;usg=AOvVaw0a3-jhD4vkxW8Kn6nSmU1h">https://doi.org/10.1007/s13347-013-0122-y</a></span></p><p class="c3"><span>Basl, J., &amp; Sandler, R. (2013). The good of non-sentient entities: Organisms, artifacts, and synthetic biology. </span><span class="c14">Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences</span><span>, </span><span class="c14">44</span><span>(4, Part B), 697&ndash;705.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.shpsc.2013.05.017&amp;sa=D&amp;source=editors&amp;ust=1660576307541545&amp;usg=AOvVaw3Q4KpL1xaCu7ZBPIGEJVfU">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.shpsc.2013.05.017&amp;sa=D&amp;source=editors&amp;ust=1660576307541725&amp;usg=AOvVaw0wqxAL9FMDhuFR3m5QU0LZ">https://doi.org/10.1016/j.shpsc.2013.05.017</a></span></p><p class="c3"><span>Beers, D. L. (2006). </span><span class="c14">For the Prevention of Cruelty: The History and Legacy of Animal Rights Activism in the United States</span><span class="c8 c12">. Ohio University Press.</span></p><p class="c3"><span>Bepress. (2021). SelectedWorks - Curtis E.A. Karnow.</span><span><a class="c11" href="https://www.google.com/url?q=https://works.bepress.com/curtis_karnow/&amp;sa=D&amp;source=editors&amp;ust=1660576307542135&amp;usg=AOvVaw2WEV9cdh32yAvAB1fpgzu9">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://works.bepress.com/curtis_karnow/&amp;sa=D&amp;source=editors&amp;ust=1660576307542328&amp;usg=AOvVaw1ECzoOdIHcsHXLa6YVTg6W">https://works.bepress.com/curtis_karnow/</a></span><span class="c8 c12">. Accessed 18 November 2021</span></p><p class="c3"><span>Bennett, B., &amp; Daly, A. (2020). Recognising rights for robots: Can we? Will we? Should we? </span><span class="c14">Law, Innovation and Technology</span><span>, </span><span class="c14">12</span><span>(1), 60&ndash;80.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/17579961.2020.1727063&amp;sa=D&amp;source=editors&amp;ust=1660576307542696&amp;usg=AOvVaw3PDIOFEJG1FPPGDV_pGhtK">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/17579961.2020.1727063&amp;sa=D&amp;source=editors&amp;ust=1660576307542874&amp;usg=AOvVaw1uvnb19NhS75SpYkiYVQJ-">https://doi.org/10.1080/17579961.2020.1727063</a></span></p><p class="c3"><span>Beran, T. N., Ramirez-Serrano, A., Kuzyk, R., Nugent, S., &amp; Fior, M. (2011). Would Children Help a Robot in Need? </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">3</span><span>(1), 83&ndash;93.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-010-0074-7&amp;sa=D&amp;source=editors&amp;ust=1660576307543262&amp;usg=AOvVaw2pg5eOipWloEidSg8qF3Vf">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-010-0074-7&amp;sa=D&amp;source=editors&amp;ust=1660576307543442&amp;usg=AOvVaw3uiHZwdIXJft-OqoF1glYB">https://doi.org/10.1007/s12369-010-0074-7</a></span></p><p class="c3"><span>Boden, M. A. (1984). Artificial intelligence and social forecasting*. </span><span class="c14">The Journal of Mathematical Sociology</span><span>, </span><span class="c14">9</span><span>(4), 341&ndash;356.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/0022250X.1984.9989954&amp;sa=D&amp;source=editors&amp;ust=1660576307543826&amp;usg=AOvVaw1NoObRawFZKZeRvU1W_BFO">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/0022250X.1984.9989954&amp;sa=D&amp;source=editors&amp;ust=1660576307544006&amp;usg=AOvVaw3GdODnJjcPT5S4A7sTJ0ID">https://doi.org/10.1080/0022250X.1984.9989954</a></span></p><p class="c3"><span>Bostrom, N. (1998). How long before superintelligence? </span><span class="c14">International Journal of Future Studies</span><span>, </span><span class="c14">2</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.nickbostrom.com/superintelligence.html&amp;sa=D&amp;source=editors&amp;ust=1660576307544386&amp;usg=AOvVaw0WRFHEPMElDmUPBBals1yw">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.nickbostrom.com/superintelligence.html&amp;sa=D&amp;source=editors&amp;ust=1660576307544580&amp;usg=AOvVaw1qOoHCofCRy3l4t02ayWvw">https://www.nickbostrom.com/superintelligence.html</a></span><span class="c8 c12">. Accessed 23 November 2021</span></p><p class="c3"><span>Bostrom, N. (2001). Ethical Principles in the Creation of Artificial Minds.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.nickbostrom.com/ethics/aiethics.html&amp;sa=D&amp;source=editors&amp;ust=1660576307544929&amp;usg=AOvVaw1EVUnpsWNMqnWyhRc-oB8t">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.nickbostrom.com/ethics/aiethics.html&amp;sa=D&amp;source=editors&amp;ust=1660576307545120&amp;usg=AOvVaw3qsv061skChSgDpZBAl5OM">https://www.nickbostrom.com/ethics/aiethics.html</a></span><span class="c8 c12">. Accessed 8 May 2022</span></p><p class="c3"><span>Bostrom, N. (2002). Existential risks: analyzing human extinction scenarios and related hazards. </span><span class="c14">Journal of Evolution and Technology</span><span>, </span><span class="c14">9</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c&amp;sa=D&amp;source=editors&amp;ust=1660576307545525&amp;usg=AOvVaw0nKbkYC2pBr8NuFcTAg4we">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c&amp;sa=D&amp;source=editors&amp;ust=1660576307545734&amp;usg=AOvVaw3rU__KbTZvxDHf2CwB4xS_">https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c</a></span><span class="c8 c12">. Accessed 23 November 2021</span></p><p class="c3"><span>Bostrom, N. (2003). Are We Living in a Computer Simulation? </span><span class="c14">The Philosophical Quarterly</span><span>, </span><span class="c14">53</span><span>(211), 243&ndash;255.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/1467-9213.00309&amp;sa=D&amp;source=editors&amp;ust=1660576307546157&amp;usg=AOvVaw2fguEFMrBOs71kiE5gVuiI">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/1467-9213.00309&amp;sa=D&amp;source=editors&amp;ust=1660576307546357&amp;usg=AOvVaw19KaVvjhtqexwRQrJ92J0z">https://doi.org/10.1111/1467-9213.00309</a></span></p><p class="c3"><span>Bostrom, N. (2005). A History of Transhumanist Thought. </span><span class="c14">Journal of Evolution and Technology</span><span>, </span><span class="c14">14</span><span class="c8 c12">(1), 1&ndash;25.</span></p><p class="c3"><span>Bostrom, N. (2008). The Simulation Argument FAQ.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.simulation-argument.com/faq.html&amp;sa=D&amp;source=editors&amp;ust=1660576307546833&amp;usg=AOvVaw1RXr6BkNZ-gTZX955avkPZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.simulation-argument.com/faq.html&amp;sa=D&amp;source=editors&amp;ust=1660576307547029&amp;usg=AOvVaw1DstGQ2Y0ld-hOZSUEKHVM">https://www.simulation-argument.com/faq.html</a></span><span class="c8 c12">. Accessed 29 December 2021</span></p><p class="c3"><span>Bostrom, N., &amp; Yudkowsky, E. (2014). The Ethics of Artificial Intelligence. In K. Frankish &amp; W. M. Ramsey (Eds.), </span><span class="c14">The Cambridge Handbook of Artificial Intelligence</span><span class="c8 c12">&nbsp;(pp. 316&ndash;334). Cambridge, UK: Cambridge University Press.</span></p><p class="c3"><span>Brahnam, S. (2005). Strategies for handling customer abuse of ECAs. In A. De Angeli, S. Brahnam, &amp; P. Wallis (Eds.), </span><span class="c14">Proceedings of Abuse: The darker side of Human-Computer Interaction</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307547543&amp;usg=AOvVaw3OsFiZWkz06s7MQd3nTvXh">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307547740&amp;usg=AOvVaw3ZhXAC4ptkmWuBtLLuINUo">http://www.agentabuse.org/Abuse_Workshop_WS5.pdf</a></span></p><p class="c3"><span>Brahnam, S. (2006). Gendered Bods and Bot Abuse. In A. De Angeli, S. Brahnam, P. Wallis, &amp; A. Dix (Eds.), </span><span class="c14">Misuse and Abuse of Interactive Technologies</span><span>&nbsp;(pp. 13&ndash;16). Presented at the CHI 2006 Conference on Human Factors in Computing Systems, Montr&eacute;al Qu&eacute;bec Canada: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307548110&amp;usg=AOvVaw3BqTrbF1YN2Pq5iCZghjw1">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307548297&amp;usg=AOvVaw0U64uAXaVR__g_0c1s99N6">https://doi.org/10.1145/1125451.1125753</a></span></p><p class="c3"><span>Brennan, A., &amp; Lo, Y.-S. (2021). Environmental Ethics. In E. N. Zalta (Ed.), </span><span class="c14">The Stanford Encyclopedia of Philosophy</span><span>&nbsp;(Winter 2021.). Metaphysics Research Lab, Stanford University.</span><span><a class="c11" href="https://www.google.com/url?q=https://plato.stanford.edu/archives/win2021/entries/ethics-environmental/&amp;sa=D&amp;source=editors&amp;ust=1660576307548705&amp;usg=AOvVaw0tXJLcfGuIBo3o9K2xoRJk">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://plato.stanford.edu/archives/win2021/entries/ethics-environmental/&amp;sa=D&amp;source=editors&amp;ust=1660576307548917&amp;usg=AOvVaw3itDELANWq-IIiI2ITzJPy">https://plato.stanford.edu/archives/win2021/entries/ethics-environmental/</a></span><span class="c8 c12">. Accessed 5 October 2021</span></p><p class="c3"><span>Brey, P. (2008). Do we have moral duties towards information objects? </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">10</span><span>(2), 109&ndash;114.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-008-9170-x&amp;sa=D&amp;source=editors&amp;ust=1660576307549295&amp;usg=AOvVaw20EyIql5EckgZ4EF4Wfes3">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-008-9170-x&amp;sa=D&amp;source=editors&amp;ust=1660576307549507&amp;usg=AOvVaw1TA5xpWbQXEtUK3diDCpox">https://doi.org/10.1007/s10676-008-9170-x</a></span></p><p class="c3"><span>Brooks, R. (2000, June 19). Will Robots Rise Up And Demand Their Rights? </span><span class="c14">Time</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://content.time.com/time/subscriber/article/0,33009,997274,00.html&amp;sa=D&amp;source=editors&amp;ust=1660576307549885&amp;usg=AOvVaw0_pLN2r70A_L_wOQA7OlMs">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://content.time.com/time/subscriber/article/0,33009,997274,00.html&amp;sa=D&amp;source=editors&amp;ust=1660576307550117&amp;usg=AOvVaw2k-uqadix-__8nRtqhUm4l">http://content.time.com/time/subscriber/article/0,33009,997274,00.html</a></span><span class="c8 c12">. Accessed 29 November 2021</span></p><p class="c3"><span>Br&scaron;&#269;i&#263;, D., Kidokoro, H., Suehiro, Y., &amp; Kanda, T. (2015). Escaping from Children&rsquo;s Abuse of Social Robots. In </span><span class="c14">Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction</span><span>&nbsp;(pp. 59&ndash;66). Presented at the HRI &rsquo;15: ACM/IEEE International Conference on Human-Robot Interaction, Portland Oregon USA: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/2696454.2696468&amp;sa=D&amp;source=editors&amp;ust=1660576307550580&amp;usg=AOvVaw0BjcbRjaUU7WVyFRsUUqzn">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/2696454.2696468&amp;sa=D&amp;source=editors&amp;ust=1660576307550769&amp;usg=AOvVaw16Dx2w98Y8CmgZUXEuTa0u">https://doi.org/10.1145/2696454.2696468</a></span></p><p class="c3"><span>Bryson, J. J. (2010). Robots should be slaves. In Y. Wilks (Ed.), </span><span class="c14">Natural Language Processing</span><span>&nbsp;(Vol. 8, pp. 63&ndash;74). Amsterdam: John Benjamins Publishing Company.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/nlp.8.11bry&amp;sa=D&amp;source=editors&amp;ust=1660576307551130&amp;usg=AOvVaw1WHBX-FbViRitvt3H2_0F8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1075/nlp.8.11bry&amp;sa=D&amp;source=editors&amp;ust=1660576307551331&amp;usg=AOvVaw084lwbHAFfm0UZnciaYV3I">https://doi.org/10.1075/nlp.8.11bry</a></span></p><p class="c3"><span>Calverley, D. J. (2005a). Additional Thoughts Concerning the Legal Status of a Non-biological Machine. In </span><span class="c14">Papers from the 2005 AAAI Fall Symposium</span><span class="c8 c12">&nbsp;(pp. 30&ndash;37). Menlo Park, CA: The AAAI Press.</span></p><p class="c3"><span>Calverley, D. J. (2005b). Toward a Method for Determining the Legal Status of a Conscious Machine. In </span><span class="c14">Proceedings of the Symposium on Next Generation Approaches to Machine Consciousness: Imagination, Development, Intersubjectivity and Embodiment</span><span>&nbsp;(pp. 75&ndash;84). Presented at the AISB&rsquo;05: Social Intelligence and Interaction in Animals, Robots and Agents, Hatfield, UK: The Society for the Study of Artificial Intelligence and the Simulation of Behaviour.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.sacral.c.u-tokyo.ac.jp/pdf/Ikegami_MachineConsciousness_2005.pdf%23page%3D86&amp;sa=D&amp;source=editors&amp;ust=1660576307551989&amp;usg=AOvVaw2AJ_cE0RKZi_lZBzfrqzjk">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.sacral.c.u-tokyo.ac.jp/pdf/Ikegami_MachineConsciousness_2005.pdf%23page%3D86&amp;sa=D&amp;source=editors&amp;ust=1660576307552297&amp;usg=AOvVaw0xsBHt4kHO1ny_mHBagnC-">https://www.sacral.c.u-tokyo.ac.jp/pdf/Ikegami_MachineConsciousness_2005.pdf#page=86</a></span></p><p class="c3"><span>Calverley, D. J. (2006). Android science and animal rights, does an analogy exist? </span><span class="c14">Connection Science</span><span>, </span><span class="c14">18</span><span>(4), 403&ndash;417.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/09540090600879711&amp;sa=D&amp;source=editors&amp;ust=1660576307552809&amp;usg=AOvVaw2FPhnJj9kfTWAj4WPGlFa1">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/09540090600879711&amp;sa=D&amp;source=editors&amp;ust=1660576307552994&amp;usg=AOvVaw0NQ_w80U-MdWqvFxcC-Syz">https://doi.org/10.1080/09540090600879711</a></span></p><p class="c3"><span>Calverley, D. J. (2008). Imagining a non-biological machine as a legal person. </span><span class="c14">AI &amp; SOCIETY</span><span>, </span><span class="c14">22</span><span>(4), 523&ndash;537.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-007-0092-7&amp;sa=D&amp;source=editors&amp;ust=1660576307553405&amp;usg=AOvVaw1SVsnhIqMVHDp7gDzWIP8-">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-007-0092-7&amp;sa=D&amp;source=editors&amp;ust=1660576307553602&amp;usg=AOvVaw0PumVAoCC0ZKvkgLBNZ9Kw">https://doi.org/10.1007/s00146-007-0092-7</a></span></p><p class="c3"><span>Cameron, D. E., Bashor, C. J., &amp; Collins, J. J. (2014). A brief history of synthetic biology. </span><span class="c14">Nature Reviews Microbiology</span><span>, </span><span class="c14">12</span><span>(5), 381&ndash;390.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1038/nrmicro3239&amp;sa=D&amp;source=editors&amp;ust=1660576307554083&amp;usg=AOvVaw1kWKJRZSvJ362EhRnd-xGj">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1038/nrmicro3239&amp;sa=D&amp;source=editors&amp;ust=1660576307554278&amp;usg=AOvVaw0hRz3y18iC5Q0l58l-_0Yi">https://doi.org/10.1038/nrmicro3239</a></span></p><p class="c3"><span>Capurro, R. (2006). Towards an ontological foundation of information ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">8</span><span>(4), 175&ndash;186.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-9108-0&amp;sa=D&amp;source=editors&amp;ust=1660576307554663&amp;usg=AOvVaw0OcNQEypK7km7HUJPWz99o">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-9108-0&amp;sa=D&amp;source=editors&amp;ust=1660576307554836&amp;usg=AOvVaw177B3zn0IAcIqG9g06jNF_">https://doi.org/10.1007/s10676-006-9108-0</a></span></p><p class="c3"><span>Capurro, R., &amp; Pingel, C. (2002). Ethical issues of online communication research. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">4</span><span>(3), 189&ndash;194.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1021372527024&amp;sa=D&amp;source=editors&amp;ust=1660576307555190&amp;usg=AOvVaw0PlsgPgL56fwFeCnhHrd-G">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1021372527024&amp;sa=D&amp;source=editors&amp;ust=1660576307555354&amp;usg=AOvVaw1A5pteWGYNZK_C8omm9len">https://doi.org/10.1023/A:1021372527024</a></span></p><p class="c3"><span>Cherry, C. (1989). The possibility of computers becoming persons. A response to Dolby. </span><span class="c14">Social Epistemology</span><span>, </span><span class="c14">3</span><span>(4), 337&ndash;348.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728908578546&amp;sa=D&amp;source=editors&amp;ust=1660576307555737&amp;usg=AOvVaw2px-M94VrM-Ayk1uV-pA2C">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728908578546&amp;sa=D&amp;source=editors&amp;ust=1660576307555938&amp;usg=AOvVaw2JetbHhmDzo0HccDeRJc3Z">https://doi.org/10.1080/02691728908578546</a></span></p><p class="c3"><span>Chopra, S., &amp; White, L. (2004). Arti&#64257;cial Agents - Personhood in Law and Philosophy. In R. L. De M&aacute;ntaras &amp; L. Saitta (Eds.), </span><span class="c14">ECAI&rsquo;04: Proceedings of the 16th European Conference on Artificial Intelligence</span><span>&nbsp;(pp. 635&ndash;639). Valencia, Spain.</span><span><a class="c11" href="https://www.google.com/url?q=http://astrofrelat.fcaglp.unlp.edu.ar/filosofia_cientifica/media/papers/Chopra-White-Artificial_Agents-Personhood_in_Law_and_Philosophy.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307556410&amp;usg=AOvVaw3IlenjK6K2zSC2Xyu736o_">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://astrofrelat.fcaglp.unlp.edu.ar/filosofia_cientifica/media/papers/Chopra-White-Artificial_Agents-Personhood_in_Law_and_Philosophy.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307556701&amp;usg=AOvVaw0yZctHpZgwxe2GQfZI8NIY">http://astrofrelat.fcaglp.unlp.edu.ar/filosofia_cientifica/media/papers/Chopra-White-Artificial_Agents-Personhood_in_Law_and_Philosophy.pdf</a></span></p><p class="c3"><span>Chu, S.-Y. (2010). Robot Rights. In </span><span class="c14">Do Metaphors Dream of Literal Sleep?: A Science-Fictional Theory of Representation</span><span class="c8 c12">&nbsp;(pp. 214&ndash;244). Cambridge, MA: Harvard University Press.</span></p><p class="c3"><span>Clifford, R. D. (1996). Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up. </span><span class="c14">Tulane Law Review</span><span>, </span><span class="c14">71</span><span class="c8 c12">, 1675.</span></p><p class="c3"><span>Coeckelbergh, M., Loh, J., &amp; Funk, M. (2018). </span><span class="c14">Envisioning Robots in Society &ndash; Power, Politics, and Public Space: Proceedings of Robophilosophy 2018 / TRANSOR 2018</span><span class="c8 c12">. IOS Press.</span></p><p class="c3"><span>Coeckelbergh, Mark. (2007). Violent computer games, empathy, and cosmopolitanism. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">9</span><span>(3), 219&ndash;231.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-007-9145-3&amp;sa=D&amp;source=editors&amp;ust=1660576307557629&amp;usg=AOvVaw2VijXsiIQlikDRpzUVkU8e">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-007-9145-3&amp;sa=D&amp;source=editors&amp;ust=1660576307557808&amp;usg=AOvVaw01SrgxsZNYGNQcXKF2qlfe">https://doi.org/10.1007/s10676-007-9145-3</a></span></p><p class="c3"><span>Coeckelbergh, Mark. (2009). Personal Robots, Appearance, and Human Good: A Methodological Reflection on Roboethics. </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">1</span><span>(3), 217&ndash;221.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-009-0026-2&amp;sa=D&amp;source=editors&amp;ust=1660576307558223&amp;usg=AOvVaw1Dnv4V5LQmWUenXQB_1Mtc">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-009-0026-2&amp;sa=D&amp;source=editors&amp;ust=1660576307558441&amp;usg=AOvVaw3Do8F4UpoekhofKIOXOGwb">https://doi.org/10.1007/s12369-009-0026-2</a></span></p><p class="c3"><span>Coeckelbergh, Mark. (2010). Robot rights? Towards a social-relational justification of moral consideration. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">12</span><span>(3), 209&ndash;221.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-010-9235-5&amp;sa=D&amp;source=editors&amp;ust=1660576307558847&amp;usg=AOvVaw052HiDBvICGQT0dEPUb9mK">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-010-9235-5&amp;sa=D&amp;source=editors&amp;ust=1660576307559017&amp;usg=AOvVaw0zC6qdxHOfkWq8TKGELBo-">https://doi.org/10.1007/s10676-010-9235-5</a></span></p><p class="c3"><span>Coeckelbergh, Mark. (2011). Humans, Animals, and Robots: A Phenomenological Approach to Human-Robot Relations. </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">3</span><span>(2), 197&ndash;204.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-010-0075-6&amp;sa=D&amp;source=editors&amp;ust=1660576307559395&amp;usg=AOvVaw00XRCzbTC0xwNn586m4mlv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-010-0075-6&amp;sa=D&amp;source=editors&amp;ust=1660576307559573&amp;usg=AOvVaw3EljWKjj7RmO0XbNrafkTC">https://doi.org/10.1007/s12369-010-0075-6</a></span></p><p class="c3"><span>Coeckelbergh, Mark. (2012). </span><span class="c14">Growing Moral Relations: Critique of Moral Status Ascription</span><span class="c8 c12">. Palgrave Macmillan.</span></p><p class="c3"><span>Coeckelbergh, Mark. (2013). David J. Gunkel: The machine question: critical perspectives on AI, robots, and ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">15</span><span>(3), 235&ndash;238.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9305-y&amp;sa=D&amp;source=editors&amp;ust=1660576307560061&amp;usg=AOvVaw28iZ27PMMSBKGkDQusTsgT">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9305-y&amp;sa=D&amp;source=editors&amp;ust=1660576307560225&amp;usg=AOvVaw19RM1qWAjE_yBev5A0Q_kO">https://doi.org/10.1007/s10676-012-9305-y</a></span></p><p class="c3"><span>Coeckelbergh, Mark. (2014). The Moral Standing of Machines: Towards a Relational and Non-Cartesian Moral Hermeneutics. </span><span class="c14">Philosophy &amp; Technology</span><span>, </span><span class="c14">27</span><span>(1), 61&ndash;77.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0133-8&amp;sa=D&amp;source=editors&amp;ust=1660576307560607&amp;usg=AOvVaw0i3EJ6RJ-Ol6bZZQ-z23x8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0133-8&amp;sa=D&amp;source=editors&amp;ust=1660576307560776&amp;usg=AOvVaw2Q95XYwGXRMMQb5OA8nlCC">https://doi.org/10.1007/s13347-013-0133-8</a></span></p><p class="c3"><span>Crimston, C. R., Bain, P. G., Hornsey, M. J., &amp; Bastian, B. (2016). Moral expansiveness: Examining variability in the extension of the moral world. </span><span class="c14">Journal of Personality and Social Psychology</span><span>, </span><span class="c14">111</span><span>(4), 636&ndash;653.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/pspp0000086&amp;sa=D&amp;source=editors&amp;ust=1660576307561134&amp;usg=AOvVaw1xyDWHe46rE4qKlckbpYHv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/pspp0000086&amp;sa=D&amp;source=editors&amp;ust=1660576307561292&amp;usg=AOvVaw28MqpFeiZqoDybIIWTHF0u">https://doi.org/10.1037/pspp0000086</a></span></p><p class="c3"><span>Crippa, A. (2017). Ted talks analyses.</span><span><a class="c11" href="https://www.google.com/url?q=https://rstudio-pubs-static.s3.amazonaws.com/321337_38458c80a3fb4edf8755e8bce876e822.html&amp;sa=D&amp;source=editors&amp;ust=1660576307561696&amp;usg=AOvVaw2--rNfHOk3Ot7xmPZlxExJ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://rstudio-pubs-static.s3.amazonaws.com/321337_38458c80a3fb4edf8755e8bce876e822.html&amp;sa=D&amp;source=editors&amp;ust=1660576307561960&amp;usg=AOvVaw0yGZdb9NH1fXEH1VPjFVbL">https://rstudio-pubs-static.s3.amazonaws.com/321337_38458c80a3fb4edf8755e8bce876e822.html</a></span><span class="c8 c12">. Accessed 10 November 2021</span></p><p class="c3"><span>Danaher, J. (2020). Welcoming Robots into the Moral Circle: A Defence of Ethical Behaviourism. </span><span class="c14">Science and Engineering Ethics</span><span>, </span><span class="c14">26</span><span>(4), 2023&ndash;2049.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-019-00119-x&amp;sa=D&amp;source=editors&amp;ust=1660576307562367&amp;usg=AOvVaw2_cEbEeNxHKjgTQd4GdQTz">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-019-00119-x&amp;sa=D&amp;source=editors&amp;ust=1660576307562561&amp;usg=AOvVaw0lqP3W4gTOZbB9OLRKzejc">https://doi.org/10.1007/s11948-019-00119-x</a></span></p><p class="c3"><span>Danto, A. C. (1960). On Consciousness in Machines. In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 180&ndash;187). New York, NY: New York University Press.</span></p><p class="c3"><span>Dator, J. (1990). It&rsquo;s only a paper moon. </span><span class="c14">Futures</span><span>, </span><span class="c14">22</span><span>(10), 1084&ndash;1102.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(90)90009-7&amp;sa=D&amp;source=editors&amp;ust=1660576307563068&amp;usg=AOvVaw2e96aAZE0YlMDJQN7oEaw_">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(90)90009-7&amp;sa=D&amp;source=editors&amp;ust=1660576307563237&amp;usg=AOvVaw0ONsXSymSbDXuYE3VOc9wC">https://doi.org/10.1016/0016-3287(90)90009-7</a></span></p><p class="c3"><span>De Angeli, A. (2006). On Verbal Abuse Towards Chatterbots. In A. De Angeli, S. Brahnam, P. Wallis, &amp; A. Dix (Eds.), </span><span class="c14">Misuse and Abuse of Interactive Technologies</span><span>&nbsp;(pp. 21&ndash;24). Presented at the CHI 2006 Conference on Human Factors in Computing Systems, Montr&eacute;al Qu&eacute;bec Canada: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307563582&amp;usg=AOvVaw3oeYkNMTWXRBke9lQ8IvaO">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307563753&amp;usg=AOvVaw1lEzrDKPs4pDZsS8xYKeCb">https://doi.org/10.1145/1125451.1125753</a></span></p><p class="c3"><span>De Angeli, A., &amp; Carpenter, R. (2005). Stupid computer! Abuse and social identities. In A. De Angeli, S. Brahnam, &amp; P. Wallis (Eds.), </span><span class="c14">Proceedings of Abuse: The darker side of Human-Computer Interaction</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307564122&amp;usg=AOvVaw2ymvLL3-OoEgYf7dDJNlIy">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307564310&amp;usg=AOvVaw2Isv7tNFAksOBdAT3G2EVA">http://www.agentabuse.org/Abuse_Workshop_WS5.pdf</a></span></p><p class="c3"><span>De Montfort University. (2021). ETHICOMP.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.dmu.ac.uk/research/centres-institutes/ccsr/ethicomp.aspx&amp;sa=D&amp;source=editors&amp;ust=1660576307564625&amp;usg=AOvVaw3basm2c2LxSw4qDXIBKIO0">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.dmu.ac.uk/research/centres-institutes/ccsr/ethicomp.aspx&amp;sa=D&amp;source=editors&amp;ust=1660576307564827&amp;usg=AOvVaw2wRd-AtH8Hd__y1BU1Z65o">https://www.dmu.ac.uk/research/centres-institutes/ccsr/ethicomp.aspx</a></span><span class="c8 c12">. Accessed 6 October 2021</span></p><p class="c3"><span>Dennett, D. C. (1978). Current Issues in the Philosophy of Mind. </span><span class="c14">American Philosophical Quarterly</span><span>, </span><span class="c14">15</span><span class="c8 c12">(4), 249&ndash;261.</span></p><p class="c3"><span>Dennett, Daniel C. (1971). Intentional Systems. </span><span class="c14">The Journal of Philosophy</span><span>, </span><span class="c14">68</span><span>(4), 87&ndash;106.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2025382&amp;sa=D&amp;source=editors&amp;ust=1660576307565425&amp;usg=AOvVaw379HkLS6LRduCejJ6tq74q">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2025382&amp;sa=D&amp;source=editors&amp;ust=1660576307565616&amp;usg=AOvVaw1SMO1JUrdVgzTT0z13iyZh">https://doi.org/10.2307/2025382</a></span></p><p class="c3"><span>Dennett, Daniel C. (1994). The practical requirements for making a conscious robot | Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences. </span><span class="c14">Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences</span><span>, </span><span class="c14">349</span><span>(1689), 133&ndash;146.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1098/rsta.1994.0118&amp;sa=D&amp;source=editors&amp;ust=1660576307566022&amp;usg=AOvVaw3dtkjKQWHHIky0YCS0jpul">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1098/rsta.1994.0118&amp;sa=D&amp;source=editors&amp;ust=1660576307566198&amp;usg=AOvVaw0siD3AhRzUWSnEmITJSV3y">https://doi.org/10.1098/rsta.1994.0118</a></span></p><p class="c3"><span>Diderot, D. (2012). D&rsquo;Alembert&rsquo;s Dream. (I. Johnston, Trans.).</span><span><a class="c11" href="https://www.google.com/url?q=http://www.blc.arizona.edu/courses/schaffer/249/Before%2520Darwin%2520-%2520New/Diderot/Diderot,%2520D&#39;Alembert&#39;s%2520Dream.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307566571&amp;usg=AOvVaw0ihuti0zJORley-S1aZHJm">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.blc.arizona.edu/courses/schaffer/249/Before%2520Darwin%2520-%2520New/Diderot/Diderot,%2520D&#39;Alembert&#39;s%2520Dream.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307566966&amp;usg=AOvVaw3U03x2McuB0LcNx0tEOmkN">http://www.blc.arizona.edu/courses/schaffer/249/Before%20Darwin%20-%20New/Diderot/Diderot,%20D&#39;Alembert&#39;s%20Dream.htm</a></span><span class="c8 c12">. Accessed 25 June 2022</span></p><p class="c3"><span>Dolby, R. G. A. (1989). The possibility of computers becoming persons. </span><span class="c14">Social Epistemology</span><span>, </span><span class="c14">3</span><span>(4), 321&ndash;336.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728908578545&amp;sa=D&amp;source=editors&amp;ust=1660576307567643&amp;usg=AOvVaw1OlzEiiqK49Q2_NgxgovYX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728908578545&amp;sa=D&amp;source=editors&amp;ust=1660576307567879&amp;usg=AOvVaw1JxihF5-iUxPNEDyeRYj4G">https://doi.org/10.1080/02691728908578545</a></span></p><p class="c3"><span>Douglas, T., &amp; Savulescu, J. (2010). Synthetic biology and the ethics of knowledge. </span><span class="c14">Journal of medical ethics</span><span>, </span><span class="c14">36</span><span>(11), 687&ndash;693.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1136/jme.2010.038232&amp;sa=D&amp;source=editors&amp;ust=1660576307568645&amp;usg=AOvVaw3M-aqAdcWax0Bq7A3aKDEr">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1136/jme.2010.038232&amp;sa=D&amp;source=editors&amp;ust=1660576307568914&amp;usg=AOvVaw0ZmuZLL0a-lCPUl7LToXtO">https://doi.org/10.1136/jme.2010.038232</a></span></p><p class="c3"><span>Doyle, T. (2010). A Critique of Information Ethics. </span><span class="c14">Knowledge, Technology &amp; Policy</span><span>, </span><span class="c14">23</span><span>(1), 163&ndash;175.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12130-010-9104-x&amp;sa=D&amp;source=editors&amp;ust=1660576307569623&amp;usg=AOvVaw0jhs3A4-j5gwy-Z2-Kvc4y">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12130-010-9104-x&amp;sa=D&amp;source=editors&amp;ust=1660576307569911&amp;usg=AOvVaw0JaS-3_sR-9PYOKSIpf49S">https://doi.org/10.1007/s12130-010-9104-x</a></span></p><p class="c3"><span>Drozdek, A. (1994). To &lsquo;the possibility of computers becoming persons&rsquo; (1989). </span><span class="c14">Social Epistemology</span><span>, </span><span class="c14">8</span><span>(2), 177&ndash;197.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691729408578742&amp;sa=D&amp;source=editors&amp;ust=1660576307570589&amp;usg=AOvVaw1kM54bhDyBGH46o90k9S3Y">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691729408578742&amp;sa=D&amp;source=editors&amp;ust=1660576307570871&amp;usg=AOvVaw3hw90TeNDXEQhGPy_Wqscz">https://doi.org/10.1080/02691729408578742</a></span></p><p class="c3"><span>Duffy, B. R. (2003). Anthropomorphism and the social robot. </span><span class="c14">Robotics and Autonomous Systems</span><span>, </span><span class="c14">42</span><span>(3), 177&ndash;190.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/S0921-8890(02)00374-3&amp;sa=D&amp;source=editors&amp;ust=1660576307571648&amp;usg=AOvVaw0A_gB1ALIQvBgs3zl9jTs6">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/S0921-8890(02)00374-3&amp;sa=D&amp;source=editors&amp;ust=1660576307571936&amp;usg=AOvVaw0ka8nB7-S-INbwrdvUFRvL">https://doi.org/10.1016/S0921-8890(02)00374-3</a></span></p><p class="c3"><span>Duffy, B. R. (2006). Fundamental Issues in Social Robotics. </span><span class="c14">The International Review of Information Ethics</span><span>, </span><span class="c14">6</span><span>, 31&ndash;36.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie137&amp;sa=D&amp;source=editors&amp;ust=1660576307572623&amp;usg=AOvVaw3llu4hKWDK0XVEjb2Tvoy_">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie137&amp;sa=D&amp;source=editors&amp;ust=1660576307572889&amp;usg=AOvVaw1g5wdKtaGtOkmr1KiVanDw">https://doi.org/10.29173/irie137</a></span></p><p class="c3"><span>Elton, M. (2000). Should Vegetarians Play Video Games? </span><span class="c14">Philosophical Papers</span><span>, </span><span class="c14">29</span><span>(1), 21&ndash;42.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/05568640009506605&amp;sa=D&amp;source=editors&amp;ust=1660576307573582&amp;usg=AOvVaw0QsD74BXumBibGbJlRQoXq">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/05568640009506605&amp;sa=D&amp;source=editors&amp;ust=1660576307573865&amp;usg=AOvVaw056ia9XxKxlo33dE1HYALL">https://doi.org/10.1080/05568640009506605</a></span></p><p class="c3"><span>Epley, N., Waytz, A., &amp; Cacioppo, J. T. (2007). On seeing human: A threefactor theory of anthropomorphism. </span><span class="c14">Psychological Review</span><span>, </span><span class="c14">114</span><span>(4), 864&ndash;886.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/0033-295X.114.4.864&amp;sa=D&amp;source=editors&amp;ust=1660576307574552&amp;usg=AOvVaw0HrAkabfvOjzJeHczOAtrh">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/0033-295X.114.4.864&amp;sa=D&amp;source=editors&amp;ust=1660576307574842&amp;usg=AOvVaw1O7ZC5qcCvGWG1N7NAtKV5">https://doi.org/10.1037/0033-295X.114.4.864</a></span></p><p class="c3"><span>Farmer, J. D., &amp; Belin, A. d&rsquo;A. (1990). </span><span class="c14">Artificial life: The coming evolution</span><span>&nbsp;(No. LA-UR-90-378; CONF-891131-). Los Alamos National Lab. (LANL), Los Alamos, NM (United States).</span><span><a class="c11" href="https://www.google.com/url?q=https://www.osti.gov/biblio/7043104&amp;sa=D&amp;source=editors&amp;ust=1660576307575442&amp;usg=AOvVaw0B98jypeT_DiqzJfT6PA6-">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.osti.gov/biblio/7043104&amp;sa=D&amp;source=editors&amp;ust=1660576307575726&amp;usg=AOvVaw3xOe_UWKY-RL6SFlyICIzk">https://www.osti.gov/biblio/7043104</a></span><span class="c8 c12">. Accessed 29 November 2021</span></p><p class="c3"><span>Fiedler, F. A., &amp; Reynolds, G. H. (1993). Legal Problems of Nanotechnology: An Overview. </span><span class="c14">Southern California Interdisciplinary Law Journal</span><span>, </span><span class="c14">3</span><span class="c8 c12">, 593.</span></p><p class="c3"><span>Fields, C. (1987). Human&#8208;computer interaction: A critical synthesis. </span><span class="c14">Social Epistemology</span><span>, </span><span class="c14">1</span><span>(1), 5&ndash;25.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728708578410&amp;sa=D&amp;source=editors&amp;ust=1660576307576704&amp;usg=AOvVaw34bED6e3y01vrCgiWj7bsZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/02691728708578410&amp;sa=D&amp;source=editors&amp;ust=1660576307576994&amp;usg=AOvVaw3zYO6uRzlYzF07CqqNNBNV">https://doi.org/10.1080/02691728708578410</a></span></p><p class="c3"><span>Fire, M., &amp; Guestrin, C. (2019). Over-optimization of academic publishing metrics: observing Goodhart&rsquo;s Law in action. </span><span class="c14">GigaScience</span><span>, </span><span class="c14">8</span><span>(6), giz053.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/gigascience/giz053&amp;sa=D&amp;source=editors&amp;ust=1660576307577665&amp;usg=AOvVaw2060tTptq1NyC5rYl_MCQN">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/gigascience/giz053&amp;sa=D&amp;source=editors&amp;ust=1660576307577947&amp;usg=AOvVaw2-6z5j6ygNLhRz5CCqqWUJ">https://doi.org/10.1093/gigascience/giz053</a></span></p><p class="c3"><span>Floridi, L. (1996a). Brave.Net.World: the Internet as a disinformation superhighway? </span><span class="c14">The Electronic Library</span><span>, </span><span class="c14">14</span><span>(6), 509&ndash;514.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1108/eb045517&amp;sa=D&amp;source=editors&amp;ust=1660576307578643&amp;usg=AOvVaw3Sv-AjLHpBXEyyCbuwBt58">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1108/eb045517&amp;sa=D&amp;source=editors&amp;ust=1660576307578932&amp;usg=AOvVaw12Rge8nOZIMQR8_kI-dDi6">https://doi.org/10.1108/eb045517</a></span></p><p class="c3"><span>Floridi, L. (1996b). Internet: Which Future for Organized Knowledge, Frankenstein or Pygmalion? </span><span class="c14">The Information Society</span><span>, </span><span class="c14">12</span><span>(1), 5&ndash;16.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/019722496129675&amp;sa=D&amp;source=editors&amp;ust=1660576307579598&amp;usg=AOvVaw2751WNPoAB6Lk3Pxb7FGUZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/019722496129675&amp;sa=D&amp;source=editors&amp;ust=1660576307579799&amp;usg=AOvVaw3sOWKUrBYwAUB6kd4mUBUs">https://doi.org/10.1080/019722496129675</a></span></p><p class="c3"><span>Floridi, L. (1998a). Does Information Have a Moral Worth in Itself? In </span><span class="c14">Computer Ethics: Philosophical Enquiry (CEPE&rsquo;98) in Association with the ACM SIG on Computers and Society</span><span>. London School of Economics and Political Science, London.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2139/ssrn.144548&amp;sa=D&amp;source=editors&amp;ust=1660576307580201&amp;usg=AOvVaw33c7WibN41YjeC6z-8z8PN">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2139/ssrn.144548&amp;sa=D&amp;source=editors&amp;ust=1660576307580380&amp;usg=AOvVaw1nSt0UIPVlEEfMg2cGzRp5">https://doi.org/10.2139/ssrn.144548</a></span></p><p class="c3"><span>Floridi, L. (1998b). Information Ethics: On the Philosophical Foundation of Computer Ethics. In J. van den Hoven, S. Rogerson, T. W. Bynum, &amp; D. Gotterbarn (Eds.), </span><span class="c14">Proceedings of the Fourth International Conference on Ethical Issues of Information Technology</span><span class="c8 c12">. Rotterdam, The Netherlands.</span></p><p class="c3"><span>Floridi, L. (1999). Information ethics: On the philosophical foundation of computer ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">1</span><span>(1), 33&ndash;52.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1010018611096&amp;sa=D&amp;source=editors&amp;ust=1660576307580943&amp;usg=AOvVaw2yk9iFKlkJi9XkXE2evSba">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1010018611096&amp;sa=D&amp;source=editors&amp;ust=1660576307581164&amp;usg=AOvVaw34lrCXhFdnG0EF8JgB0jbr">https://doi.org/10.1023/A:1010018611096</a></span></p><p class="c3"><span>Floridi, L. (2002). On the intrinsic value of information objects and the infosphere. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">4</span><span>(4), 287&ndash;304.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1021342422699&amp;sa=D&amp;source=editors&amp;ust=1660576307581813&amp;usg=AOvVaw044Vd_ULtvHvedjVk0ZTDC">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1021342422699&amp;sa=D&amp;source=editors&amp;ust=1660576307582085&amp;usg=AOvVaw0LoXb5lrNHwv99D_L1rhNn">https://doi.org/10.1023/A:1021342422699</a></span></p><p class="c3"><span>Floridi, L. (2006). Information ethics, its nature and scope. </span><span class="c14">ACM SIGCAS Computers and Society</span><span>, </span><span class="c14">36</span><span>(3), 21&ndash;36.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1195716.1195719&amp;sa=D&amp;source=editors&amp;ust=1660576307582612&amp;usg=AOvVaw2J5fk-oy6Srejo_6k6WiMI">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1195716.1195719&amp;sa=D&amp;source=editors&amp;ust=1660576307582868&amp;usg=AOvVaw3nAgvIWS3OYgcllWEbx5x9">https://doi.org/10.1145/1195716.1195719</a></span></p><p class="c3"><span>Floridi, L. (2010a). </span><span class="c14">Information: A Very Short Introduction</span><span class="c8 c12">. OUP Oxford.</span></p><p class="c3"><span>Floridi, L. (2010b). </span><span class="c14">The Cambridge Handbook of Information and Computer Ethics</span><span class="c8 c12">. Cambridge University Press.</span></p><p class="c3"><span>Floridi, L. (2011). </span><span class="c14">The fourth technological revolution</span><span>. TEDxMaastricht.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3Dc-kJsyU8tgI%26ab_channel%3DTEDxTalks&amp;sa=D&amp;source=editors&amp;ust=1660576307583500&amp;usg=AOvVaw1PUO8V_CYN2aR01vNCif9h">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3Dc-kJsyU8tgI%26ab_channel%3DTEDxTalks&amp;sa=D&amp;source=editors&amp;ust=1660576307583717&amp;usg=AOvVaw2vnfJ__0IMeQ19StCPVfMJ">https://www.youtube.com/watch?v=c-kJsyU8tgI&amp;ab_channel=TEDxTalks</a></span><span class="c8 c12">. Accessed 10 November 2021</span></p><p class="c3"><span>Floridi, L. (2013). </span><span class="c14">The Ethics of Information</span><span class="c8 c12">. OUP Oxford.</span></p><p class="c3"><span>Floridi, L. (2017a). Robots, Jobs, Taxes, and Responsibilities. </span><span class="c14">Philosophy &amp; Technology</span><span>, </span><span class="c14">30</span><span>(1), 1&ndash;4.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-017-0257-3&amp;sa=D&amp;source=editors&amp;ust=1660576307584246&amp;usg=AOvVaw2EMcjx1PRtOv6UxcQ5srIk">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-017-0257-3&amp;sa=D&amp;source=editors&amp;ust=1660576307584422&amp;usg=AOvVaw0kB2KxfPDC2n6i3fQaEEFR">https://doi.org/10.1007/s13347-017-0257-3</a></span></p><p class="c3"><span>Floridi, L. (2017b, February 22). Roman law offers a better guide to robot rights than sci-fi. </span><span class="c14">Financial Times</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.ft.com/content/99d60326-f85d-11e6-bd4e-68d53499ed71&amp;sa=D&amp;source=editors&amp;ust=1660576307584773&amp;usg=AOvVaw3tnfJQxzR3IkFg9dEs10ww">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.ft.com/content/99d60326-f85d-11e6-bd4e-68d53499ed71&amp;sa=D&amp;source=editors&amp;ust=1660576307584966&amp;usg=AOvVaw3Uvm_lSUKez5qh47ZEjALj">https://www.ft.com/content/99d60326-f85d-11e6-bd4e-68d53499ed71</a></span><span class="c8 c12">. Accessed 10 November 2021</span></p><p class="c3"><span>Floridi, L., &amp; Sanders, J. W. (2001). Artificial evil and the foundation of computer ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">3</span><span>(1), 55&ndash;66.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1011440125207&amp;sa=D&amp;source=editors&amp;ust=1660576307585327&amp;usg=AOvVaw1V-tSsFh4Lmg_rpFVOls0a">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1011440125207&amp;sa=D&amp;source=editors&amp;ust=1660576307585503&amp;usg=AOvVaw3xL1jQ4kJq9QRGFOKg4Bv6">https://doi.org/10.1023/A:1011440125207</a></span></p><p class="c3"><span>Floridi, L., &amp; Sanders, J. W. (2002). Mapping the foundationalist debate in computer ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">4</span><span>(1), 1&ndash;9.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1015209807065&amp;sa=D&amp;source=editors&amp;ust=1660576307585944&amp;usg=AOvVaw1BpILhVG9-sRnrugHkYy0x">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1015209807065&amp;sa=D&amp;source=editors&amp;ust=1660576307586191&amp;usg=AOvVaw0vtHaUPigNt9A4n570lzPg">https://doi.org/10.1023/A:1015209807065</a></span></p><p class="c3"><span>Floridi, L., &amp; Sanders, J. W. (2004). On the Morality of Artificial Agents. </span><span class="c14">Minds and Machines</span><span>, </span><span class="c14">14</span><span>(3), 349&ndash;379.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/B:MIND.0000035461.63578.9d&amp;sa=D&amp;source=editors&amp;ust=1660576307586806&amp;usg=AOvVaw2joDXRlAe0MwlKLQv0hvl1">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/B:MIND.0000035461.63578.9d&amp;sa=D&amp;source=editors&amp;ust=1660576307587074&amp;usg=AOvVaw3y2MAqgK0_7flD6dePYDpT">https://doi.org/10.1023/B:MIND.0000035461.63578.9d</a></span></p><p class="c3"><span>Floridi, L., &amp; Taddeo, M. (2018). Romans would have denied robots legal personhood. </span><span class="c14">Nature</span><span>, </span><span class="c14">557</span><span>(7705), 309&ndash;309.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1038/d41586-018-05154-5&amp;sa=D&amp;source=editors&amp;ust=1660576307587724&amp;usg=AOvVaw1PwrjpgTAABoJw98kyN2zx">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1038/d41586-018-05154-5&amp;sa=D&amp;source=editors&amp;ust=1660576307587987&amp;usg=AOvVaw0-QNISeEM1lUxtRxSSBfKE">https://doi.org/10.1038/d41586-018-05154-5</a></span></p><p class="c3"><span>Freier, N. G. (2008). Children attribute moral standing to a personified agent. In </span><span class="c14">Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems - CHI &rsquo;08</span><span>&nbsp;(pp. 343&ndash;352). Presented at the Proceeding of the twenty-sixth annual CHI conference, Florence, Italy: ACM Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1357054.1357113&amp;sa=D&amp;source=editors&amp;ust=1660576307588560&amp;usg=AOvVaw1lKY3g81GC_mGvatrp7uSB">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1357054.1357113&amp;sa=D&amp;source=editors&amp;ust=1660576307588822&amp;usg=AOvVaw0-FcRB1zj-GI-zFMH2_Iuf">https://doi.org/10.1145/1357054.1357113</a></span></p><p class="c3"><span>Freitas, R. A. (1985). Legal Rights of Robots. </span><span class="c14">Student Lawyer</span><span>, </span><span class="c14">13</span><span class="c8 c12">, 54&ndash;56.</span></p><p class="c3"><span>Frickel, S., &amp; Gross, N. (2005). A General Theory of Scientific/Intellectual Movements. </span><span class="c14">American Sociological Review</span><span>, </span><span class="c14">70</span><span>(2), 204&ndash;232.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/000312240507000202&amp;sa=D&amp;source=editors&amp;ust=1660576307589782&amp;usg=AOvVaw12alF2pgN8N3vh_fmqHM2S">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/000312240507000202&amp;sa=D&amp;source=editors&amp;ust=1660576307590059&amp;usg=AOvVaw3DGorIQPSKz1BvVZZP23Il">https://doi.org/10.1177/000312240507000202</a></span></p><p class="c3"><span>Friedman, B. (1995). It&rsquo;s the computer&rsquo;s fault: reasoning about computers as moral agents. In </span><span class="c14">Conference Companion on Human Factors in Computing Systems</span><span>&nbsp;(pp. 226&ndash;227). New York, NY, USA: Association for Computing Machinery.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/223355.223537&amp;sa=D&amp;source=editors&amp;ust=1660576307590664&amp;usg=AOvVaw3IkQBV1a9waScPdLqpmXLx">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/223355.223537&amp;sa=D&amp;source=editors&amp;ust=1660576307590938&amp;usg=AOvVaw2_ssIpKTw8c1iBY986QHFy">https://doi.org/10.1145/223355.223537</a></span></p><p class="c3"><span>Friedman, B., Kahn, P. H., &amp; Hagman, J. (2003). Hardware companions? what online AIBO discussion forums reveal about the human-robotic relationship. In </span><span class="c14">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</span><span>&nbsp;(pp. 273&ndash;280). New York, NY, USA: Association for Computing Machinery.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/642611.642660&amp;sa=D&amp;source=editors&amp;ust=1660576307591555&amp;usg=AOvVaw1E0MlWurEigawsIdvQhUxR">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/642611.642660&amp;sa=D&amp;source=editors&amp;ust=1660576307591816&amp;usg=AOvVaw14dpSFEMNHg5IlQmLakLQO">https://doi.org/10.1145/642611.642660</a></span></p><p class="c3"><span>Froehlich, T. (2004). A brief history of information ethics. </span><span class="c14">BiD: textos universitaris de biblioteconomia i documentaci&oacute;</span><span>, </span><span class="c14">13</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://bid.ub.edu/13froel2.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307592469&amp;usg=AOvVaw38_818TZ7uT0eRqN_5Cano">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://bid.ub.edu/13froel2.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307592730&amp;usg=AOvVaw0jk3v7f4jKdDioLMCUwgmW">http://bid.ub.edu/13froel2.htm</a></span><span class="c8 c12">. Accessed 29 December 2021</span></p><p class="c3"><span>Gamez, D. (2008). Progress in machine consciousness. </span><span class="c14">Consciousness and Cognition</span><span>, </span><span class="c14">17</span><span>(3), 887&ndash;910.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.concog.2007.04.005&amp;sa=D&amp;source=editors&amp;ust=1660576307593387&amp;usg=AOvVaw3nTDYo3HqCNTuWeR90qyfj">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.concog.2007.04.005&amp;sa=D&amp;source=editors&amp;ust=1660576307593665&amp;usg=AOvVaw1CL0SDW2KUr4q1rwhr3t0J">https://doi.org/10.1016/j.concog.2007.04.005</a></span></p><p class="c3"><span>Gandon, F. L. (2003). Combining reactive and deliberative agents for complete ecosystems in infospheres. In </span><span class="c14">IEEE/WIC International Conference on Intelligent Agent Technology, 2003. IAT 2003.</span><span>&nbsp;(pp. 297&ndash;303). Presented at the IEEE/WIC International Conference on Intelligent Agent Technology, 2003. IAT 2003.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/IAT.2003.1241082&amp;sa=D&amp;source=editors&amp;ust=1660576307594276&amp;usg=AOvVaw3sreoCvtPWSloY_f_u1cPe">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/IAT.2003.1241082&amp;sa=D&amp;source=editors&amp;ust=1660576307594574&amp;usg=AOvVaw0vzWrNnzWqxfS2ArWQtFWo">https://doi.org/10.1109/IAT.2003.1241082</a></span></p><p class="c3"><span>Gellers, J. C. (2020). </span><span class="c14">Rights for Robots: Artificial Intelligence, Animal and Environmental Law</span><span>&nbsp;(1st ed.). London, UK: Routledge.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4324/9780429288159&amp;sa=D&amp;source=editors&amp;ust=1660576307595105&amp;usg=AOvVaw3PMZVs1tt3oOiPgpgtbylv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4324/9780429288159&amp;sa=D&amp;source=editors&amp;ust=1660576307595405&amp;usg=AOvVaw0YvSaSjzz4o-ARev2N681c">https://doi.org/10.4324/9780429288159</a></span></p><p class="c3"><span>Goldie, P. (2010). The Moral Risks of Risky Technologies. In S. Roeser (Ed.), </span><span class="c14">Emotions and Risky Technologies</span><span>&nbsp;(pp. 127&ndash;138). Dordrecht: Springer Netherlands.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-90-481-8647-1_8&amp;sa=D&amp;source=editors&amp;ust=1660576307596016&amp;usg=AOvVaw2BX4ZeN_TUVyxFVLxRv-jc">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-90-481-8647-1_8&amp;sa=D&amp;source=editors&amp;ust=1660576307596336&amp;usg=AOvVaw2roBxQhjUBM_FWds53Ni0V">https://doi.org/10.1007/978-90-481-8647-1_8</a></span></p><p class="c3"><span>Goodrich, M. A., &amp; Schultz, A. C. (2008). Human&ndash;Robot Interaction: A Survey. </span><span class="c14">Foundations and Trends in Human&ndash;Computer Interaction</span><span>, </span><span class="c14">1</span><span>(3), 203&ndash;275.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1561/1100000005&amp;sa=D&amp;source=editors&amp;ust=1660576307597022&amp;usg=AOvVaw2Q215XDTTWtYafYPWIUwbb">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1561/1100000005&amp;sa=D&amp;source=editors&amp;ust=1660576307597294&amp;usg=AOvVaw3kTn5ZyLIyH4LPEtN4CqLC">https://doi.org/10.1561/1100000005</a></span></p><p class="c3"><span>Google Scholar. (2021a). (&ldquo;Mindcrime&rdquo; OR &ldquo;mind crime&rdquo; OR &ldquo;mind-crime&rdquo;) AND Bostrom.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/scholar?start%3D0%26q%3D(%2522Mindcrime%2522%2BOR%2B%2522mind%2Bcrime%2522%2BOR%2B%2522mind-crime%2522)%2BAND%2BBostrom%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;source=editors&amp;ust=1660576307597903&amp;usg=AOvVaw1uLShKpfdYB7b8CXNv12HA">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/scholar?start%3D0%26q%3D(%2522Mindcrime%2522%2BOR%2B%2522mind%2Bcrime%2522%2BOR%2B%2522mind-crime%2522)%2BAND%2BBostrom%26hl%3Den%26as_sdt%3D0,5&amp;sa=D&amp;source=editors&amp;ust=1660576307598324&amp;usg=AOvVaw1K0rwJPXLcIJEaiHViArLc">https://scholar.google.com/scholar?start=0&amp;q=(%22Mindcrime%22+OR+%22mind+crime%22+OR+%22mind-crime%22)+AND+Bostrom&amp;hl=en&amp;as_sdt=0,5</a></span></p><p class="c3"><span>Google Scholar. (2021b). Luciano Floridi.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.co.uk/citations?user%3DjZdTOaoAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307598888&amp;usg=AOvVaw1XxV56clI5T22b1_HjQV3q">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.co.uk/citations?user%3DjZdTOaoAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307599216&amp;usg=AOvVaw0gTp337wOmjd2JMKuBT3Wq">https://scholar.google.co.uk/citations?user=jZdTOaoAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 10 November 2021</span></p><p class="c3"><span>Google Scholar. (2021c). Mel Slater.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D5gGSgcUAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307599756&amp;usg=AOvVaw1fPNdnbTLpsUHlOtR5Qt00">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D5gGSgcUAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307600083&amp;usg=AOvVaw16Eec7r1ar4ZA2zQsxatcR">https://scholar.google.com/citations?user=5gGSgcUAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 14 December 2021</span></p><p class="c3"><span>Google Scholar. (2021d). suffering subroutines.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/scholar?hl%3Den%26as_sdt%3D0%252C5%26q%3D%2522suffering%2Bsubroutines%2522%26btnG%3D&amp;sa=D&amp;source=editors&amp;ust=1660576307600642&amp;usg=AOvVaw34Qxh77oOpTrPyO-6xelaq">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/scholar?hl%3Den%26as_sdt%3D0%252C5%26q%3D%2522suffering%2Bsubroutines%2522%26btnG%3D&amp;sa=D&amp;source=editors&amp;ust=1660576307601011&amp;usg=AOvVaw3Ff04f-yO9FLwwUPMbQOxu">https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=%22suffering+subroutines%22&amp;btnG=</a></span></p><p class="c3"><span>Google Scholar. </span><span>(2021e)</span><span>. Batya Friedman.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DdkjR4cAAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307601659&amp;usg=AOvVaw2wzwB_TkYwxwidXHoHfRyX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DdkjR4cAAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307601980&amp;usg=AOvVaw07ChvKe20_97ROZuKFwaHv">https://scholar.google.com/citations?user=dkjR4cAAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 15 December 2021</span></p><p class="c3"><span>Google Scholar. (2021f). Christoph Bartneck.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DNrcTgeUAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307602493&amp;usg=AOvVaw3cJvInwP1Q1PE6uG0nn8I8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DNrcTgeUAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307602809&amp;usg=AOvVaw3TI2DUqQUjVNlKF55y4qBs">https://scholar.google.com/citations?user=NrcTgeUAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 7 December 2021</span></p><p class="c3"><span>Google Scholar. (2021g). Daniel C. Dennett.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D3FWe5OQAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307603226&amp;usg=AOvVaw0S1esomfdtp27VNUnDAGfC">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D3FWe5OQAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307603434&amp;usg=AOvVaw3NcieEivIT0uVjy6N957FO">https://scholar.google.com/citations?user=3FWe5OQAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 19 November 2021</span></p><p class="c3"><span>Google Scholar. (2021h). &#8234;Frankenstein unbound: Towards a legal definition of artificial intelligence&#8236;.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?view_op%3Dview_citation%26hl%3Den%26user%3DYln8YccAAAAJ%26citation_for_view%3DYln8YccAAAAJ:9yKSN-GCB0IC&amp;sa=D&amp;source=editors&amp;ust=1660576307603856&amp;usg=AOvVaw1d6v4gv2GGHUWPcCyr-zJ8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?view_op%3Dview_citation%26hl%3Den%26user%3DYln8YccAAAAJ%26citation_for_view%3DYln8YccAAAAJ:9yKSN-GCB0IC&amp;sa=D&amp;source=editors&amp;ust=1660576307604116&amp;usg=AOvVaw3hQFHtc_-CKTM3MswhdFh2">https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=Yln8YccAAAAJ&amp;citation_for_view=Yln8YccAAAAJ:9yKSN-GCB0IC</a></span><span class="c8 c12">. Accessed 12 November 2021</span></p><p class="c3"><span>Google Scholar. (2021i). Johnny Hartz S&oslash;raker.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DZiW2NWoAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307604441&amp;usg=AOvVaw2P-8blr25Xpxzwm4nkGF15">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DZiW2NWoAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307604645&amp;usg=AOvVaw1LwA_3TioRdjr40OMMwWvJ">https://scholar.google.com/citations?user=ZiW2NWoAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 20 December 2021</span></p><p class="c3"><span>Google Scholar. (2021j). Lawrence Solum.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DvXYJjpEAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307604943&amp;usg=AOvVaw00Nni5qp-mwzdsVaiuddX_">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DvXYJjpEAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307605199&amp;usg=AOvVaw0ELpgzKWg6iORb2YPgE2gg">https://scholar.google.com/citations?user=vXYJjpEAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 16 November 2021</span></p><p class="c3"><span>Google Scholar. (2021k). Robert A. Freitas Jr.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DDpoSX2QAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307605676&amp;usg=AOvVaw3Jkhy5KZBqqbZkxJL9ZI-o">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DDpoSX2QAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307605892&amp;usg=AOvVaw2GHr8cNglXfEHVFMCbnt9n">https://scholar.google.com/citations?user=DpoSX2QAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 12 November 2021</span></p><p class="c3"><span>Google Scholar. (2021l). sohail inayatullah.</span><span><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com.au/citations?user%3DgB0Ea_wAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307606273&amp;usg=AOvVaw0fs_F921p_VSAVXRw4-7Cs">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://scholar.google.com.au/citations?user%3DgB0Ea_wAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1660576307606547&amp;usg=AOvVaw21INN2HcRI-9_VM9QrsuCY">https://scholar.google.com.au/citations?user=gB0Ea_wAAAAJ&amp;hl=en</a></span><span class="c8 c12">. Accessed 16 November 2021</span></p><p class="c3"><span>Gordon, J.-S. (Ed.). (2020). </span><span class="c14">Smart Technologies and Fundamental Rights</span><span>. Leiden, The Netherlands: Brill.</span><span><a class="c11" href="https://www.google.com/url?q=https://brill.com/view/title/55392&amp;sa=D&amp;source=editors&amp;ust=1660576307606930&amp;usg=AOvVaw3cQSEeJr6fB9usXom7ooNq">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://brill.com/view/title/55392&amp;sa=D&amp;source=editors&amp;ust=1660576307607105&amp;usg=AOvVaw1TH4kQ-74AcH8Y74zsxiuY">https://brill.com/view/title/55392</a></span><span class="c8 c12">. Accessed 3 January 2022</span></p><p class="c3"><span>Gray, H. M., Gray, K., &amp; Wegner, D. M. (2007). Dimensions of Mind Perception. </span><span class="c14">Science</span><span>, </span><span class="c14">315</span><span>(5812), 619&ndash;619.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1126/science.1134475&amp;sa=D&amp;source=editors&amp;ust=1660576307607486&amp;usg=AOvVaw2y1go2DwbSeiYx2Zq1bYA3">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1126/science.1134475&amp;sa=D&amp;source=editors&amp;ust=1660576307607659&amp;usg=AOvVaw0AaMZEP-YBTlHto4mPQ_4o">https://doi.org/10.1126/science.1134475</a></span></p><p class="c3"><span>Gray, K., &amp; Wegner, D. M. (2009). Moral typecasting: Divergent perceptions of moral agents and moral patients. </span><span class="c14">Journal of Personality and Social Psychology</span><span>, </span><span class="c14">96</span><span>(3), 505&ndash;520.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/a0013748&amp;sa=D&amp;source=editors&amp;ust=1660576307608035&amp;usg=AOvVaw2nV4O-v3TvN96yp3PcJCfi">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/a0013748&amp;sa=D&amp;source=editors&amp;ust=1660576307608197&amp;usg=AOvVaw3zAYZSaUBv-1Kqs53PuC8o">https://doi.org/10.1037/a0013748</a></span></p><p class="c3"><span>Guither, H. D. (1998). </span><span class="c14">Animal Rights: History and Scope of a Radical Social Movement</span><span class="c8 c12">. SIU Press.</span></p><p class="c3"><span>Gunkel, D., Bryson, J., &amp; Torrance, S. (Eds.). (2012). </span><span class="c14">The Machine Question: AI, Ethics and Moral Responsibility</span><span>. Birmingham, UK: AISB.</span><span><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.446.9723%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307608693&amp;usg=AOvVaw1E5oXrVMl23quSJgXUQYYz">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.446.9723%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307608934&amp;usg=AOvVaw1Sk8bLSFQHcQbMXxE0YKYl">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9723&amp;rep=rep1&amp;type=pdf</a></span></p><p class="c3"><span>Gunkel, D. J. (2006). The Machine Question: Ethics, Alterity, and Technology. </span><span class="c14">Explorations in Media Ecology</span><span>, </span><span class="c14">5</span><span>(4), 259&ndash;278.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1386/eme.5.4.259_1&amp;sa=D&amp;source=editors&amp;ust=1660576307609356&amp;usg=AOvVaw08NiYqrqd31-yYDpvVAmtq">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1386/eme.5.4.259_1&amp;sa=D&amp;source=editors&amp;ust=1660576307609540&amp;usg=AOvVaw3C37qgS6-Z639ut5EKLj1r">https://doi.org/10.1386/eme.5.4.259_1</a></span></p><p class="c3"><span>Gunkel, D. J. (2012). </span><span class="c14">The Machine Question: Critical Perspectives on AI, Robots, and Ethics</span><span class="c8 c12">. MIT Press.</span></p><p class="c3"><span>Gunkel, D. J. (2013). Mark Coeckelbergh: Growing moral relations: critique of moral status ascription. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">15</span><span>(3), 239&ndash;241.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9308-8&amp;sa=D&amp;source=editors&amp;ust=1660576307610032&amp;usg=AOvVaw0t6bJzuk9iiM5NHVpmaoWP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9308-8&amp;sa=D&amp;source=editors&amp;ust=1660576307610197&amp;usg=AOvVaw0XnVrJIrmP-RXHw6WDDpih">https://doi.org/10.1007/s10676-012-9308-8</a></span></p><p class="c3"><span>Gunkel, D. J. (2014). A Vindication of the Rights of Machines. </span><span class="c14">Philosophy &amp; Technology</span><span>, </span><span class="c14">27</span><span>(1), 113&ndash;132.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0121-z&amp;sa=D&amp;source=editors&amp;ust=1660576307610571&amp;usg=AOvVaw30yDredSJRLXX92_maDpvV">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s13347-013-0121-z&amp;sa=D&amp;source=editors&amp;ust=1660576307610739&amp;usg=AOvVaw3z1fDma6s_7DxepTBfpbC6">https://doi.org/10.1007/s13347-013-0121-z</a></span></p><p class="c3"><span>Gunkel, D. J. (2015). The Rights of Machines: Caring for Robotic Care-Givers. In S. P. van Rysewyk &amp; M. Pontier (Eds.), </span><span class="c14">Machine Medical Ethics</span><span>&nbsp;(pp. 151&ndash;166). Cham: Springer International Publishing.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-3-319-08108-3_10&amp;sa=D&amp;source=editors&amp;ust=1660576307611068&amp;usg=AOvVaw3gDAfUu0qcdWM6FBQz86Lw">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-3-319-08108-3_10&amp;sa=D&amp;source=editors&amp;ust=1660576307611241&amp;usg=AOvVaw14aA_qmybETl2wtcI6ACjl">https://doi.org/10.1007/978-3-319-08108-3_10</a></span></p><p class="c3"><span>Gunkel, D. J. (2016). David J. Gunkel - Information.</span><span><a class="c11" href="https://www.google.com/url?q=http://gunkelweb.com/info.html&amp;sa=D&amp;source=editors&amp;ust=1660576307611513&amp;usg=AOvVaw011hyGAhlPW5Nsm4mknbnT">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://gunkelweb.com/info.html&amp;sa=D&amp;source=editors&amp;ust=1660576307611711&amp;usg=AOvVaw1CqAnVUmUz0Z61cOeAiL1q">http://gunkelweb.com/info.html</a></span><span class="c8 c12">. Accessed 22 September 2021</span></p><p class="c3"><span>Gunkel, D. J. (2018). </span><span class="c14">Robot Rights</span><span class="c8 c12">. Cambridge, MA: MIT Press.</span></p><p class="c3"><span>Gunkel, D. J. (2020). Shifting Perspectives. </span><span class="c14">Science and Engineering Ethics</span><span>, </span><span class="c14">26</span><span>(5), 2527&ndash;2532.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-020-00247-9&amp;sa=D&amp;source=editors&amp;ust=1660576307612207&amp;usg=AOvVaw2bkERLj-4N0URL6_U10ARL">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-020-00247-9&amp;sa=D&amp;source=editors&amp;ust=1660576307612380&amp;usg=AOvVaw0rKVjHsGuH4CEsJDIptZyz">https://doi.org/10.1007/s11948-020-00247-9</a></span></p><p class="c3"><span>Hajdin, M. (1987). </span><span class="c14">Agents, Patients, and Moral Discourse</span><span>. McGill University. Retrieved from</span><span><a class="c11" href="https://www.google.com/url?q=https://central.bac-lac.gc.ca/.item?id%3DTC-QMM-75751%26op%3Dpdf%26app%3DLibrary%26oclc_number%3D897472150&amp;sa=D&amp;source=editors&amp;ust=1660576307612988&amp;usg=AOvVaw1Gv0kzVbLqNSLE2URXEkGQ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://central.bac-lac.gc.ca/.item?id%3DTC-QMM-75751%26op%3Dpdf%26app%3DLibrary%26oclc_number%3D897472150&amp;sa=D&amp;source=editors&amp;ust=1660576307613219&amp;usg=AOvVaw2HJtKKWubwsvLwXAitfrzL">https://central.bac-lac.gc.ca/.item?id=TC-QMM-75751&amp;op=pdf&amp;app=Library&amp;oclc_number=897472150</a></span></p><p class="c3"><span>Hale, B. (2009). Technology, the Environment and the Moral Considerability of Artefacts. In J. K. B. Olsen, E. Selinger, &amp; S. Riis (Eds.), </span><span class="c14">New Waves in Philosophy of Technology</span><span>&nbsp;(pp. 216&ndash;240). London: Palgrave Macmillan UK.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1057/9780230227279_11&amp;sa=D&amp;source=editors&amp;ust=1660576307613574&amp;usg=AOvVaw3g61fPJ82WhXFjlgr87_7L">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1057/9780230227279_11&amp;sa=D&amp;source=editors&amp;ust=1660576307613741&amp;usg=AOvVaw0RewTDOB6BF_0a6inoGoft">https://doi.org/10.1057/9780230227279_11</a></span></p><p class="c3"><span>Hall, J. S. (2000). Ethics for Machines.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.kurzweilai.net/ethics-for-machines&amp;sa=D&amp;source=editors&amp;ust=1660576307614028&amp;usg=AOvVaw3pzOSP3DSRgnUVyt9xxk2w">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.kurzweilai.net/ethics-for-machines&amp;sa=D&amp;source=editors&amp;ust=1660576307614239&amp;usg=AOvVaw0qhIfbfD9JjQWNtz0aMvUI">https://www.kurzweilai.net/ethics-for-machines</a></span><span class="c8 c12">. Accessed 22 September 2021</span></p><p class="c3"><span>Harris, J. (2019, May 17). What can the farmed animal movement learn from history? </span><span class="c14">Sentience Institute</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.sentienceinstitute.org/blog/what-can-the-farmed-animal-movement-learn-from-history&amp;sa=D&amp;source=editors&amp;ust=1660576307614734&amp;usg=AOvVaw2TWHONY5cHp0D07hIUV4OC">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.sentienceinstitute.org/blog/what-can-the-farmed-animal-movement-learn-from-history&amp;sa=D&amp;source=editors&amp;ust=1660576307614980&amp;usg=AOvVaw0t4dxLexYRKJtwSPCjNHMm">http://www.sentienceinstitute.org/blog/what-can-the-farmed-animal-movement-learn-from-history</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Harris, J. (2021). The Importance of Artificial Sentience. </span><span class="c14">Sentience Institute</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience&amp;sa=D&amp;source=editors&amp;ust=1660576307615338&amp;usg=AOvVaw0XfvSJUYvc3EQXeGwqpJQm">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience&amp;sa=D&amp;source=editors&amp;ust=1660576307615581&amp;usg=AOvVaw3zpgYKHv3ZhEb_v0l4vGtF">http://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience</a></span><span class="c8 c12">. Accessed 30 December 2021</span></p><p class="c3"><span>Harris, J., &amp; Anthis, J. R. (2021). The Moral Consideration of Artificial Entities: A Literature Review. </span><span class="c14">Science and Engineering Ethics</span><span>, </span><span class="c14">27</span><span>(4), 53.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-021-00331-8&amp;sa=D&amp;source=editors&amp;ust=1660576307615993&amp;usg=AOvVaw0Wt1ia2v4nyT3a9luWlC5R">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-021-00331-8&amp;sa=D&amp;source=editors&amp;ust=1660576307616175&amp;usg=AOvVaw0v8TW4Ja9xhUJBLnNRTsn5">https://doi.org/10.1007/s11948-021-00331-8</a></span></p><p class="c3"><span>Harrison, P. (1992). Descartes on Animals. </span><span class="c14">The Philosophical Quarterly</span><span>, </span><span class="c14">42</span><span>(167), 219&ndash;227.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2220217&amp;sa=D&amp;source=editors&amp;ust=1660576307616548&amp;usg=AOvVaw098NM32g3MRqDPySjcxLVu">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2220217&amp;sa=D&amp;source=editors&amp;ust=1660576307616721&amp;usg=AOvVaw3BxEw2q0BX38i_ylBjzfMF">https://doi.org/10.2307/2220217</a></span></p><p class="c3"><span>Hartmann, T., Toz, E., &amp; Brandon, M. (2010). Just a Game? Unjustified Virtual Violence Produces Guilt in Empathetic Players. </span><span class="c14">Media Psychology</span><span>, </span><span class="c14">13</span><span>(4), 339&ndash;363.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/15213269.2010.524912&amp;sa=D&amp;source=editors&amp;ust=1660576307617102&amp;usg=AOvVaw2meEhB9L8Qz7aP9iPa9U13">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/15213269.2010.524912&amp;sa=D&amp;source=editors&amp;ust=1660576307617269&amp;usg=AOvVaw3p6iG1Eo5UEMLhzBoVu9G_">https://doi.org/10.1080/15213269.2010.524912</a></span></p><p class="c3"><span>Harvard University. (2021). Hilary Putnam Bibliography.</span><span><a class="c11" href="https://www.google.com/url?q=https://philosophy.fas.harvard.edu/people/hilary-putnam&amp;sa=D&amp;source=editors&amp;ust=1660576307617626&amp;usg=AOvVaw2qoXEnY5cqxTzi5ruYbSQk">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://philosophy.fas.harvard.edu/people/hilary-putnam&amp;sa=D&amp;source=editors&amp;ust=1660576307617886&amp;usg=AOvVaw0YAdH4BncRWBTQEV1Uvpys">https://philosophy.fas.harvard.edu/people/hilary-putnam</a></span><span class="c8 c12">. Accessed 18 November 2021</span></p><p class="c3"><span>Haslam, N. (2006). Dehumanization: An Integrative Review. </span><span class="c14">Personality and Social Psychology Review</span><span>, </span><span class="c14">10</span><span>(3), 252&ndash;264.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1207/s15327957pspr1003_4&amp;sa=D&amp;source=editors&amp;ust=1660576307618459&amp;usg=AOvVaw0eW8-LbuRTAuU04vdJGQz0">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1207/s15327957pspr1003_4&amp;sa=D&amp;source=editors&amp;ust=1660576307618692&amp;usg=AOvVaw0e3JZMFf7vOiio6qT7QKFM">https://doi.org/10.1207/s15327957pspr1003_4</a></span></p><p class="c3"><span>Herrick, B. (2002). Evolution Paradigms and Constitutional Rights: The Imminent Danger of Artificial Intelligence. </span><span class="c14">Student Scholarship</span><span class="c8 c12">, 50.</span></p><p class="c3"><span>Hewett, T. T., Baecker, R., Card, S., Carey, T., Gasen, J., Mantei, M., et al. (1992). </span><span class="c14">ACM SIGCHI Curricula for Human-Computer Interaction</span><span class="c8 c12">. New York, NY: Association for Computing Machinery.</span></p><p class="c3"><span>Himma, K. E. (2004). There&rsquo;s something about Mary: The moral value of things qua information objects. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">6</span><span>(3), 145&ndash;159.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-004-3804-4&amp;sa=D&amp;source=editors&amp;ust=1660576307619466&amp;usg=AOvVaw3FIiOvZWzxjgVS1YKFKRFy">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-004-3804-4&amp;sa=D&amp;source=editors&amp;ust=1660576307619652&amp;usg=AOvVaw1RVpBcUnT86JeMjAG8ZAnQ">https://doi.org/10.1007/s10676-004-3804-4</a></span></p><p class="c3"><span>Holland, O., &amp; Goodman, R. (2003). Robots With Internal Models A Route to Machine Consciousness? </span><span class="c14">Journal of Consciousness Studies</span><span>, </span><span class="c14">10</span><span class="c8 c12">(4&ndash;5), 77&ndash;109.</span></p><p class="c3"><span>Hook, S. (1960). A Pragmatic Note. In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 202&ndash;207). New York, NY: New York University Press.</span></p><p class="c3"><span>Horstmann, A. C., Bock, N., Linhuber, E., Szczuka, J. M., Stra&szlig;mann, C., &amp; Kr&auml;mer, N. C. (2018). Do a robot&rsquo;s social skills and its objection discourage interactants from switching the robot off? </span><span class="c14">PLOS ONE</span><span>, </span><span class="c14">13</span><span>(7), e0201581.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0201581&amp;sa=D&amp;source=editors&amp;ust=1660576307620556&amp;usg=AOvVaw1H3OFyr6YrXKasEJq4q7J-">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0201581&amp;sa=D&amp;source=editors&amp;ust=1660576307620741&amp;usg=AOvVaw2737jHoqpWR5u-9v35lvV0">https://doi.org/10.1371/journal.pone.0201581</a></span></p><p class="c3"><span>Hu, S. D. (1987). What Software Engineers and Managers Need to Know. In S. D. Hu (Ed.), </span><span class="c14">Expert Systems for Software Engineers and Managers</span><span>&nbsp;(pp. 38&ndash;64). Boston, MA: Springer US.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-1-4613-1065-5_3&amp;sa=D&amp;source=editors&amp;ust=1660576307621199&amp;usg=AOvVaw2izGHn72c7XWxlgROoSN1R">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-1-4613-1065-5_3&amp;sa=D&amp;source=editors&amp;ust=1660576307621426&amp;usg=AOvVaw1LBFSgpg8KFTietG-J1zQt">https://doi.org/10.1007/978-1-4613-1065-5_3</a></span></p><p class="c3"><span>Hughes, J. J. (2005). </span><span class="c14">Report on the 2005 Interests and Beliefs Survey of the Members of the World Transhumanist Association</span><span class="c8 c12">&nbsp;(p. 16). World Transhumanist Association.</span></p><p class="c3"><span>Inayatullah, S. (2001a). The Rights of Robot: Inclusion, Courts and Unexpected Futures. </span><span class="c14">Journal of Futures Studies</span><span>, </span><span class="c14">6</span><span class="c8 c12">(2), 93&ndash;102.</span></p><p class="c3"><span>Inayatullah, S. (2001b). The Rights of Your Robots: Exclusion and Inclusion in History and Future.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.kurzweilai.net/the-rights-of-your-robots-exclusion-and-inclusion-in-history-and-future&amp;sa=D&amp;source=editors&amp;ust=1660576307622306&amp;usg=AOvVaw1hb-WbG0PURF-X-06rQmR5">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.kurzweilai.net/the-rights-of-your-robots-exclusion-and-inclusion-in-history-and-future&amp;sa=D&amp;source=editors&amp;ust=1660576307622545&amp;usg=AOvVaw1jmvy7gUGupjrkiiuCmNqA">https://www.kurzweilai.net/the-rights-of-your-robots-exclusion-and-inclusion-in-history-and-future</a></span><span class="c8 c12">. Accessed 16 November 2021</span></p><p class="c3"><span>James, M., &amp; Scott, K. (2008). </span><span class="c14">Robots &amp; Rights: Will Artificial Intelligence Change The Meaning Of Human Rights?</span><span>&nbsp;London, UK: BioCentre.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.bioethics.ac.uk/cmsfiles/files/resources/biocentre_symposium_report__robots_and_rights_150108.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307622939&amp;usg=AOvVaw0K7rXckJcXu7N2WR-eOR9p">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.bioethics.ac.uk/cmsfiles/files/resources/biocentre_symposium_report__robots_and_rights_150108.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307623285&amp;usg=AOvVaw3Y4uz6yRcFCvA-46klh2Vq">https://www.bioethics.ac.uk/cmsfiles/files/resources/biocentre_symposium_report__robots_and_rights_150108.pdf</a></span></p><p class="c3"><span>Jenkins, P. (2006). </span><span class="c14">Historical Simulations - Motivational, Ethical and Legal Issues</span><span>&nbsp;(SSRN Scholarly Paper No. ID 929327). Rochester, NY: Social Science Research Network.</span><span><a class="c11" href="https://www.google.com/url?q=https://papers.ssrn.com/abstract%3D929327&amp;sa=D&amp;source=editors&amp;ust=1660576307623755&amp;usg=AOvVaw0NNY1w4n6ZC4MS7BS5iFbS">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://papers.ssrn.com/abstract%3D929327&amp;sa=D&amp;source=editors&amp;ust=1660576307623945&amp;usg=AOvVaw1vkbKXGHwq_EiuQf04pVsT">https://papers.ssrn.com/abstract=929327</a></span><span class="c8 c12">. Accessed 16 November 2021</span></p><p class="c3"><span>Kahn, P. H., Friedman, B., Perez-Granados, D. R., &amp; Freier, N. G. (2004). Robotic pets in the lives of preschool children. In </span><span class="c14">CHI &rsquo;04 Extended Abstracts on Human Factors in Computing Systems</span><span>&nbsp;(pp. 1449&ndash;1452). New York, NY, USA: Association for Computing Machinery.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/985921.986087&amp;sa=D&amp;source=editors&amp;ust=1660576307624290&amp;usg=AOvVaw12imkAZ3ckNEk8Bpkpwi0S">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/985921.986087&amp;sa=D&amp;source=editors&amp;ust=1660576307624462&amp;usg=AOvVaw1PFfEfumZgWqsdnZ729nQB">https://doi.org/10.1145/985921.986087</a></span></p><p class="c3"><span>Kahn, P. H., Ishiguro, H., Friedman, B., &amp; Kanda, T. (2006). What is a Human? - Toward Psychological Benchmarks in the Field of Human-Robot Interaction. In </span><span class="c14">ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication</span><span>&nbsp;(pp. 364&ndash;371). Presented at the ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ROMAN.2006.314461&amp;sa=D&amp;source=editors&amp;ust=1660576307624816&amp;usg=AOvVaw3cJ4qGT6V36wWIsDneX0Ox">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ROMAN.2006.314461&amp;sa=D&amp;source=editors&amp;ust=1660576307625049&amp;usg=AOvVaw37Jmn2AwqQxU0xPlDOKlaH">https://doi.org/10.1109/ROMAN.2006.314461</a></span></p><p class="c3"><span>Kahn, P. H., Kanda, T., Ishiguro, H., Freier, N. G., Severson, R. L., Gill, B. T., et al. (2012). &ldquo;Robovie, you&rsquo;ll have to go into the closet now&rdquo;: Children&rsquo;s social and moral relationships with a humanoid robot. </span><span class="c14">Developmental Psychology</span><span>, </span><span class="c14">48</span><span>(2), 303&ndash;314.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/a0027033&amp;sa=D&amp;source=editors&amp;ust=1660576307625594&amp;usg=AOvVaw1a8-JCpVs4jZ-4BZzgr3bA">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/a0027033&amp;sa=D&amp;source=editors&amp;ust=1660576307625790&amp;usg=AOvVaw1qf5CtmQ7K9secDpSqQ1Xl">https://doi.org/10.1037/a0027033</a></span></p><p class="c3"><span>Kak, S. (2021). The Limits to Machine Consciousness. </span><span class="c14">Journal of Artificial Intelligence and Consciousness</span><span>, 1&ndash;14.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1142/S2705078521500193&amp;sa=D&amp;source=editors&amp;ust=1660576307626149&amp;usg=AOvVaw3BjG96fgdRMkH6P_YiINOP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1142/S2705078521500193&amp;sa=D&amp;source=editors&amp;ust=1660576307626334&amp;usg=AOvVaw3hQXR_l6cFY3ZO74NNt1nf">https://doi.org/10.1142/S2705078521500193</a></span></p><p class="c3"><span>Karnow, C. E. A. (1994). The Encrypted Self: Fleshing out the Rights of Electronic Personalities. </span><span class="c14">John Marshall Journal of Computer and Information Law</span><span>, </span><span class="c14">13</span><span class="c8 c12">, 1.</span></p><p class="c3"><span>Karnow, C. E. A. (1996). Liability for Distributed Artificial Intelligences. </span><span class="c14">Berkeley Technology Law Journal</span><span>, </span><span class="c14">11</span><span class="c8 c12">(1), 147&ndash;204.</span></p><p class="c3"><span>Kester, C. M. (1993). Is There a Person in That Body: An Argument for the Priority of Persons and the Need for a New Legal Paradigm. </span><span class="c14">Georgetown Law Journal</span><span>, </span><span class="c14">82</span><span class="c8 c12">, 1643.</span></p><p class="c3"><span>Kim, J. (2005). Making Right(s) Decision: Artificial Life and Rights Reconsidered. Presented at the DiGRA Conference.</span><span><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.96.8255%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307627134&amp;usg=AOvVaw1BybZaqJEEnsYnfzsnziW-">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.96.8255%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307627348&amp;usg=AOvVaw3q_sw_0VCJ91Lpp2M-uE6o">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.8255&amp;rep=rep1&amp;type=pdf</a></span></p><p class="c3"><span>Kim, J., &amp; Petrina, S. (2006). Artificial life rights: Facing moral dilemmas through The Sims. </span><span class="c14">Educational Insights</span><span>, </span><span class="c14">10</span><span class="c8 c12">(2), 12.</span></p><p class="c3"><span>Krach, S., Hegel, F., Wrede, B., Sagerer, G., Binkofski, F., &amp; Kircher, T. (2008). Can Machines Think? Interaction and Perspective Taking with Robots Investigated via fMRI. </span><span class="c14">PLOS ONE</span><span>, </span><span class="c14">3</span><span>(7), e2597.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0002597&amp;sa=D&amp;source=editors&amp;ust=1660576307627931&amp;usg=AOvVaw1ZVswXezT-uJY8MRlTq7iy">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0002597&amp;sa=D&amp;source=editors&amp;ust=1660576307628102&amp;usg=AOvVaw19dEPQjvweOW-UJXfgNBER">https://doi.org/10.1371/journal.pone.0002597</a></span></p><p class="c3"><span>Krebs, S. (2006). On the Anticipation of Ethical Conflicts between Humans and Robots in Japanese Mangas. </span><span class="c14">The International Review of Information Ethics</span><span>, </span><span class="c14">6</span><span>, 63&ndash;68.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie141&amp;sa=D&amp;source=editors&amp;ust=1660576307628470&amp;usg=AOvVaw1l14Q2oODJPLmbEosFoE94">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.29173/irie141&amp;sa=D&amp;source=editors&amp;ust=1660576307628631&amp;usg=AOvVaw25U6UkNDHF4PZT_ysQybmo">https://doi.org/10.29173/irie141</a></span></p><p class="c3"><span>Krenn, B., &amp; Gstrein, E. (2006). The Human Behind: Strategies Against Agent Abuse. In A. De Angeli, S. Brahnam, P. Wallis, &amp; A. Dix (Eds.), </span><span class="c14">Misuse and Abuse of Interactive Technologies</span><span>&nbsp;(pp. 33&ndash;36). Presented at the CHI 2006 Conference on Human Factors in Computing Systems, Montr&eacute;al Qu&eacute;bec Canada: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307629018&amp;usg=AOvVaw0kWBdRZHQ9fwQ5A6UDLEfj">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307629184&amp;usg=AOvVaw3k__Lbu1ZtFNGDhdkUsLzO">https://doi.org/10.1145/1125451.1125753</a></span></p><p class="c3"><span>Krogh, C. (1996). The rights of agents. In M. Wooldridge, J. P. M&uuml;ller, &amp; M. Tambe (Eds.), </span><span class="c14">Intelligent Agents II Agent Theories, Architectures, and Languages</span><span>&nbsp;(pp. 1&ndash;16). Berlin, Heidelberg: Springer.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/3540608052_55&amp;sa=D&amp;source=editors&amp;ust=1660576307629533&amp;usg=AOvVaw1Gt5rdRi-IxN9JUv3kTVMp">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/3540608052_55&amp;sa=D&amp;source=editors&amp;ust=1660576307629696&amp;usg=AOvVaw3hieM-B11lhgvl7Pxdc9I-">https://doi.org/10.1007/3540608052_55</a></span></p><p class="c3"><span>Kurzweil, R. (1999). </span><span class="c14">The Age of Spiritual Machines: When Computers Exceed Human Intelligence</span><span class="c8 c12">. New York, NY: Viking.</span></p><p class="c3"><span>K&uuml;ster, D., &amp; &#346;widerska, A. (2016). Moral Patients: What Drives the Perceptions of Moral Actions Towards Humans and Robots? In J. Seibt, M. N&oslash;rskov, &amp; S. S. Andersen (Eds.), </span><span class="c14">What Social Robots Can and Should Do: Proceedings of Robophilosophy 2016 / TRANSOR 2016</span><span class="c8 c12">&nbsp;(pp. 340&ndash;343). Amsterdam, Netherlands: IOS Press.</span></p><p class="c3"><span>K&uuml;ster, D., Swiderska, A., &amp; Gunkel, D. (2021). I saw it on YouTube! How online videos shape perceptions of mind, morality, and fears about robots. </span><span class="c14">New Media &amp; Society</span><span>, </span><span class="c14">23</span><span>(11), 3312&ndash;3331.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/1461444820954199&amp;sa=D&amp;source=editors&amp;ust=1660576307630322&amp;usg=AOvVaw2qoQBEvocl5VlfqQhyEOHo">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/1461444820954199&amp;sa=D&amp;source=editors&amp;ust=1660576307630512&amp;usg=AOvVaw1chTVSNzEh8EXBQRZobs3z">https://doi.org/10.1177/1461444820954199</a></span></p><p class="c3"><span>LaChat, M. R. (1986). Artificial Intelligence and Ethics: An Exercise in the Moral Imagination. </span><span class="c14">AI Magazine</span><span>, </span><span class="c14">7</span><span>(2), 70&ndash;70.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v7i2.540&amp;sa=D&amp;source=editors&amp;ust=1660576307630907&amp;usg=AOvVaw0rWrQeMOUbCl0o4WK6jDoI">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v7i2.540&amp;sa=D&amp;source=editors&amp;ust=1660576307631081&amp;usg=AOvVaw2_AYyiOdL16jkr8iormKCC">https://doi.org/10.1609/aimag.v7i2.540</a></span></p><p class="c3"><span>Laham, S. M. (2009). Expanding the moral circle: Inclusion and exclusion mindsets and the circle of moral regard. </span><span class="c14">Journal of Experimental Social Psychology</span><span>, </span><span class="c14">45</span><span>(1), 250&ndash;253.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.jesp.2008.08.012&amp;sa=D&amp;source=editors&amp;ust=1660576307631459&amp;usg=AOvVaw1NkC2g9CoMQqK8Iibv1VrU">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.jesp.2008.08.012&amp;sa=D&amp;source=editors&amp;ust=1660576307631626&amp;usg=AOvVaw0ixGpaPBW6S1YiLY6wSoD0">https://doi.org/10.1016/j.jesp.2008.08.012</a></span></p><p class="c3"><span>Laukyte, M. (2017). Artificial agents among us: Should we recognize them as agents proper? </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">19</span><span>(1), 1&ndash;17.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-016-9411-3&amp;sa=D&amp;source=editors&amp;ust=1660576307631984&amp;usg=AOvVaw30KsBT2Rq-OykHYoQWElIw">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-016-9411-3&amp;sa=D&amp;source=editors&amp;ust=1660576307632147&amp;usg=AOvVaw0uUv0DWvUwURLYFX9CrO6Z">https://doi.org/10.1007/s10676-016-9411-3</a></span></p><p class="c3"><span>lawyers.com. (2022). Marshal S. Willick, Esq.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.lawyers.com/las-vegas/nevada/marshal-s-willick-esq-1067567-a/&amp;sa=D&amp;source=editors&amp;ust=1660576307632449&amp;usg=AOvVaw0HmHG5f-lWnHGFXVwCuMtv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.lawyers.com/las-vegas/nevada/marshal-s-willick-esq-1067567-a/&amp;sa=D&amp;source=editors&amp;ust=1660576307632653&amp;usg=AOvVaw0bQMGSqjOVJdwRTWsqC587">https://www.lawyers.com/las-vegas/nevada/marshal-s-willick-esq-1067567-a/</a></span><span class="c8 c12">. Accessed 12 January 2022</span></p><p class="c3"><span>Lehman-Wilzig, S. N. (1981). Frankenstein unbound: Towards a legal definition of artificial intelligence. </span><span class="c14">Futures</span><span>, </span><span class="c14">13</span><span>(6), 442&ndash;457.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(81)90100-2&amp;sa=D&amp;source=editors&amp;ust=1660576307633033&amp;usg=AOvVaw2Tob7SWfbGYVPTUhwYGo8H">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(81)90100-2&amp;sa=D&amp;source=editors&amp;ust=1660576307633200&amp;usg=AOvVaw1XTuiZP1XLsDp-Hra0oHx-">https://doi.org/10.1016/0016-3287(81)90100-2</a></span></p><p class="c3"><span>Leopold, A. (1949). </span><span class="c14">A Sand County Almanac, and Sketches Here and There</span><span class="c8 c12">. New York, NY: Oxford University Press.</span></p><p class="c3"><span>Levy, D. (2005). </span><span class="c14">Robots Unlimited: Life in a Virtual Age</span><span class="c8 c12">. CRC Press.</span></p><p class="c3"><span>Levy, D. (2009). The Ethical Treatment of Artificially Conscious Robots. </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">1</span><span>(3), 209&ndash;216.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-009-0022-6&amp;sa=D&amp;source=editors&amp;ust=1660576307633787&amp;usg=AOvVaw0NJ4rzMYgIqQr0J-EyduDX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-009-0022-6&amp;sa=D&amp;source=editors&amp;ust=1660576307633953&amp;usg=AOvVaw0_GqEQS5_4g2Q9Ei3ykvhe">https://doi.org/10.1007/s12369-009-0022-6</a></span></p><p class="c3"><span>Lichocki, P., Kahn, P. H., &amp; Billard, A. (2011). A Survey of the Robotics Ethical Landscape. </span><span class="c14">IEEE Robotics &amp; Automation Magazine</span><span>, </span><span class="c14">18</span><span class="c8 c12">(1), 39&ndash;50.</span></p><p class="c3"><span>Lima, G., Kim, C., Ryu, S., Jeon, C., &amp; Cha, M. (2020). Collecting the Public Perception of AI and Robot Rights. </span><span class="c14">Proceedings of the ACM on Human-Computer Interaction</span><span>, </span><span class="c14">4</span><span>(CSCW2), 135:1&ndash;135:24.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/3415206&amp;sa=D&amp;source=editors&amp;ust=1660576307634468&amp;usg=AOvVaw2JgKe6exxTiu73dhE4zZdl">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/3415206&amp;sa=D&amp;source=editors&amp;ust=1660576307634634&amp;usg=AOvVaw2sTc0eRLFAV3ShUKKcXd8Y">https://doi.org/10.1145/3415206</a></span></p><p class="c3"><span>Loughnan, S., &amp; Haslam, N. (2007). Animals and Androids: Implicit Associations Between Social Categories and Nonhumans. </span><span class="c14">Psychological Science</span><span>, </span><span class="c14">18</span><span>(2), 116&ndash;121.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1467-9280.2007.01858.x&amp;sa=D&amp;source=editors&amp;ust=1660576307635002&amp;usg=AOvVaw3OAVjKxMligPagOHRoBHSf">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1467-9280.2007.01858.x&amp;sa=D&amp;source=editors&amp;ust=1660576307635171&amp;usg=AOvVaw0KmLEpRElgzVYmazDTRCHQ">https://doi.org/10.1111/j.1467-9280.2007.01858.x</a></span></p><p class="c3"><span>Lycan, W. G. (1985). Abortion and the Civil Rights of Machines. In N. T. Potter &amp; M. Timmons (Eds.), </span><span class="c14">Morality and Universality: Essays on Ethical Universalizability</span><span>&nbsp;(pp. 139&ndash;156). Dordrecht: Springer Netherlands.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-94-009-5285-0_7&amp;sa=D&amp;source=editors&amp;ust=1660576307635533&amp;usg=AOvVaw2E76_VKMCy08aVSm-0M3g2">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-94-009-5285-0_7&amp;sa=D&amp;source=editors&amp;ust=1660576307635709&amp;usg=AOvVaw2PHNXDLlyeCoJtkwI38BxD">https://doi.org/10.1007/978-94-009-5285-0_7</a></span></p><p class="c3"><span>MacAskill, W. (2019). The Definition of Effective Altruism. In </span><span class="c14">Effective Altruism</span><span>&nbsp;(pp. 10&ndash;28). Oxford University Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/oso/9780198841364.003.0001&amp;sa=D&amp;source=editors&amp;ust=1660576307636044&amp;usg=AOvVaw2jjfRDPBxnzMLJQzKn7oFB">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/oso/9780198841364.003.0001&amp;sa=D&amp;source=editors&amp;ust=1660576307636229&amp;usg=AOvVaw1iwnCE3xhE7Jxwufij0Gsm">https://doi.org/10.1093/oso/9780198841364.003.0001</a></span></p><p class="c3"><span>MacAskill, W. (2022). </span><span class="c14">What We Owe the Future</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.williammacaskill.com/what-we-owe-the-future&amp;sa=D&amp;source=editors&amp;ust=1660576307636574&amp;usg=AOvVaw1Q1OG_4dVze2ALMbnmPCpR">&nbsp;[Unpublished manuscript]. </a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.williammacaskill.com/what-we-owe-the-future&amp;sa=D&amp;source=editors&amp;ust=1660576307636761&amp;usg=AOvVaw0XKXgXhfhmV4tnfenzuTPK">https://www.williammacaskill.com/what-we-owe-the-future</a></span></p><p class="c3"><span>MacDorman, K. F., &amp; Cowley, S. J. (2006). Long-term relationships as a benchmark for robot personhood. In </span><span class="c14">ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication</span><span>&nbsp;(pp. 378&ndash;383). Presented at the ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ROMAN.2006.314463&amp;sa=D&amp;source=editors&amp;ust=1660576307637105&amp;usg=AOvVaw1YmIfjv2QMrMfuuUsYXxxh">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ROMAN.2006.314463&amp;sa=D&amp;source=editors&amp;ust=1660576307637270&amp;usg=AOvVaw3I6OwgY0vowtXccc6Bkjlh">https://doi.org/10.1109/ROMAN.2006.314463</a></span></p><p class="c3"><span>Magnani, L. (2005). Technological Artifacts as Moral Carriers and Mediators. In </span><span class="c14">Machine ethics, papers from AAAI fall symposium technical report FS-05-06</span><span>&nbsp;(pp. 62&ndash;69).</span><span><a class="c11" href="https://www.google.com/url?q=https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-009.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307637630&amp;usg=AOvVaw3ZKFbGvQGqsK-VTh2yN2Ty">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-009.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307637826&amp;usg=AOvVaw1JCcYJKEv5rG2fZiBYKUbr">https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-009.pdf</a></span></p><p class="c3"><span>Magnuson, M. A. (2014). What is Transhumanism? </span><span class="c14">What is Transhumanism?</span><span><a class="c11" href="https://www.google.com/url?q=https://whatistranshumanism.org/&amp;sa=D&amp;source=editors&amp;ust=1660576307638240&amp;usg=AOvVaw1O38p_7vJX4C7amtGwz0PO">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://whatistranshumanism.org/&amp;sa=D&amp;source=editors&amp;ust=1660576307638432&amp;usg=AOvVaw21bPHtZPGRIpWkg7Knjlb_">https://whatistranshumanism.org/</a></span><span class="c8 c12">. Accessed 23 November 2021</span></p><p class="c3"><span>Matthews, D. (2014). This guy thinks killing video game characters is immoral. </span><span class="c14">Vox</span><span>. </span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.vox.com/2014/4/23/5643418/this-guy-thinks-killing-video-game-characters-is-immoral&amp;sa=D&amp;source=editors&amp;ust=1660576307638862&amp;usg=AOvVaw2jBbNtPUb-yWwvQzVlVimd">https://www.vox.com/2014/4/23/5643418/this-guy-thinks-killing-video-game-characters-is-immoral</a></span><span class="c8 c12">. Accessed 11 March 2022</span></p><p class="c3"><span>McCarthy, J., Minsky, M. L., Rochester, N., &amp; Shannon, C. E. (2006). A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955. </span><span class="c14">AI Magazine</span><span>, </span><span class="c14">27</span><span>(4), 12&ndash;12.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v27i4.1904&amp;sa=D&amp;source=editors&amp;ust=1660576307639254&amp;usg=AOvVaw3mPBWMroKqjIg6ggYGwn1i">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v27i4.1904&amp;sa=D&amp;source=editors&amp;ust=1660576307639429&amp;usg=AOvVaw2otxhP2eTw7Iwe4sAeybAF">https://doi.org/10.1609/aimag.v27i4.1904</a></span></p><p class="c3"><span>McCorduck, P. (2004). </span><span class="c14">Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence</span><span class="c8 c12">. Boca Raton, FL: CRC Press.</span></p><p class="c3"><span>McNally, P., &amp; Inayatullah, S. (1988). The rights of robots: Technology, culture and law in the 21st century. </span><span class="c14">Futures</span><span>, </span><span class="c14">20</span><span>(2), 119&ndash;136.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(88)90019-5&amp;sa=D&amp;source=editors&amp;ust=1660576307639947&amp;usg=AOvVaw0JmM8pGvcGvx9_7a0D942G">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/0016-3287(88)90019-5&amp;sa=D&amp;source=editors&amp;ust=1660576307640120&amp;usg=AOvVaw3k0xhs83w535ALgpwWKpg3">https://doi.org/10.1016/0016-3287(88)90019-5</a></span></p><p class="c3"><span>Melson, G. F., Kahn, P. H., Beck, A., Friedman, B., Roberts, T., Garrett, E., &amp; Gill, B. T. (2009). Children&rsquo;s behavior toward and understanding of robotic and living dogs. </span><span class="c14">Journal of Applied Developmental Psychology</span><span>, </span><span class="c14">30</span><span>(2), 92&ndash;102.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.appdev.2008.10.011&amp;sa=D&amp;source=editors&amp;ust=1660576307640505&amp;usg=AOvVaw1dmdF2BVMYXss4ErqpdhOT">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.appdev.2008.10.011&amp;sa=D&amp;source=editors&amp;ust=1660576307640676&amp;usg=AOvVaw36mET1XX7_8drqxGS-_b30">https://doi.org/10.1016/j.appdev.2008.10.011</a></span></p><p class="c3"><span>Melson, G. F., Kahn, P. H., Jr., Beck, A., &amp; Friedman, B. (2009). Robotic Pets in Human Lives: Implications for the Human&ndash;Animal Bond and for Human Relationships with Personified Technologies. </span><span class="c14">Journal of Social Issues</span><span>, </span><span class="c14">65</span><span>(3), 545&ndash;567.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1540-4560.2009.01613.x&amp;sa=D&amp;source=editors&amp;ust=1660576307641051&amp;usg=AOvVaw13ynFY1CU8tiL_CxiG9L56">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1540-4560.2009.01613.x&amp;sa=D&amp;source=editors&amp;ust=1660576307641231&amp;usg=AOvVaw0kLMf6asuatVJ1UWIo30gI">https://doi.org/10.1111/j.1540-4560.2009.01613.x</a></span></p><p class="c3"><span>Metzinger, T. (2013). Two Principles for Robot Ethics. In J.-P. G&uuml;nther &amp; E. Hilgendorf (Eds.), </span><span class="c14">Robotik und Gesetzgebung</span><span>&nbsp;(pp. 263-302.). Baden-Baden, Germany: Nomos.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.5771/9783845242200-263&amp;sa=D&amp;source=editors&amp;ust=1660576307641574&amp;usg=AOvVaw0rAgQS0QjqoopLtCX0M830">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.5771/9783845242200-263&amp;sa=D&amp;source=editors&amp;ust=1660576307641748&amp;usg=AOvVaw0hsvnpEWiq_nNtZDLiqMFO">https://doi.org/10.5771/9783845242200-263</a></span></p><p class="c3"><span>Miah, A. (2009). A Critical History of Posthumanism. In B. Gordijn &amp; R. Chadwick (Eds.), </span><span class="c14">Medical Enhancement and Posthumanity</span><span>&nbsp;(pp. 71&ndash;94). Dordrecht: Springer Netherlands.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-1-4020-8852-0_6&amp;sa=D&amp;source=editors&amp;ust=1660576307642082&amp;usg=AOvVaw3XrsTKom36hctDWeX0HAC_">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-1-4020-8852-0_6&amp;sa=D&amp;source=editors&amp;ust=1660576307642252&amp;usg=AOvVaw187Z-wTjUMCVY4mBu2K8NK">https://doi.org/10.1007/978-1-4020-8852-0_6</a></span></p><p class="c3"><span>Milgram, S. (1974). </span><span class="c14">Obedience to Authority: An Experimental View</span><span>. New York, NY: Harper &amp; Row.</span><span><a class="c11" href="https://www.google.com/url?q=https://repository.library.georgetown.edu/handle/10822/766828&amp;sa=D&amp;source=editors&amp;ust=1660576307642612&amp;usg=AOvVaw1F5F4Mbgz_2APfdM44rV3X">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://repository.library.georgetown.edu/handle/10822/766828&amp;sa=D&amp;source=editors&amp;ust=1660576307642813&amp;usg=AOvVaw1R9GfG-7Gz9VP6NiPqSyyb">https://repository.library.georgetown.edu/handle/10822/766828</a></span><span class="c8 c12">. Accessed 14 December 2021</span></p><p class="c3"><span>Minsky, M. (1994). Will Robots Inherit the Earth? </span><span class="c14">Scientific American</span><span>, </span><span class="c14">271</span><span class="c8 c12">(4), 108&ndash;113.</span></p><p class="c3"><span>Minsky, M. L. (1991). Conscious Machines. In </span><span class="c14">National Research Council of Canada, 75th Anniversary Symposium on Science in Society</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.aurellem.org/6.868/resources/conscious-machines.html&amp;sa=D&amp;source=editors&amp;ust=1660576307643353&amp;usg=AOvVaw1-PefWr8pme8tHFkC11pko">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.aurellem.org/6.868/resources/conscious-machines.html&amp;sa=D&amp;source=editors&amp;ust=1660576307643556&amp;usg=AOvVaw213M0iq7AWKBfxXDCp9R8D">http://www.aurellem.org/6.868/resources/conscious-machines.html</a></span></p><p class="c3"><span>Misselhorn, C. (2009). Empathy with Inanimate Objects and the Uncanny Valley. </span><span class="c14">Minds and Machines</span><span>, </span><span class="c14">19</span><span>(3), 345.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11023-009-9158-2&amp;sa=D&amp;source=editors&amp;ust=1660576307643915&amp;usg=AOvVaw3k4Wxlg24vtZmce-6CbP2m">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11023-009-9158-2&amp;sa=D&amp;source=editors&amp;ust=1660576307644080&amp;usg=AOvVaw25nul7oO857KNzUySLvPQt">https://doi.org/10.1007/s11023-009-9158-2</a></span></p><p class="c3"><span>Mittelstadt, B. D., &amp; Floridi, L. (2016). The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts. In B. D. Mittelstadt &amp; L. Floridi (Eds.), </span><span class="c14">The Ethics of Biomedical Big Data</span><span>&nbsp;(pp. 445&ndash;480). Cham: Springer International Publishing.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-3-319-33525-4_19&amp;sa=D&amp;source=editors&amp;ust=1660576307644426&amp;usg=AOvVaw1YQpyH9016Nier8vA4J6t6">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/978-3-319-33525-4_19&amp;sa=D&amp;source=editors&amp;ust=1660576307644608&amp;usg=AOvVaw3EbX4gHOqNZToy3NOznXBX">https://doi.org/10.1007/978-3-319-33525-4_19</a></span></p><p class="c3"><span>Modern Mechanix. (1957). You&rsquo;ll Own &ldquo;Slaves&rdquo; by 1965.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.impactlab.com/2008/04/14/you%25E2%2580%2599ll-own-%25E2%2580%259Cslaves%25E2%2580%259D-by-1965/&amp;sa=D&amp;source=editors&amp;ust=1660576307644936&amp;usg=AOvVaw39m4726fR1eun3r_jN6gwb">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.impactlab.com/2008/04/14/you%25E2%2580%2599ll-own-%25E2%2580%259Cslaves%25E2%2580%259D-by-1965/&amp;sa=D&amp;source=editors&amp;ust=1660576307645150&amp;usg=AOvVaw3mXdrFlKKnS7LB8MoYb8_S">https://www.impactlab.com/2008/04/14/you&rsquo;ll-own-&ldquo;slaves&rdquo;-by-1965/</a></span><span class="c8 c12">. Accessed 29 December 2021</span></p><p class="c3"><span>Moravec, H. (1988). </span><span class="c14">Mind Children: The Future of Robot and Human Intelligence</span><span class="c8 c12">. Harvard University Press.</span></p><p class="c3"><span>Moravec, H. (1998). When will computer hardware match the human brain? </span><span class="c14">Journal of Evolution and Technology</span><span>, </span><span class="c14">1</span><span class="c8 c12">, 12.</span></p><p class="c3"><span>Moravec, H. P. (2000). </span><span class="c14">Robot: Mere Machine to Transcendent Mind</span><span class="c8 c12">. Oxford University Press.</span></p><p class="c3"><span>Muehlhauser, L. (2017). Some Case Studies in Early Field Growth. </span><span class="c14">Open Philanthropy</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth&amp;sa=D&amp;source=editors&amp;ust=1660576307646094&amp;usg=AOvVaw0oRU0zFe0D0QpI1DK2lQrW">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth&amp;sa=D&amp;source=editors&amp;ust=1660576307646338&amp;usg=AOvVaw0OVfJBKMCcZLjsFKNFhDZ_">https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Musial, M. (2016). Magical Thinking and Empathy Towards Robots. In J. Seibt, M. N&oslash;rskov, &amp; S. S. Andersen (Eds.), </span><span class="c14">What Social Robots Can and Should Do: Proceedings of Robophilosophy 2016 / TRANSOR 2016</span><span class="c8 c12">&nbsp;(pp. 347&ndash;356). Amsterdam, Netherlands: IOS Press.</span></p><p class="c3"><span>Nilsson, N. J. (2009). </span><span class="c14">The Quest for Artificial Intelligence: A History of Ideas and Achievements</span><span>. Cambridge: Cambridge University Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1017/CBO9780511819346&amp;sa=D&amp;source=editors&amp;ust=1660576307646822&amp;usg=AOvVaw18H8sizkYeFusYF3LckTWO">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1017/CBO9780511819346&amp;sa=D&amp;source=editors&amp;ust=1660576307646997&amp;usg=AOvVaw2yrPaZ8wQksGPqat7hshE0">https://doi.org/10.1017/CBO9780511819346</a></span></p><p class="c3"><span>Nomura, T., Kanda, T., &amp; Suzuki, T. (2006). Experimental investigation into influence of negative attitudes toward robots on human&ndash;robot interaction. </span><span class="c14">AI &amp; SOCIETY</span><span>, </span><span class="c14">20</span><span>(2), 138&ndash;150.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-005-0012-7&amp;sa=D&amp;source=editors&amp;ust=1660576307647373&amp;usg=AOvVaw3trarFqcD5JLH_TKM2IEye">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-005-0012-7&amp;sa=D&amp;source=editors&amp;ust=1660576307647546&amp;usg=AOvVaw0dR_Bj8QdZ9xwFBsUadZ80">https://doi.org/10.1007/s00146-005-0012-7</a></span></p><p class="c3"><span>Nomura, T., Uratani, T., Kanda, T., Matsumoto, K., Kidokoro, H., Suehiro, Y., &amp; Yamada, S. (2015). Why Do Children Abuse Robots? In </span><span class="c14">Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts</span><span>&nbsp;(pp. 63&ndash;64). Presented at the HRI &rsquo;15: ACM/IEEE International Conference on Human-Robot Interaction, Portland Oregon USA: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/2701973.2701977&amp;sa=D&amp;source=editors&amp;ust=1660576307647879&amp;usg=AOvVaw26HAKhT2wygBmkK9ILUnsZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/2701973.2701977&amp;sa=D&amp;source=editors&amp;ust=1660576307648040&amp;usg=AOvVaw0khKaLbEW-JNGwMf92FicL">https://doi.org/10.1145/2701973.2701977</a></span></p><p class="c3"><span>N&oslash;rskov, M., Seibt, J., &amp; Quick, O. S. (2021). </span><span class="c14">Culturally Sustainable Social Robotics: Proceedings of Robophilosophy 2020</span><span class="c8 c12">. IOS Press.</span></p><p class="c3"><span>Oppy, G., &amp; Dowe, D. (2021). The Turing Test. In E. N. Zalta (Ed.), </span><span class="c14">The Stanford Encyclopedia of Philosophy</span><span>&nbsp;(Winter 2021.). Metaphysics Research Lab, Stanford University.</span><span><a class="c11" href="https://www.google.com/url?q=https://plato.stanford.edu/archives/win2021/entriesuring-test/&amp;sa=D&amp;source=editors&amp;ust=1660576307648511&amp;usg=AOvVaw1S2cbYc2QvKXO52AUX8-GX">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://plato.stanford.edu/archives/win2021/entriesuring-test/&amp;sa=D&amp;source=editors&amp;ust=1660576307648709&amp;usg=AOvVaw2giHmJA2xSqetwsFQEXPKm">https://plato.stanford.edu/archives/win2021/entriesuring-test/</a></span><span class="c8 c12">. Accessed 18 November 2021</span></p><p class="c3"><span>Pauketat, J. V. (2021). </span><span class="c14">The Terminology of Artificial Sentience</span><span>. PsyArXiv.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.31234/osf.io/sujwf&amp;sa=D&amp;source=editors&amp;ust=1660576307649128&amp;usg=AOvVaw2r38Ujx0AM9uldi0nL1-cr">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.31234/osf.io/sujwf&amp;sa=D&amp;source=editors&amp;ust=1660576307649352&amp;usg=AOvVaw14Ql086lbXtwl9Oi-F9YRA">https://doi.org/10.31234/osf.io/sujwf</a></span></p><p class="c3"><span>Petersen, S. (2007). The ethics of robot servitude. </span><span class="c14">Journal of Experimental &amp; Theoretical Artificial Intelligence</span><span>, </span><span class="c14">19</span><span>(1), 43&ndash;54.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/09528130601116139&amp;sa=D&amp;source=editors&amp;ust=1660576307649797&amp;usg=AOvVaw1TGBqHmIbfi3xfB1RQlDS8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/09528130601116139&amp;sa=D&amp;source=editors&amp;ust=1660576307649990&amp;usg=AOvVaw00sqlb4YJhLDDESrcwdqQy">https://doi.org/10.1080/09528130601116139</a></span></p><p class="c3"><span>Petrina, S., Volk, K., &amp; Kim, S. (2004). Technology and Rights. </span><span class="c14">International Journal of Technology and Design Education</span><span>, </span><span class="c14">14</span><span>(3), 181&ndash;204.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10798-004-0809-6&amp;sa=D&amp;source=editors&amp;ust=1660576307650431&amp;usg=AOvVaw085UX7faVHzzx3elzHkXq3">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10798-004-0809-6&amp;sa=D&amp;source=editors&amp;ust=1660576307650626&amp;usg=AOvVaw1ORIW27g96pjhJKzQo-jq6">https://doi.org/10.1007/s10798-004-0809-6</a></span></p><p class="c3"><span>PETRL. (2015). People for the Ethical Treatment of Reinforcement Learners.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.petrl.org/&amp;sa=D&amp;source=editors&amp;ust=1660576307650895&amp;usg=AOvVaw1uIj4_XPDi_-FLYrQgLrfv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.petrl.org/&amp;sa=D&amp;source=editors&amp;ust=1660576307651045&amp;usg=AOvVaw30JFPW0891Q40aiGlPTf0_">http://www.petrl.org/</a></span><span class="c8 c12">. Accessed 25 November 2021</span></p><p class="c3"><span>Platt, C. (1995). Superhumanism. </span><span class="c14">Wired</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.wired.com/1995/10/moravec/&amp;sa=D&amp;source=editors&amp;ust=1660576307651346&amp;usg=AOvVaw0yFeqqvgG6PuHDaNaDdkVP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.wired.com/1995/10/moravec/&amp;sa=D&amp;source=editors&amp;ust=1660576307651524&amp;usg=AOvVaw1mV8eGY0guNzIY7lgoWWTu">https://www.wired.com/1995/10/moravec/</a></span><span class="c8 c12">. Accessed 29 December 2021</span></p><p class="c3"><span>Putnam, H. (1960). Minds and Machines. In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 148&ndash;180). New York, NY: New York University Press.</span></p><p class="c3"><span>Putnam, H. (1964). Robots: Machines or Artificially Created Life? </span><span class="c14">The Journal of Philosophy</span><span>, </span><span class="c14">61</span><span>(21), 668&ndash;691.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2023045&amp;sa=D&amp;source=editors&amp;ust=1660576307651991&amp;usg=AOvVaw3ZkRIz_rPQ-8nND_SGa7q8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2307/2023045&amp;sa=D&amp;source=editors&amp;ust=1660576307652180&amp;usg=AOvVaw0V_XrBNogJ8I7Yv9xMUuHU">https://doi.org/10.2307/2023045</a></span></p><p class="c3"><span>Radio New Zealand. (2020). The Morality of Abusing A Robot. </span><span class="c14">Nights</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.rnz.co.nz/national/programmes/nights/audio/2018757787/the-morality-of-abusing-a-robot&amp;sa=D&amp;source=editors&amp;ust=1660576307652557&amp;usg=AOvVaw3-j6S4ST_4s9GJx9H2nN1l">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.rnz.co.nz/national/programmes/nights/audio/2018757787/the-morality-of-abusing-a-robot&amp;sa=D&amp;source=editors&amp;ust=1660576307652784&amp;usg=AOvVaw13KSbirHVk-siUIVPTrBU6">https://www.rnz.co.nz/national/programmes/nights/audio/2018757787/the-morality-of-abusing-a-robot</a></span><span class="c8 c12">. Accessed 7 December 2021</span></p><p class="c3"><span>Reed, A. II., &amp; Aquino, K. F. (2003). Moral identity and the expanding circle of moral regard toward out-groups. </span><span class="c14">Journal of Personality and Social Psychology</span><span>, </span><span class="c14">84</span><span>(6), 1270&ndash;1286.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/0022-3514.84.6.1270&amp;sa=D&amp;source=editors&amp;ust=1660576307653156&amp;usg=AOvVaw0ayzCtndkKqFJjZe8bes39">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1037/0022-3514.84.6.1270&amp;sa=D&amp;source=editors&amp;ust=1660576307653323&amp;usg=AOvVaw3Bvr_Pedyihmek-H9pJiGi">https://doi.org/10.1037/0022-3514.84.6.1270</a></span></p><p class="c3"><span>Regan, T. (2004). </span><span class="c14">The Case for Animal Rights</span><span class="c8 c12">. University of California Press.</span></p><p class="c3"><span>Reggia, J. (2013). The rise of machine consciousness: Studying consciousness with computational models. </span><span class="c14">Neural Networks</span><span>, </span><span class="c14">44</span><span>, 112&ndash;131.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.neunet.2013.03.011&amp;sa=D&amp;source=editors&amp;ust=1660576307653809&amp;usg=AOvVaw3SSynvnK20_4pxHzjuhm-y">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.neunet.2013.03.011&amp;sa=D&amp;source=editors&amp;ust=1660576307653984&amp;usg=AOvVaw1dZyg7S3FVNVbcQBBVuwtB">https://doi.org/10.1016/j.neunet.2013.03.011</a></span></p><p class="c3"><span>Robertson, J. (2014). HUMAN RIGHTS VS. ROBOT RIGHTS: Forecasts from Japan. </span><span class="c14">Critical Asian Studies</span><span>, </span><span class="c14">46</span><span>(4), 571&ndash;598.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/14672715.2014.960707&amp;sa=D&amp;source=editors&amp;ust=1660576307654369&amp;usg=AOvVaw3axRtKA6P6zCSlxocQK5eP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/14672715.2014.960707&amp;sa=D&amp;source=editors&amp;ust=1660576307654541&amp;usg=AOvVaw1OkTJ-zCp3hISnxslZvCOJ">https://doi.org/10.1080/14672715.2014.960707</a></span></p><p class="c3"><span>Rogerson, N. B. F., S. (2001). A moral approach to electronic patient records. </span><span class="c14">Medical Informatics and the Internet in Medicine</span><span>, </span><span class="c14">26</span><span>(3), 219&ndash;234.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/14639230110076412&amp;sa=D&amp;source=editors&amp;ust=1660576307654929&amp;usg=AOvVaw1GkMwovMyuy4WNyqwfvocZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/14639230110076412&amp;sa=D&amp;source=editors&amp;ust=1660576307655103&amp;usg=AOvVaw1glSybkNs9sHv8vfC9w51T">https://doi.org/10.1080/14639230110076412</a></span></p><p class="c3"><span>Rorvik, D. M. (1979). </span><span class="c14">As Man Becomes Machine: The Evolution of the Cyborg</span><span class="c8 c12">. London, UK: Sphere Books Limited.</span></p><p class="c3"><span>Rosenthal-von der P&uuml;tten, A. M., Kr&auml;mer, N. C., Hoffmann, L., Sobieraj, S., &amp; Eimler, S. C. (2013). An Experimental Study on Emotional Reactions Towards a Robot. </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">5</span><span>(1), 17&ndash;34.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-012-0173-8&amp;sa=D&amp;source=editors&amp;ust=1660576307655629&amp;usg=AOvVaw288oEFOszWpY5q1qcG65RH">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-012-0173-8&amp;sa=D&amp;source=editors&amp;ust=1660576307655815&amp;usg=AOvVaw3oOtMlNRuQDcoDGV4GOBaE">https://doi.org/10.1007/s12369-012-0173-8</a></span></p><p class="c3"><span>Roser, M., &amp; Ritchie, H. (2013). Technological Progress. </span><span class="c14">Our World in Data</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://ourworldindata.org/technological-progress&amp;sa=D&amp;source=editors&amp;ust=1660576307656141&amp;usg=AOvVaw1AXNL-j7IWQDC4gdnDyuMT">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://ourworldindata.org/technological-progress&amp;sa=D&amp;source=editors&amp;ust=1660576307656318&amp;usg=AOvVaw2jTSdZY_m_S7bNITF9azus">https://ourworldindata.org/technological-progress</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Ruzich, C. M. (2006). With Deepest Sympathy: Understanding Computer Crashes, Grief, and Loss. In A. De Angeli, S. Brahnam, P. Wallis, &amp; A. Dix (Eds.), </span><span class="c14">Misuse and Abuse of Interactive Technologies</span><span>&nbsp;(pp. 37&ndash;40). Presented at the CHI 2006 Conference on Human Factors in Computing Systems, Montr&eacute;al Qu&eacute;bec Canada: ACM.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307656662&amp;usg=AOvVaw02cBSNpEflXnSA9RS1pf5M">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/1125451.1125753&amp;sa=D&amp;source=editors&amp;ust=1660576307656824&amp;usg=AOvVaw0SfxnvH9eoreVgoTJPs5l0">https://doi.org/10.1145/1125451.1125753</a></span></p><p class="c3"><span>Ryder, R.D. (1992). Painism: Ethics, Animal Rights and Environmentalism. </span><span class="c14">Global Bioethics</span><span>, </span><span class="c14">5</span><span>(4), 27&ndash;35.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/11287462.1992.10800621&amp;sa=D&amp;source=editors&amp;ust=1660576307657194&amp;usg=AOvVaw1yZgQGKD1Ty3lNT_D_cxaR">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/11287462.1992.10800621&amp;sa=D&amp;source=editors&amp;ust=1660576307657404&amp;usg=AOvVaw0N04JI-NRtofSDavp58lBu">https://doi.org/10.1080/11287462.1992.10800621</a></span></p><p class="c3"><span>Ryder, Richard D. (1975). </span><span class="c14">Victims of Science: The Use of Animals in Research</span><span class="c8 c12">. London, UK: Davis Poynter. Accessed 22 December 2021</span></p><p class="c3"><span>Sapontzis, S. F. (1981). A Critique of Personhood. </span><span class="c14">Ethics</span><span>, </span><span class="c14">91</span><span class="c8 c12">(4), 607&ndash;618.</span></p><p class="c3"><span>Savulescu, J., &amp; Bostrom, N. (2009). </span><span class="c14">Human Enhancement</span><span class="c8 c12">. OUP Oxford.</span></p><p class="c3"><span>Scheutz, M., &amp; Crowell, C. R. (2007). The Burden of Embodied Autonomy: Some Reflections on the Social and Ethical Implications of Autonomous Robots. In </span><span class="c14">Proceedings of Workshop on Roboethics at ICRA 2007</span><span>. Rome, Italy.</span><span><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.115.6076%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307658294&amp;usg=AOvVaw1Yl8mWex83fcyun4phJsYz">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.115.6076%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;source=editors&amp;ust=1660576307658554&amp;usg=AOvVaw2Hx8oU4W_KPNjx3mat0TPw">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6076&amp;rep=rep1&amp;type=pdf</a></span></p><p class="c3"><span>Schmidt, M., Ganguli-Mitra, A., Torgersen, H., Kelle, A., Deplazes, A., &amp; Biller-Andorno, N. (2009). A priority paper for the societal and ethical aspects of synthetic biology. </span><span class="c14">Systems and Synthetic Biology</span><span>, </span><span class="c14">3</span><span>(1), 3.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11693-009-9034-7&amp;sa=D&amp;source=editors&amp;ust=1660576307659087&amp;usg=AOvVaw3QtW5Hp3Hj8FWi2Wa1BY36">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11693-009-9034-7&amp;sa=D&amp;source=editors&amp;ust=1660576307659262&amp;usg=AOvVaw2MY6jL-cN2x1UFGwAaKS-3">https://doi.org/10.1007/s11693-009-9034-7</a></span></p><p class="c3"><span>Schweighofer, E. (2001). Vor&uuml;berlegungen zu k&uuml;nstlichen Personen: autonome Roboter und intelligente Softwareagenten. </span><span class="c14">Jusletter IT</span><span>, (IRIS).</span><span><a class="c11" href="https://www.google.com/url?q=https://jusletter-it.weblaw.ch/issues/2001/IRIS/TB-3_I4_Schweighofer.html__ONCE%26login%3Dfalse&amp;sa=D&amp;source=editors&amp;ust=1660576307659642&amp;usg=AOvVaw1guHxUN9SQLBKJQb4bp3JS">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://jusletter-it.weblaw.ch/issues/2001/IRIS/TB-3_I4_Schweighofer.html__ONCE%26login%3Dfalse&amp;sa=D&amp;source=editors&amp;ust=1660576307659864&amp;usg=AOvVaw08wUfo-5LMV3Jel_OY06wt">https://jusletter-it.weblaw.ch/issues/2001/IRIS/TB-3_I4_Schweighofer.html__ONCE&amp;login=false</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Scriven, M. (1960). The Compleat Robot: A Prolegomena to Androidology. In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 118&ndash;147). New York, NY: New York University Press.</span></p><p class="c3"><span>Searle, J. (2009). Chinese room argument. </span><span class="c14">Scholarpedia</span><span>, </span><span class="c14">4</span><span>(8), 3100.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4249/scholarpedia.3100&amp;sa=D&amp;source=editors&amp;ust=1660576307660366&amp;usg=AOvVaw10UKerBamRWJlA2VsvsJJi">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4249/scholarpedia.3100&amp;sa=D&amp;source=editors&amp;ust=1660576307660565&amp;usg=AOvVaw11jN4hE6ZIuWYIvLHqQuu-">https://doi.org/10.4249/scholarpedia.3100</a></span></p><p class="c3"><span>Searle, J. R. (1980). Minds, brains, and programs. </span><span class="c14">Behavioral and Brain Sciences</span><span>, </span><span class="c14">3</span><span>(3), 417&ndash;424.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1017/S0140525X00005756&amp;sa=D&amp;source=editors&amp;ust=1660576307660953&amp;usg=AOvVaw1rVWAaIKhS03FvgrOaAU20">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1017/S0140525X00005756&amp;sa=D&amp;source=editors&amp;ust=1660576307661119&amp;usg=AOvVaw0PelJwFqIJ2nBWzovfJu-f">https://doi.org/10.1017/S0140525X00005756</a></span></p><p class="c3"><span>Seibt, J., N&oslash;rskov, M., &amp; Hakli, R. (2014). </span><span class="c14">Sociable Robots and the Future of Social Relations: Proceedings of Robo-Philosophy 2014</span><span class="c8 c12">. IOS Press.</span></p><p class="c3"><span>Severson, R. J. (1997). </span><span class="c14">The Principles of Information Ethics</span><span class="c8 c12">. M.E. Sharpe.</span></p><p class="c3"><span>Shulman, C., &amp; Bostrom, N. (2021). Sharing the World with Digital Minds. In </span><span class="c14">Rethinking Moral Status</span><span>&nbsp;(pp. 306&ndash;326). Oxford University Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/oso/9780192894076.003.0018&amp;sa=D&amp;source=editors&amp;ust=1660576307661694&amp;usg=AOvVaw2QfPHl79y-BtavoVQPe2Ir">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/oso/9780192894076.003.0018&amp;sa=D&amp;source=editors&amp;ust=1660576307661874&amp;usg=AOvVaw0whbHgaset0mX-Q9YCOpac">https://doi.org/10.1093/oso/9780192894076.003.0018</a></span></p><p class="c3"><span>Simon, M. A. (1969). Could There Be a Conscious Automaton? </span><span class="c14">American Philosophical Quarterly</span><span>, </span><span class="c14">6</span><span class="c8 c12">(1), 71&ndash;78.</span></p><p class="c3"><span>Singer, P. (1995). </span><span class="c14">Animal Liberation</span><span class="c8 c12">. Random House.</span></p><p class="c3"><span>Singer, P., &amp; Sagan, A. (2009, December 14). When robots have feelings. </span><span class="c14">The Guardian</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.theguardian.com/commentisfree/2009/dec/14/rage-against-machines-robots&amp;sa=D&amp;source=editors&amp;ust=1660576307662512&amp;usg=AOvVaw3KdMp3fuUEPtsFFzGOpupK">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.theguardian.com/commentisfree/2009/dec/14/rage-against-machines-robots&amp;sa=D&amp;source=editors&amp;ust=1660576307662776&amp;usg=AOvVaw1Hf-SRo66YzUuj048WAp6f">https://www.theguardian.com/commentisfree/2009/dec/14/rage-against-machines-robots</a></span><span class="c8 c12">. Accessed 22 December 2021</span></p><p class="c3"><span>Siponen, M. (2000). Is Polyinstantation Morally Blameworthy? </span><span class="c14">AMCIS 2000 Proceedings</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://aisel.aisnet.org/amcis2000/227&amp;sa=D&amp;source=editors&amp;ust=1660576307663121&amp;usg=AOvVaw1ArfdDpLj6iOSO_MNFpYj5">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://aisel.aisnet.org/amcis2000/227&amp;sa=D&amp;source=editors&amp;ust=1660576307663290&amp;usg=AOvVaw2rFm5MANN69Sk-Vynby4BX">https://aisel.aisnet.org/amcis2000/227</a></span></p><p class="c3"><span>Slater, M., Antley, A., Davison, A., Swapp, D., Guger, C., Barker, C., et al. (2006). A Virtual Reprise of the Stanley Milgram Obedience Experiments. </span><span class="c14">PLOS ONE</span><span>, </span><span class="c14">1</span><span>(1), e39.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0000039&amp;sa=D&amp;source=editors&amp;ust=1660576307663724&amp;usg=AOvVaw0_fKMhxXsjp149dTXbYRhi">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1371/journal.pone.0000039&amp;sa=D&amp;source=editors&amp;ust=1660576307663901&amp;usg=AOvVaw1jSvLFXeHqVc-V5jwhbWRr">https://doi.org/10.1371/journal.pone.0000039</a></span></p><p class="c3"><span>Solum, L. B. (1992). Legal Personhood for Artificial Intelligences. </span><span class="c14">North Carolina Law Review</span><span>, </span><span class="c14">70</span><span class="c8 c12">, 1231&ndash;1287.</span></p><p class="c3"><span>Solum, L. B. (2001). To Our Children&rsquo;s Children&rsquo;s Children: The Problems of Intergenerational Ethics. </span><span class="c14">Loyola of Los Angeles Law Review</span><span>, </span><span class="c14">35</span><span class="c8 c12">, 163.</span></p><p class="c3"><span>Solum, L. B. (2014). Artificial Meaning. </span><span class="c14">Washington Law Review</span><span>, </span><span class="c14">89</span><span class="c8 c12">, 69.</span></p><p class="c3"><span>Solum, L. B. (2019). </span><span class="c14">Artificially Intelligent Law</span><span>&nbsp;(SSRN Scholarly Paper No. ID 3337696). Rochester, NY: Social Science Research Network.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2139/ssrn.3337696&amp;sa=D&amp;source=editors&amp;ust=1660576307664751&amp;usg=AOvVaw1TRJSXUW1DRufH5JaCfHWz">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.2139/ssrn.3337696&amp;sa=D&amp;source=editors&amp;ust=1660576307664922&amp;usg=AOvVaw0-iujEgxDA4tquczxvOKDj">https://doi.org/10.2139/ssrn.3337696</a></span></p><p class="c3"><span>S&oslash;raker, J. H. (2006a). The Moral Status of Information and Information Technologies: A Relational Theory of Moral Status. In S. Hongladarom &amp; C. Ess (Eds.), </span><span class="c14">Information Technology Ethics: Cultural Perspectives</span><span class="c8 c12">&nbsp;(pp. 1&ndash;19). Hershey, PA: Idea Group Reference.</span></p><p class="c3"><span>S&oslash;raker, J. H. (2006b). The role of pragmatic arguments in computer ethics. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">8</span><span>(3), 121&ndash;130.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-9119-x&amp;sa=D&amp;source=editors&amp;ust=1660576307665426&amp;usg=AOvVaw3kYiTl7CXw0l5hYtBUkVho">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-9119-x&amp;sa=D&amp;source=editors&amp;ust=1660576307665609&amp;usg=AOvVaw16fAXU5dMAywlRxc6AFFC9">https://doi.org/10.1007/s10676-006-9119-x</a></span></p><p class="c3"><span>Sotala, K., &amp; Gloor, L. (2017). Superintelligence As a Cause or Cure For Risks of Astronomical Suffering. </span><span class="c14">Informatica</span><span>, </span><span class="c14">41</span><span>(4).</span><span><a class="c11" href="https://www.google.com/url?q=https://www.informatica.si/index.php/informatica/article/view/1877&amp;sa=D&amp;source=editors&amp;ust=1660576307666026&amp;usg=AOvVaw10E7SKWrLIU3s03wWhR4e9">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.informatica.si/index.php/informatica/article/view/1877&amp;sa=D&amp;source=editors&amp;ust=1660576307666230&amp;usg=AOvVaw1uXAL05Z8IovD3TRz4CwLB">https://www.informatica.si/index.php/informatica/article/view/1877</a></span><span class="c8 c12">. Accessed 25 November 2021</span></p><p class="c3"><span>Sparrow, R. (2004). The Turing Triage Test. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">6</span><span>(4), 203&ndash;213.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-004-6491-2&amp;sa=D&amp;source=editors&amp;ust=1660576307666746&amp;usg=AOvVaw340hTM_hXHKW4499i33qiZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-004-6491-2&amp;sa=D&amp;source=editors&amp;ust=1660576307667002&amp;usg=AOvVaw0SoBeScF8Gt2y5-U64PZ2Y">https://doi.org/10.1007/s10676-004-6491-2</a></span></p><p class="c3"><span>Spence, P. R., Edwards, A., &amp; Edwards, C. (2018). Attitudes, Prior Interaction, and Petitioner Credibility Predict Support for Considering the Rights of Robots. In </span><span class="c14">Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction</span><span>&nbsp;(pp. 243&ndash;244). New York, NY, USA: Association for Computing Machinery.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/3173386.3177071&amp;sa=D&amp;source=editors&amp;ust=1660576307667621&amp;usg=AOvVaw24lix7WJ_P9Xr7zBQdRXAY">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1145/3173386.3177071&amp;sa=D&amp;source=editors&amp;ust=1660576307667894&amp;usg=AOvVaw0AV1APCGiOTefVW08bjVh3">https://doi.org/10.1145/3173386.3177071</a></span></p><p class="c3"><span>Spennemann, D. H. R. (2007). On the Cultural Heritage of Robots. </span><span class="c14">International Journal of Heritage Studies</span><span>, </span><span class="c14">13</span><span>(1), 4&ndash;21.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/13527250601010828&amp;sa=D&amp;source=editors&amp;ust=1660576307668565&amp;usg=AOvVaw2euXFMIbF9zoiJuYD1xQz5">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1080/13527250601010828&amp;sa=D&amp;source=editors&amp;ust=1660576307668838&amp;usg=AOvVaw1Ri86p0yHm-4pMLHo3TUab">https://doi.org/10.1080/13527250601010828</a></span></p><p class="c3"><span>Stahl, B. C., &amp; Coeckelbergh, M. (2016). Ethics of healthcare robotics: Towards responsible research and innovation. </span><span class="c14">Robotics and Autonomous Systems</span><span>, </span><span class="c14">86</span><span>, 152&ndash;161.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.robot.2016.08.018&amp;sa=D&amp;source=editors&amp;ust=1660576307669515&amp;usg=AOvVaw2FXQB-lzNIe4zTxyWS6s8w">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.robot.2016.08.018&amp;sa=D&amp;source=editors&amp;ust=1660576307669716&amp;usg=AOvVaw3IFARF1HocZZx7p7nyjVg9">https://doi.org/10.1016/j.robot.2016.08.018</a></span></p><p class="c3"><span>Starmans, C., &amp; Friedman, O. (2016). If I am free, you can&rsquo;t own me: Autonomy makes entities less ownable. </span><span class="c14">Cognition</span><span>, </span><span class="c14">148</span><span>, 145&ndash;153.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.cognition.2015.11.001&amp;sa=D&amp;source=editors&amp;ust=1660576307670354&amp;usg=AOvVaw1p8EaOuq7PsvPYxNXhoQet">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.cognition.2015.11.001&amp;sa=D&amp;source=editors&amp;ust=1660576307670640&amp;usg=AOvVaw3kfzkliRyGVR3uzavKxHoZ">https://doi.org/10.1016/j.cognition.2015.11.001</a></span></p><p class="c3"><span>Steinert, S. (2013). The Five Robots&mdash;A Taxonomy for Roboethics. </span><span class="c14">International Journal of Social Robotics</span><span>, </span><span class="c14">6</span><span>, 249&ndash;260.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-013-0221-z&amp;sa=D&amp;source=editors&amp;ust=1660576307671297&amp;usg=AOvVaw0B_F0Fy6FD1i7Nnvas0HKJ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s12369-013-0221-z&amp;sa=D&amp;source=editors&amp;ust=1660576307671589&amp;usg=AOvVaw0JvfN1iDlatrcIX4IR1NnU">https://doi.org/10.1007/s12369-013-0221-z</a></span></p><p class="c3"><span>Stone, C. D. (1972). Should Trees Have Standing--Toward Legal Rights for Natural Objects. </span><span class="c14">Southern California Law Review</span><span>, </span><span class="c14">45</span><span class="c8 c12">, 450&ndash;501.</span></p><p class="c3"><span>Sudia, F. W. (2001). A Jurisprudence of Artilects: Blueprint for a Synthetic Citizen. </span><span class="c14">Journal of Futures Studies</span><span>, </span><span class="c14">6</span><span class="c8 c12">(2), 65&ndash;80.</span></p><p class="c3"><span>Sullins, J. P. (2005). Ethics and Artificial life: From Modeling to Moral Agents. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">7</span><span>(3), 139&ndash;148.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-0003-5&amp;sa=D&amp;source=editors&amp;ust=1660576307672886&amp;usg=AOvVaw3qDm_9mHWnsRio5_h2tnQZ">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-006-0003-5&amp;sa=D&amp;source=editors&amp;ust=1660576307673183&amp;usg=AOvVaw0kPeWF7F7NPGfEg-vAXwIZ">https://doi.org/10.1007/s10676-006-0003-5</a></span></p><p class="c3"><span>Sullins, J. P. (2009). Artificial Moral Agency in Technoethics. In R. Luppicini &amp; R. Adell (Eds.), </span><span class="c14">Handbook of Research on Technoethics</span><span>&nbsp;(pp. 205&ndash;221). Information Science Reference.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4018/978-1-60566-022-6.ch014&amp;sa=D&amp;source=editors&amp;ust=1660576307673625&amp;usg=AOvVaw13if7MlihSm5ZOE8BbTtaK">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.4018/978-1-60566-022-6.ch014&amp;sa=D&amp;source=editors&amp;ust=1660576307673807&amp;usg=AOvVaw1o74xpzvIc2vBJwJ50Wd1g">https://doi.org/10.4018/978-1-60566-022-6.ch014</a></span></p><p class="c3"><span>Swiderska, A., &amp; K&uuml;ster, D. (2018). Avatars in Pain: Visible Harm Enhances Mind Perception in Humans and Robots. </span><span class="c14">Perception</span><span>, </span><span class="c14">47</span><span>(12), 1139&ndash;1152.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/0301006618809919&amp;sa=D&amp;source=editors&amp;ust=1660576307674480&amp;usg=AOvVaw36qnBdLPNHOm0FYPqF-B1s">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/0301006618809919&amp;sa=D&amp;source=editors&amp;ust=1660576307674760&amp;usg=AOvVaw0IFJITTJJWp6-5EyBQHT4s">https://doi.org/10.1177/0301006618809919</a></span></p><p class="c3"><span>Tavani, H. T. (2002). The uniqueness debate in computer ethics: What exactly is at issue, and why does it matter? </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">4</span><span>(1), 37&ndash;54.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1015283808882&amp;sa=D&amp;source=editors&amp;ust=1660576307675444&amp;usg=AOvVaw29vAqjBarnnOjRdvZHZvLg">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1023/A:1015283808882&amp;sa=D&amp;source=editors&amp;ust=1660576307675732&amp;usg=AOvVaw1nzOKSHQLEkONL8MY_H8T2">https://doi.org/10.1023/A:1015283808882</a></span></p><p class="c3"><span>Taylor, P. W. (1981). The Ethics of Respect for Nature. </span><span class="c14">Environmental Ethics, 3</span><span class="c8 c12">(3), 197-218.</span></p><p class="c3"><span>Taylor, P. W. (2011). </span><span class="c14">Respect for Nature: A Theory of Environmental Ethics</span><span>. Princeton, NJ: Princeton University Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://press.princeton.edu/books/paperback/9780691150246/respect-for-nature&amp;sa=D&amp;source=editors&amp;ust=1660576307676651&amp;usg=AOvVaw0J34jb2FOy0h_3vs4l0gtn">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://press.princeton.edu/books/paperback/9780691150246/respect-for-nature&amp;sa=D&amp;source=editors&amp;ust=1660576307677012&amp;usg=AOvVaw2zl7Tv6PEVxVeuBj3JzH0H">https://press.princeton.edu/books/paperback/9780691150246/respect-for-nature</a></span><span class="c8 c12">. Accessed 22 December 2021</span></p><p class="c3"><span>The American Society for the Prevention of Cruelty to Robots. (1999a). Frequently Asked Questions.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.aspcr.com/newcss_faq.html&amp;sa=D&amp;source=editors&amp;ust=1660576307677521&amp;usg=AOvVaw1SoBa_QnKRRmiPjKjMK19g">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.aspcr.com/newcss_faq.html&amp;sa=D&amp;source=editors&amp;ust=1660576307677875&amp;usg=AOvVaw2ldOYdwNV-h-hIC5v0uTrg">http://www.aspcr.com/newcss_faq.html</a></span><span class="c8 c12">. Accessed 17 November 2021</span></p><p class="c3"><span>The American Society for the Prevention of Cruelty to Robots. (1999b). What is a Robot?</span><span><a class="c11" href="https://www.google.com/url?q=http://www.aspcr.com/newcss_robots.html&amp;sa=D&amp;source=editors&amp;ust=1660576307678363&amp;usg=AOvVaw052Hl0QymtJl-L0GpctfOH">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.aspcr.com/newcss_robots.html&amp;sa=D&amp;source=editors&amp;ust=1660576307678658&amp;usg=AOvVaw0mbcSBgQ8iJwHMC3UNfw7s">http://www.aspcr.com/newcss_robots.html</a></span><span class="c8 c12">. Accessed 17 November 2021</span></p><p class="c3"><span>Thompson, D. (1965). Can a machine be conscious?1. </span><span class="c14">The British Journal for the Philosophy of Science</span><span>, </span><span class="c14">16</span><span>(61), 33&ndash;43.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/bjps/XVI.61.33&amp;sa=D&amp;source=editors&amp;ust=1660576307679326&amp;usg=AOvVaw0_aHMV77TDh6ETA7e2ixVP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/bjps/XVI.61.33&amp;sa=D&amp;source=editors&amp;ust=1660576307679620&amp;usg=AOvVaw1yNVQKoGg3t7hqrnNkR7re">https://doi.org/10.1093/bjps/XVI.61.33</a></span></p><p class="c3"><span>Thornhill, J. (2017, March 3). Philosopher Daniel Dennett on AI, robots and religion. </span><span class="c14">Financial Times</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.ft.com/content/96187a7a-fce5-11e6-96f8-3700c5664d30&amp;sa=D&amp;source=editors&amp;ust=1660576307680236&amp;usg=AOvVaw3wIz_Qmtw7O1D5PmDtULPw">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.ft.com/content/96187a7a-fce5-11e6-96f8-3700c5664d30&amp;sa=D&amp;source=editors&amp;ust=1660576307680562&amp;usg=AOvVaw3X5QoKHGEiGKjsKVdsJPyD">https://www.ft.com/content/96187a7a-fce5-11e6-96f8-3700c5664d30</a></span><span class="c8 c12">. Accessed 19 November 2021</span></p><p class="c3"><span>Tomasik, B. (2011). Risks of Astronomical Future Su&#64256;ering. </span><span class="c14">Center on Long-Term Risk</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://longtermrisk.org/files/risks-of-astronomical-future-suffering.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307681234&amp;usg=AOvVaw0ROne2QjusnEaJSRZgTV8O">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://longtermrisk.org/files/risks-of-astronomical-future-suffering.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307681603&amp;usg=AOvVaw2IXnox8XHnnXIrKAZi8TcS">https://longtermrisk.org/files/risks-of-astronomical-future-suffering.pdf</a></span></p><p class="c3"><span>Tomasik, B. (2014). Do Artificial Reinforcement-Learning Agents Matter Morally? </span><span class="c14">arXiv:1410.8233 [cs]</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://arxiv.org/abs/1410.8233&amp;sa=D&amp;source=editors&amp;ust=1660576307682207&amp;usg=AOvVaw2b0IFYAOsfkJsHf94ZOsKh">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://arxiv.org/abs/1410.8233&amp;sa=D&amp;source=editors&amp;ust=1660576307682488&amp;usg=AOvVaw1K76FV_VLleAeN_hTjmjtq">http://arxiv.org/abs/1410.8233</a></span><span class="c8 c12">. Accessed 29 November 2021</span></p><p class="c3"><span>Tomasik, B. (2015). A Dialogue on Suffering Subroutines. </span><span class="c14">Center on Long-Term Risk</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://longtermrisk.org/a-dialogue-on-suffering-subroutines/&amp;sa=D&amp;source=editors&amp;ust=1660576307683102&amp;usg=AOvVaw0dLD8ACvBBf8-K1vWNbXDA">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://longtermrisk.org/a-dialogue-on-suffering-subroutines/&amp;sa=D&amp;source=editors&amp;ust=1660576307683423&amp;usg=AOvVaw3cShxhChuEW4Jax2-4ssfR">https://longtermrisk.org/a-dialogue-on-suffering-subroutines/</a></span><span class="c8 c12">. Accessed 25 November 2021</span></p><p class="c3"><span>Tonkens, R. (2012). Out of character: on the creation of virtuous machines. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">14</span><span>(2), 137&ndash;149.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9290-1&amp;sa=D&amp;source=editors&amp;ust=1660576307684118&amp;usg=AOvVaw1KXYUtfsW5kxDdm3kwOA6B">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-012-9290-1&amp;sa=D&amp;source=editors&amp;ust=1660576307684395&amp;usg=AOvVaw2u1sohkS0A7zhpOit7o5H6">https://doi.org/10.1007/s10676-012-9290-1</a></span></p><p class="c3"><span>Torrance, S. (1984). Editor&rsquo;s Introduction: Philosophy and AI: Some Issues. In S. Torrance (Ed.), </span><span class="c14">The Mind and the Machine: Philosophical Aspects of Artificial Intelligence</span><span class="c8 c12">&nbsp;(pp. 11&ndash;28). Chichester, UK: Ellis Horwood.</span></p><p class="c3"><span>Torrance, S. (2000). Towards an Ethics for Epersons. </span><span class="c14">AISB Quarterly</span><span class="c8 c12">, 38&ndash;41.</span></p><p class="c3"><span>Torrance, S. (2005). A Robust View of Machine Ethics. In </span><span class="c14">Papers from the 2005 AAAI Fall Symposium</span><span class="c8 c12">&nbsp;(pp. 88&ndash;93). Menlo Park, CA: The AAAI Press.</span></p><p class="c3"><span>Torrance, S. (2008). Ethics and Consciousness in Artificial Agents. </span><span class="c14">AI and Society</span><span>, </span><span class="c14">22</span><span>(4), 495&ndash;521.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-007-0091-8&amp;sa=D&amp;source=editors&amp;ust=1660576307685814&amp;usg=AOvVaw2YA3EfFoKqVdQe_V26wpXF">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s00146-007-0091-8&amp;sa=D&amp;source=editors&amp;ust=1660576307686124&amp;usg=AOvVaw2Td2q_ONMYdeNe4TA6tCsN">https://doi.org/10.1007/s00146-007-0091-8</a></span></p><p class="c3"><span>Torrance, S., Tamburrini, G., &amp; Datteri, E. (2006). The Ethical Status of Artificial Agents &ndash; With and Without Consciousness. In </span><span class="c14">Ethics of Human Interaction with Robotic, Bionic and AI Systems: Concepts and Policies</span><span class="c8 c12">. Naples, Italy: Italian Institute for Philosophical Studies.</span></p><p class="c3"><span>Treiblmaier, H., Madlberger, M., Knotzer, N., &amp; Pollach, I. (2004). Evaluating personalization and customization from an ethical point of view: an empirical study. In </span><span class="c14">37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the</span><span>&nbsp;(p. 10 pp.-). Presented at the 37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/HICSS.2004.1265434&amp;sa=D&amp;source=editors&amp;ust=1660576307687019&amp;usg=AOvVaw3DDbzr2reryG7YDjJ6WYV8">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/HICSS.2004.1265434&amp;sa=D&amp;source=editors&amp;ust=1660576307687313&amp;usg=AOvVaw1JaCAHjlqScEdUgHU16y_B">https://doi.org/10.1109/HICSS.2004.1265434</a></span></p><p class="c3"><span>Uzgalis, B. (2002). Information Informs the Field: A Conversation with Luciano Floridi. </span><span class="c14">APA Newsletters: Newsletter on Philosophy and Computers</span><span>, </span><span class="c14">2</span><span class="c8 c12">(1), 72&ndash;77.</span></p><p class="c3"><span>Vanman, E. J., &amp; Kappas, A. (2019). &ldquo;Danger, Will Robinson!&rdquo; The challenges of social robots for intergroup relations. </span><span class="c14">Social and Personality Psychology Compass</span><span>, </span><span class="c14">13</span><span>(8), e12489.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/spc3.12489&amp;sa=D&amp;source=editors&amp;ust=1660576307688337&amp;usg=AOvVaw1FIMU9nUqh0R1v9yjMd9Ch">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/spc3.12489&amp;sa=D&amp;source=editors&amp;ust=1660576307688608&amp;usg=AOvVaw3WXBPNrJbaE7tzcrurxda5">https://doi.org/10.1111/spc3.12489</a></span></p><p class="c3"><span>Versenyi, L. (1974). Can Robots be Moral? </span><span class="c14">Ethics</span><span>, </span><span class="c14">84</span><span>(3), 248&ndash;259.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1086/291922&amp;sa=D&amp;source=editors&amp;ust=1660576307689171&amp;usg=AOvVaw1lXz6hKwqegQ1mf89jPqRf">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1086/291922&amp;sa=D&amp;source=editors&amp;ust=1660576307689419&amp;usg=AOvVaw0c3_2nS2QBnIjfQmOwYkZZ">https://doi.org/10.1086/291922</a></span></p><p class="c3"><span>Veruggio, G. (2006). The EURON Roboethics Roadmap. In </span><span class="c14">2006 6th IEEE-RAS International Conference on Humanoid Robots</span><span>&nbsp;(pp. 612&ndash;617). Presented at the 2006 6th IEEE-RAS International Conference on Humanoid Robots.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ICHR.2006.321337&amp;sa=D&amp;source=editors&amp;ust=1660576307689975&amp;usg=AOvVaw3LIs6xCntl3GvB-mrR7rwV">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1109/ICHR.2006.321337&amp;sa=D&amp;source=editors&amp;ust=1660576307690238&amp;usg=AOvVaw0xw9xJraGQTfo9iwb4Jl6t">https://doi.org/10.1109/ICHR.2006.321337</a></span></p><p class="c3"><span>Vigderson, T. (1994). Hamlet II: The Sequel: The Rights of Authors vs. Computer-Generated Read-Alike Works. </span><span class="c14">Loyola of Los Angeles Law Review</span><span>, </span><span class="c14">28</span><span class="c8 c12">, 401.</span></p><p class="c3"><span>Vize, B. (2011). </span><span class="c14">Do Androids Dream of Electric Shocks?</span><span>&nbsp;Victoria University of Wellington. Retrieved from</span><span><a class="c11" href="https://www.google.com/url?q=http://researcharchive.vuw.ac.nz/xmlui/bitstream/handle/10063/1686/thesis.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307691169&amp;usg=AOvVaw3X542WVo1wUlki-Bybw041">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://researcharchive.vuw.ac.nz/xmlui/bitstream/handle/10063/1686/thesis.pdf?sequence%3D2&amp;sa=D&amp;source=editors&amp;ust=1660576307691563&amp;usg=AOvVaw2cz66iK6o8smIMM4WJqIIG">http://researcharchive.vuw.ac.nz/xmlui/bitstream/handle/10063/1686/thesis.pdf?sequence=2</a></span></p><p class="c3"><span>Volkman, R. (2010). Why Information Ethics Must Begin with Virtue Ethics. </span><span class="c14">Metaphilosophy</span><span>, </span><span class="c14">41</span><span>(3), 380&ndash;401.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1467-9973.2010.01638.x&amp;sa=D&amp;source=editors&amp;ust=1660576307692246&amp;usg=AOvVaw0gfeJc9PFaB13YnHn5sc40">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1111/j.1467-9973.2010.01638.x&amp;sa=D&amp;source=editors&amp;ust=1660576307692542&amp;usg=AOvVaw31LHjzH8orLRQ8cgAwOhcq">https://doi.org/10.1111/j.1467-9973.2010.01638.x</a></span></p><p class="c3"><span>Walker, M. (2006). A Moral Paradox in the Creation of Artificial Intelligence: Mary Poppins 3000s of the World Unite. In </span><span class="c14">Human implications of human-robot Interaction: Papers from the AAAI workshop</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.aaai.org/Papers/Workshops/2006/WS-06-09/WS06-09-005.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307693160&amp;usg=AOvVaw2GzLGQZ2R3W_gU01rPTcd6">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.aaai.org/Papers/Workshops/2006/WS-06-09/WS06-09-005.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307693498&amp;usg=AOvVaw2BzqkfLPJOi6mBN06ya_eX">https://www.aaai.org/Papers/Workshops/2006/WS-06-09/WS06-09-005.pdf</a></span></p><p class="c3"><span>Wallach, W., &amp; Allen, C. (2008). </span><span class="c14">Moral Machines: Teaching Robots Right from Wrong</span><span class="c8 c12">. Oxford University Press.</span></p><p class="c3"><span>Ward, A. F., Olsen, A. S., &amp; Wegner, D. M. (2013). The Harm-Made Mind: Observing Victimization Augments Attribution of Minds to Vegetative Patients, Robots, and the Dead. </span><span class="c14">Psychological Science</span><span>, </span><span class="c14">24</span><span>(8), 1437&ndash;1445.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/0956797612472343&amp;sa=D&amp;source=editors&amp;ust=1660576307694158&amp;usg=AOvVaw29Cd3sRTgHBqk5x3p0vYqP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/0956797612472343&amp;sa=D&amp;source=editors&amp;ust=1660576307694394&amp;usg=AOvVaw0LrDI7d2JMGDL3pkepHfEs">https://doi.org/10.1177/0956797612472343</a></span></p><p class="c3"><span>Ware, M., &amp; Mabe, M. (2015). </span><span class="c14">The STM Report: An overview of scientific and scholarly journal publishing</span><span>&nbsp;(p. 181). The Hague, The Netherlands: International Association of Scientific, Technical and Medical Publishers.</span><span><a class="c11" href="https://www.google.com/url?q=https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article%3D1008%26context%3Dscholcom&amp;sa=D&amp;source=editors&amp;ust=1660576307695062&amp;usg=AOvVaw1YOvyC1-8R4-o0np2dRrND">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article%3D1008%26context%3Dscholcom&amp;sa=D&amp;source=editors&amp;ust=1660576307695412&amp;usg=AOvVaw2dTMfIRs0Fl9pWcZ2v5Xx0">https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1008&amp;context=scholcom</a></span></p><p class="c3"><span>Wareham, C. (1AD). On the Moral Equality of Artificial Agents. In R. Luppicini (Ed.), </span><span class="c14">Moral, ethical, and social dilemmas in the age of technology: Theories and practice</span><span>&nbsp;(pp. 70&ndash;78). IGI Global.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.igi-global.com/gateway/chapter/www.igi-global.com/gateway/chapter/73611&amp;sa=D&amp;source=editors&amp;ust=1660576307695979&amp;usg=AOvVaw3sxIIz-tMQmFPx8zn-vjTp">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.igi-global.com/gateway/chapter/www.igi-global.com/gateway/chapter/73611&amp;sa=D&amp;source=editors&amp;ust=1660576307696252&amp;usg=AOvVaw2OmB6WBG8PiY7T15owMyF7">https://www.igi-global.com/gateway/chapter/www.igi-global.com/gateway/chapter/73611</a></span><span class="c8 c12">. Accessed 3 January 2022</span></p><p class="c3"><span>Warwick, K. (2010). Implications and consequences of robots with biological brains. </span><span class="c14">Ethics and Information Technology</span><span>, </span><span class="c14">12</span><span>(3), 223&ndash;234.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-010-9218-6&amp;sa=D&amp;source=editors&amp;ust=1660576307696884&amp;usg=AOvVaw1Jy9Pi5Rpnv8I9r7T1YTsm">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s10676-010-9218-6&amp;sa=D&amp;source=editors&amp;ust=1660576307697149&amp;usg=AOvVaw2Jo618giE9ydeMnPsv5_K-">https://doi.org/10.1007/s10676-010-9218-6</a></span></p><p class="c3"><span>Watanabe, S. (1960). Comments on Key Issues. In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 143&ndash;147). New York, NY: New York University Press.</span></p><p class="c3"><span>Waytz, A., Cacioppo, J., &amp; Epley, N. (2010). Who Sees Human?: The Stability and Importance of Individual Differences in Anthropomorphism. </span><span class="c14">Perspectives on Psychological Science</span><span>, </span><span class="c14">5</span><span>(3), 219&ndash;232.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/1745691610369336&amp;sa=D&amp;source=editors&amp;ust=1660576307698132&amp;usg=AOvVaw2Jfz0mO1_Vr98GRyiYkJyC">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1177/1745691610369336&amp;sa=D&amp;source=editors&amp;ust=1660576307698397&amp;usg=AOvVaw3BOg7-gDQz7k0yJU3edSwg">https://doi.org/10.1177/1745691610369336</a></span></p><p class="c3"><span>Wheeler, M. (2008). God&rsquo;s Machines: Descartes on the Mechanization of Mind. In P. Husbands, O. Holland, &amp; M. Wheeler (Eds.), </span><span class="c14">The Mechanical Mind in History</span><span>&nbsp;(pp. 307&ndash;330). The MIT Press.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.7551/mitpress/9780262083775.003.0013&amp;sa=D&amp;source=editors&amp;ust=1660576307698860&amp;usg=AOvVaw1sGtFJ5UupqRSZLh97AVrP">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.7551/mitpress/9780262083775.003.0013&amp;sa=D&amp;source=editors&amp;ust=1660576307699067&amp;usg=AOvVaw2Gh9_QFF-GauljrHruXhCM">https://doi.org/10.7551/mitpress/9780262083775.003.0013</a></span></p><p class="c3"><span>Whitby, B. (1996). The Potential Moral Duties and Rights of Intelligent Artifacts. In </span><span class="c14">Reflections on Artificial Intelligence</span><span class="c8 c12">&nbsp;(pp. 93&ndash;105). Intellect Books.</span></p><p class="c3"><span>Whitby, B. (2008). Sometimes it&rsquo;s hard to be a robot: A call for action on the ethics of abusing artificial agents. </span><span class="c14">Interacting with Computers</span><span>, </span><span class="c14">20</span><span>(3), 326&ndash;333. Presented at the Interacting with Computers.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.intcom.2008.02.002&amp;sa=D&amp;source=editors&amp;ust=1660576307699619&amp;usg=AOvVaw11PY2WRbRvdfdUXR9to9y9">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1016/j.intcom.2008.02.002&amp;sa=D&amp;source=editors&amp;ust=1660576307699799&amp;usg=AOvVaw0Q-Ua3vlvrRNB0R_43Oopr">https://doi.org/10.1016/j.intcom.2008.02.002</a></span></p><p class="c3"><span>White, B. (1993). Sacrificial Rights: The Conflict between Free Exercise of Religion and Animal Rights. </span><span class="c14">St. John&rsquo;s Journal of Legal Commentary</span><span>, </span><span class="c14">9</span><span class="c8 c12">, 835.</span></p><p class="c3"><span>White, L. (1967). The Historical Roots of Our Ecologic Crisis. </span><span class="c14">Science</span><span>, </span><span class="c14">155</span><span class="c8 c12">(3767), 1203&ndash;1207.</span></p><p class="c3"><span>White, L. (1973). Continuing the Conversation. In I. G. Barbour (Ed.), </span><span class="c14">Western Man and Environmental Ethics: Attitudes Toward Nature and Technology</span><span class="c8 c12">&nbsp;(pp. 55&ndash;64). Addison-Wesley Publishing Company.</span></p><p class="c3"><span>Wiener, N. (1960). The Brain and the Machine (summary). In S. Hook (Ed.), </span><span class="c14">Dimensions of Mind</span><span class="c8 c12">&nbsp;(pp. 113&ndash;117). New York, NY: New York University Press.</span></p><p class="c3"><span>Wikipedia. (2021). List of fictional robots and androids. In </span><span class="c14">Wikipedia</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/w/index.php?title%3DList_of_fictional_robots_and_androids%26oldid%3D1052639591&amp;sa=D&amp;source=editors&amp;ust=1660576307700775&amp;usg=AOvVaw32OCTdgcY7O620Z7XJjGWa">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/w/index.php?title%3DList_of_fictional_robots_and_androids%26oldid%3D1052639591&amp;sa=D&amp;source=editors&amp;ust=1660576307701018&amp;usg=AOvVaw1q0BsoueWLzbSD8MPLP2X-">https://en.wikipedia.org/w/index.php?title=List_of_fictional_robots_and_androids&amp;oldid=1052639591</a></span><span class="c8 c12">. Accessed 17 November 2021</span></p><p class="c3"><span>Wilks, Y. (1975). Putnam and Clarke and Mind and Body. </span><span class="c14">The British Journal for the Philosophy of Science</span><span>, </span><span class="c14">26</span><span>(3), 213&ndash;225.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/bjps/26.3.213&amp;sa=D&amp;source=editors&amp;ust=1660576307701421&amp;usg=AOvVaw20uZ6Cc1XrhXPJ3Vm9iv-A">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1093/bjps/26.3.213&amp;sa=D&amp;source=editors&amp;ust=1660576307701607&amp;usg=AOvVaw02BgfPPh--Asyn3fK_B3Rj">https://doi.org/10.1093/bjps/26.3.213</a></span></p><p class="c3"><span>Wilks, Y. (1985). Responsible Computers? In A. Joshi (Ed.), </span><span class="c14">Proceedings of the Ninth International Joint Conference on Artificial Intelligence</span><span class="c8 c12">&nbsp;(pp. 1279&ndash;1280). Los Altos, CA: Kaufmann.</span></p><p class="c3"><span>Wilks, Y. (1998). Liability and Consent. In A. Narayanan &amp; M. Bennun (Eds.), </span><span class="c14">Law, Computer Science, and Artificial Intelligence</span><span class="c8 c12">. Intellect Books.</span></p><p class="c3"><span>Willick, M. (1985). Constitutional Law and Artificial Intelligence: The Potential Legal Recognition of Computers as &ldquo;Persons&rdquo;. In A. Joshi (Ed.), </span><span class="c14">Proceedings of the Ninth International Joint Conference on Artificial Intelligence</span><span class="c8 c12">&nbsp;(pp. 1271&ndash;1273). Los Altos, CA: Kaufmann.</span></p><p class="c3"><span>Willick, M. S. (1983). Artificial Intelligence: Some Legal Approaches and Implications. </span><span class="c14">AI Magazine</span><span>, </span><span class="c14">4</span><span>(2), 5&ndash;5.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v4i2.392&amp;sa=D&amp;source=editors&amp;ust=1660576307702359&amp;usg=AOvVaw2QmXvji-1x4EbN2tl_0-sm">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1609/aimag.v4i2.392&amp;sa=D&amp;source=editors&amp;ust=1660576307702545&amp;usg=AOvVaw2FBeuz3V3WiI757qNxrupv">https://doi.org/10.1609/aimag.v4i2.392</a></span></p><p class="c3"><span>World Scientific. (2021). Journal of Artificial Intelligence and Consciousness.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.worldscientific.com/page/jaic/aims-scope&amp;sa=D&amp;source=editors&amp;ust=1660576307702848&amp;usg=AOvVaw1otH2d0ujl_dNi9Ix_aWgB">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.worldscientific.com/page/jaic/aims-scope&amp;sa=D&amp;source=editors&amp;ust=1660576307703045&amp;usg=AOvVaw0sqbUAMtVf4WZThqcjUo01">https://www.worldscientific.com/page/jaic/aims-scope</a></span></p><p class="c3"><span>Yampolskiy, R., &amp; Fox, J. (2013). Safety Engineering for Artificial General Intelligence. </span><span class="c14">Topoi</span><span>, </span><span class="c14">32</span><span>(2), 217&ndash;226.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11245-012-9128-9&amp;sa=D&amp;source=editors&amp;ust=1660576307703511&amp;usg=AOvVaw20BjViuAO0yW26CHwMiho6">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11245-012-9128-9&amp;sa=D&amp;source=editors&amp;ust=1660576307703707&amp;usg=AOvVaw3FK-iFAZp-CYLnRjXsWXVQ">https://doi.org/10.1007/s11245-012-9128-9</a></span></p><p class="c3"><span>York, P. F. (2005). Respect for the World: Universal Ethics and the Morality of Terraforming.</span><span><a class="c11" href="https://www.google.com/url?q=https://www.tesionline.it/tesi/respect-for-the-world-universal-ethics-and-the-morality-of-terraforming/15012&amp;sa=D&amp;source=editors&amp;ust=1660576307704078&amp;usg=AOvVaw2kRyAAnyiMRjvhr8OZlZBv">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.tesionline.it/tesi/respect-for-the-world-universal-ethics-and-the-morality-of-terraforming/15012&amp;sa=D&amp;source=editors&amp;ust=1660576307704310&amp;usg=AOvVaw0EjFowZnmgNQjlowJc6ZBO">https://www.tesionline.it/tesi/respect-for-the-world-universal-ethics-and-the-morality-of-terraforming/15012</a></span><span class="c8 c12">. Accessed 11 November 2021</span></p><p class="c3"><span>Young, P. R. (1991). </span><span class="c14">Persons and artificial intelligence</span><span>. The Catholic University of America. Retrieved from</span><span><a class="c11" href="https://www.google.com/url?q=https://www.proquest.com/openview/e780faa1e13d815919c3aa2bbb353892/1?pq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&amp;sa=D&amp;source=editors&amp;ust=1660576307704691&amp;usg=AOvVaw1CRZO3bDqx-Lsnl3iTDer4">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://www.proquest.com/openview/e780faa1e13d815919c3aa2bbb353892/1?pq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&amp;sa=D&amp;source=editors&amp;ust=1660576307704930&amp;usg=AOvVaw00Q9ZVnR5pZCXXchtrM5-R">https://www.proquest.com/openview/e780faa1e13d815919c3aa2bbb353892/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a></span></p><p class="c3"><span>Yudkowsky, E. (1996). Staring Into The Singularity.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.fairpoint.net/~jpierce/staring_into_the_singularity.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307705276&amp;usg=AOvVaw1abvMcr2tOSgK_Mmnd-r5P">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.fairpoint.net/~jpierce/staring_into_the_singularity.htm&amp;sa=D&amp;source=editors&amp;ust=1660576307705482&amp;usg=AOvVaw33QUQQ4N4kjjl-o6NGxlVk">http://www.fairpoint.net/~jpierce/staring_into_the_singularity.htm</a></span><span class="c8 c12">. Accessed 23 November 2021</span></p><p class="c3"><span>Yudkowsky, E. (2008). Artificial Intelligence as a Positive and Negative Factor in Global Risk. In N. Bostrom &amp; M. M. Cirkovic (Eds.), </span><span class="c14">Global Catastrophic Risks</span><span class="c8 c12">&nbsp;(pp. 308&ndash;345). Oxford, UK: OUP Oxford.</span></p><p class="c3"><span>Yudkowsky, E. (2015). Mindcrime. </span><span class="c14">Arbital</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=https://arbital.greaterwrong.com/p/mindcrime?l%3D6v&amp;sa=D&amp;source=editors&amp;ust=1660576307705942&amp;usg=AOvVaw0I_4ONRG3hxALvqxwpoqMu">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://arbital.greaterwrong.com/p/mindcrime?l%3D6v&amp;sa=D&amp;source=editors&amp;ust=1660576307706137&amp;usg=AOvVaw3KHYflLNHgqeBlXxcA19Is">https://arbital.greaterwrong.com/p/mindcrime?l=6v</a></span><span class="c8 c12">. Accessed 29 November 2021</span></p><p class="c3"><span>Zancanaro, M., &amp; Leonardi, C. (2005). A trouble shared is a troubled halved: Disruptive and self-help patterns of usage for co-located interfaces. In A. De Angeli, S. Brahnam, &amp; P. Wallis (Eds.), </span><span class="c14">Proceedings of Abuse: The darker side of Human-Computer Interaction</span><span>.</span><span><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307706493&amp;usg=AOvVaw1YGjsP8wB-Fw6vAbHyGmr1">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=http://www.agentabuse.org/Abuse_Workshop_WS5.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307706672&amp;usg=AOvVaw1inRnYJ4o0AGUkuTPLUxyI">http://www.agentabuse.org/Abuse_Workshop_WS5.pdf</a></span></p><p class="c3"><span>Zhang, D., Mishra, S., Brynjolfsson, E., Etchemendy, J., Ganguli, D., Grosz, B., et al. (2021). </span><span class="c14">The AI Index 2021 Annual Report,</span><span>. Stanford, CA: AI Index Steering Committee, Human-Centered AI Institute, Stanford University.</span><span><a class="c11" href="https://www.google.com/url?q=https://aiindex.stanford.edu/wp-content/uploads/2021/11/2021-AI-Index-Report_Master.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307707110&amp;usg=AOvVaw1MZlBowTV6IRyd_Dp2bSEw">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://aiindex.stanford.edu/wp-content/uploads/2021/11/2021-AI-Index-Report_Master.pdf&amp;sa=D&amp;source=editors&amp;ust=1660576307707346&amp;usg=AOvVaw3PiQFiI1Do9nMfVD2LxBUD">https://aiindex.stanford.edu/wp-content/uploads/2021/11/2021-AI-Index-Report_Master.pdf</a></span><span class="c8 c12">. Accessed 31 December 2021</span></p><p class="c3"><span>Zhu, Q., Williams, T., Jackson, B., &amp; Wen, R. (2020). Blame-Laden Moral Rebukes and the Morally Competent Robot: A Confucian Ethical Perspective. </span><span class="c14">Science and Engineering Ethics</span><span>, </span><span class="c14">26</span><span>(5), 2511&ndash;2526.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-020-00246-w&amp;sa=D&amp;source=editors&amp;ust=1660576307707736&amp;usg=AOvVaw2RpvO9mWV5Ub4tGkXfZB86">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.1007/s11948-020-00246-w&amp;sa=D&amp;source=editors&amp;ust=1660576307707918&amp;usg=AOvVaw0g1tVdf9umrPv8slMqvb0m">https://doi.org/10.1007/s11948-020-00246-w</a></span></p><p class="c3"><span>Ziesche, S., &amp; Yampolskiy, R. (2019). Towards AI Welfare Science and Policies. </span><span class="c14">Big Data and Cognitive Computing</span><span>, </span><span class="c14">3</span><span>(1), 2.</span><span><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.3390/bdcc3010002&amp;sa=D&amp;source=editors&amp;ust=1660576307708291&amp;usg=AOvVaw2Iv5IkMblEGRbiuBwXyUWp">&nbsp;</a></span><span class="c0"><a class="c11" href="https://www.google.com/url?q=https://doi.org/10.3390/bdcc3010002&amp;sa=D&amp;source=editors&amp;ust=1660576307708458&amp;usg=AOvVaw1iBXkmEtfrN5lzqFW72Wh4">https://doi.org/10.3390/bdcc3010002</a></span></p><div><p class="c1 c6 c2"><span class="c8 c12"></span></p></div><hr class="c37"><div><p class="c4 c6"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c5">&nbsp;For discussions of terminology, see Harris and Anthis (2021) and Pauketat (2021).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c5">&nbsp;In the tables below, the authors themselves are excluded from the count. For example, a search for &ldquo;Gunkel&rdquo; returns 87 results of which 14 are articles that David Gunkel wrote or co-authored, so the table below would report the total number as being 73, which is 28.5% of the 256.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c5">&nbsp;E.g., the keyword &ldquo;A Space Odyssey&rdquo; was used rather than &ldquo;2001&rdquo;, which would return any result with a citation dated to 2001.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c5">&nbsp;Petersen (2007) concludes that the topic is &ldquo;strangely neglected,&rdquo; citing a few examples of previous brief discussion of the topic, but missing several relevant streams of literature, such as most of the previous writings on legal rights for artificial entities, transhumanism, and information ethics (see the relevant subsections below).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c5">&nbsp;For some other examples of early, adjacent discussion, see footnote 2 in Thompson (1965).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c5">&nbsp;I have not identified any papers by Minsky from the 1950s explicitly claiming that artificial sentience was possible.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c5">&nbsp;Wiener (1960) added that, &ldquo;[a] slave is expected to have two qualities: intelligence, and subservence. These two qualities are by no means perfectly compatible. The more intelligent the slave is, the more he will insist on his own way of doing things in opposition to the way of doing things imposed on him by his owner. To that extent he will cease to be a slave.&rdquo; </span></p></div><div><p class="c4 c6"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c5">&nbsp;One contributor in the same proceedings (Watanabe, 1960) commented in reply to Scriven (1960) that, &ldquo;[i]f a machine is made out of protein, then it may have consciousness, but a machine made out of vacuum tubes, diodes, and transistors cannot be expected to have consciousness. I do not here offer a proof for this statement, except that it is obvious according to well-disciplined common sense. A &lsquo;conscious&rsquo; machine made out of protein is no longer a machine, it is a man-made animal.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c7">&nbsp;Putnam (1960) closes with the comment that, &ldquo;if the mind-body problem is identified with any problem of more than purely conceptual interest (e.g. with the question of whether human beings have &lsquo;souls&rsquo;), then </span><span class="c7 c14">either </span><span class="c7">it must be that (a) no argument </span><span class="c7 c14">ever </span><span class="c7">used by a philosopher sheds the </span><span class="c7 c14">slightest </span><span class="c7">light on it (and this independently of the way the argument tends), or (b) that some philosophic argument for mechanism is correct, or (c) that some dualistic argument does show that </span><span class="c7 c14">both </span><span class="c7">human beings </span><span class="c7 c14">and </span><span class="c5">Turing machines have souls! I leave it to the reader to decide which of these three alternatives is at all plausible.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c5">&nbsp;Putnam (1964) adds that, &ldquo;[m]y interest in the latter question derives from my just-mentioned conviction: that clarity with respect to the &lsquo;borderline case&rsquo; of robots, if it can only be achieved, will carry with it clarity with respect to the &lsquo;central area&rsquo; of talk about feelings, thoughts, consciousness, life, etc.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c5">&nbsp;The field has moved beyond merely discussing whether consciousness in artificial entities is possible to a proactive effort to create it (Holland &amp; Goodman, 2003; Gamez, 2008; Reggia, 2013), with an academic journal explicitly advancing this goal (World Scientific, 2021).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Gamez&rsquo;s (2008) review described this field of &ldquo;machine consciousness&rdquo; as &ldquo;a relatively new research area that has gained considerable momentum over the last few years.&rdquo; Of the 85 references in the article, 52 (61%) were published in the 2000s and a further 23 (27%) were published in the 1990s.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c5">&nbsp;In an interview (Thornhill, 2017), Dennett summarized that he had been &ldquo;arguing for years that, yes, in principle it&rsquo;s possible for human consciousness to be realised in a machine. After all, that&rsquo;s what we are&hellip; We&rsquo;re robots made of robots made of robots. We&rsquo;re incredibly complex, trillions of moving parts. But they&rsquo;re all non-miraculous robotic parts.&rdquo; For an example of academic discussion, see Dennett (1994).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c7">&nbsp;</span><span class="c7">Dennett was well aware of Putnam&rsquo;s work.</span><span class="c5">&nbsp;For example, Dennett (1978) cites various publications by Putnam, including the 1964 article that mentions &ldquo;civil rights of robots.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c5">&nbsp;Harris and Anthis&rsquo; (2021) systematic search methods identified few publications discussing this topic in much depth before the 21st century, with the earliest item being McNally and Inayatullah (1988).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Searching through the items that have cited Putnam (1964) does reveal a few items that touch on the topic, but the discussion tends to be tangential or brief. For example, Versenyi (1974) was primarily concerned with the question of moral agency of artificial entities, but links this briefly to the idea of moral patiency, noting that &ldquo;whether robots should be blamed or praised, loved or hated, given rights and duties, etc., are in principle the same sort of questions as, &lsquo;Should cars be serviced, cared for, and supplied with what they require for their operation?&rsquo;&rdquo; In the final section of their article, Wilks (1975) examined the arguments of Hilary Putnam and J. J. Clarke, suggesting that although &ldquo;[n]either of them considers the privacy of machines seriously,&rdquo; their arguments nevertheless support &ldquo;a frivolous speculation about the possible privacy of a machine,&rdquo; where observers would &ldquo;ascribe to the machine the final authority as to what state it was in, in the way that we now do for persons.&rdquo; Granting an entity this authority could be interpreted as a form of moral consideration, but it seems less relevant than Putnam&rsquo;s (1964) brief comments about &ldquo;civil rights of robots.&rdquo; Sapontzis (1981) cited Putnam (1964) in &ldquo;a critique of personhood&rdquo; as a useful concept for &ldquo;moral theory and practice,&rdquo; briefly using &ldquo;machines&rdquo; as a contrast to persons.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Lycan (1985) addressed the topic in some depth, arguing that &ldquo;[i]t seems&hellip; possible (as they say) in principle to build our own androids, artificial humans, which would have at least as firm a claim to be called persons as we do&hellip; It would seem that these artificial humans, if they are indeed as clearly entitled to be called persons as we are, will have moral rights of exactly the same sort we have, whatever those rights may be.&rdquo; The &ldquo;main point&rdquo; of the paper is to address the question &ldquo;of whether it is wrong for a mother to abort a pre-viable fetus solely for reasons of convenience,&rdquo; although they note that they discussed &ldquo;the civil rights of robots&hellip; more fully&rdquo; in a lecture at Kansas State University in 1972. The sole publication not by Lycan themselves citing Lycan&rsquo;s (1985) article was about abortion, rather than artificial entities. </span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Hajdin&lsquo;s (1987) PhD thesis contained a section discussing whether &ldquo;highly sophisticated future computers&rdquo; might count as &ldquo;members of the moral community,&rdquo; but accrued no citations. Scheutz and Crowell (2007) address a number of &ldquo;Social and Ethical Implications of Autonomous Robots,&rdquo; though the discussion of the possibility of robot rights is very brief and described as not &ldquo;of pressing urgency, since such questions may only be relevant for robots much more advanced than those available at present.&rdquo; Vize&rsquo;s (2011) master&rsquo;s thesis cites Putnam (1964) prominently in an extensive discussion of the &ldquo;moral considerability&rdquo; of machines; a </span><span class="c7">handful of other publications on the topic from this date onwards have cited the article.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c5">&nbsp;Sullins (2005) noted that the &lsquo;90s saw an &ldquo;initial burst of articles and books&rdquo; discussing artificial life, either with the goal of attempting &ldquo;to describe fundamental qualities of living systems through agent based computer models&rdquo; or to study &ldquo;whether or not we can artificially create living things in computational mediums that can be realized either, virtually in software, or through bio-technology.&rdquo; Sullins provided numerous references for research in this technical field. Sullins commented that philosophers &ldquo;have not helped work through the various ethical issues&rdquo; raised by this literature. They cited discussion by Floridi and Sanders (2004) but not other previous contributions addressing relevant ethical issues, such as the relevant writings from environmental ethics, animal ethics, legal rights for artificial entities, or transhumanism that predated this publication (see the corresponding sections of this report for examples). Sullins&rsquo; (2005) article itself discussed a number of moral issues, including both moral agency and patiency of these entities.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Elton (2000) argued that, like animals, agents in video games engage in &ldquo;cognition&rdquo; and &ldquo;striving&rdquo; to stay alive, and that the possession of these two capacities merits moral consideration. Elton (2000) did not cite previous publications focusing explicitly on the moral consideration of artificial entities, but there are two references to previous discussions of &ldquo;artificial life,&rdquo; and Dennett was also cited. Kim (2004) cited Elton (2000) and numerous publications about &ldquo;artificial life,&rdquo; as well as Freitas (1985) and McNally and Inayatullah (1988) who had previously discussed legal rights for artificial entities.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">A number of publications by </span><span class="c7">Steve Torrance (e.g. 2000, 2007)</span><span class="c7">&nbsp;discussed the possibility of artificial consciousness and then moved on to discuss the ethical implications that this would have for how we treat artificial entities. Most of the citations in Torrance&rsquo;s 2006 and 2007 papers are from cognitive science (e.g. Dennett) or publications on the development of artificial consciousness. There are also a small number of citations relating to legal personhood for machines, though Torrance&rsquo;s interest in the topic may have predated these publications</span><span class="c7">; </span><span class="c7">Torrance (1984) had long previously edited a volume on </span><span class="c7 c14">Philosophical Aspects of Artificial Intelligence</span><span class="c7">, citing Putnam&rsquo;s (1960) &ldquo;Minds and machines&rdquo; and subsequent papers as a key influence, alongside Searle and others addressing the capabilities of machines</span><span class="c7">. Torrance&rsquo;s (1984) introduction focused on the capacities and consciousness of artificial entities, but commented briefly on moral issues, noting (p. 14) that, &ldquo;[a] machine which gave howsoever lifelike an imitation &mdash; but only an imitation &mdash; of pain would not be adding to the sum total of misery in the universe, would not merit our </span><span class="c7 c14">concern</span><span class="c5">, in the way that would a behaviourally indistinguishable machine that really was in agony.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Rodney Brooks, director of the Artificial Intelligence Lab at M.I.T., wrote an article for </span><span class="c7 c14">Time </span><span class="c5">magazine (2000) noting that, &ldquo;[a]rtificial life forms that &lsquo;live&rsquo; inside computers have evolved to the point where they can chase prey, evade predators and compete for limited resources&rdquo; and commented briefly that &ldquo;these endeavors will eventually lead to robots to which we will want to extend the same inalienable rights that humans enjoy.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Metzinger (2013) argued against the creation of conscious artificial entities, to avoid the problem of &ldquo;artificial suffering.&rdquo; Most of the citations were to previous literature on artificial life and consciousness; Metzinger appears to have applied (negative) utilitarian ethics to the problem, without reference to previous works discussing the moral consideration of artificial entities.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c5">&nbsp;Schmidt et al. (2009) and Holm and Powell (2013) did the same, except for the latter publication citing Douglas and Savulescu (2010). There are some indirect connections between this stream of literature and other streams addressing the moral consideration of artificial entities. For example, Julian Savulescu, prior to co-authoring the Douglas and Savulescu (2010) paper, had co-edited Savulescu and Bostrom (2009), and so may well have been aware of relevant ideas from the early transhumanist writers that related to the moral consideration of artificial entities (see the relevant subsection below). Sullins (2009) focused primarily on moral agency rather than moral patiency, but noted that &ldquo;[a]rtificial autonomous agents can be separated into three categories: synthetic biological constructs, robots, and software agents.&rdquo; There is also at least some direct overlap in authors addressing both topics. For example, John Basl has written specifically about the moral consideration of both machines (e.g. Basl, 2014) and the creations of synthetic biology (e.g. Basl &amp; Sandler, 2013).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c5">&nbsp;Taylor (2011, pp. 122-4) argued that, unlike &ldquo;[a]ll organisms, whether conscious or not&hellip; inanimate objects&rdquo; cannot be &ldquo;a teleological center of life&hellip; This point holds even for those complex mechanisms (such as self-monitoring space satellites, chess-playing computers, and assembly-line &lsquo;robots&rsquo;) that have been constructed by humans to function in a quasi-autonomous, self-regulating manner in the process of accomplishing certain purposes&hellip; machines do not, as independent entities, have a good of their own. Their &lsquo;good&rsquo; is &lsquo;furthered&rsquo; only insofar as they are treated in such a way as to be an effective means to human ends.&rdquo; However, Taylor (2011, pp 124-5) added that &ldquo;this difference between mechanism and organism may no longer be maintainable with regard to those complex electronic devices now being developed under the name of artificial intelligence. Perhaps some day computer scientists and engineers will construct beings whose internal processes and electrical responses to their surroundings closely parallel the functions of the human brain and nervous system. Concerning such beings we may begin to speak of their having a good of their own independently of the purposes of their creators. At that point the distinction drawn above between living things and inanimate machines may break down. It is best, I think, to have an open mind about this.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c7">&nbsp;Stone (1987, pp. 28-9) asked &ldquo;what place need we make in law and morals for robots, artificial intelligence (A.I), and clones? Is the day so far off that we will be wondering what obligations we ought to hold toward, even expect of, </span><span class="c7 c14">them</span><span class="c5">?&rdquo; The discussion on the following two pages focused, however, on &ldquo;questions regarding the liability of the manufacturer&rdquo; and &ldquo;the liability of the A.I. itself&rdquo; in instances where damages or accidents occur. Subsequently, Stone (1987, p. 47-8) uses AI as an example of &ldquo;utterly disinterested entities devoid of feelings or interests except in the most impoverished or even metaphorical sense,&rdquo; but argues that a legal guardian might nevertheless &ldquo;be empowered to speak for&rdquo; them.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c5">&nbsp;Lehman-Wilzig (1981) freely admitted to &ldquo;preliminary discussion&rdquo; of potential future developments and to &ldquo;jurisprudential speculation,&rdquo; which he noted that &ldquo;the Anglo-Saxon legal tradition is generally averse to.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref20" id="ftnt20">[20]</a><span class="c5">&nbsp;A reference to Hook&rsquo;s (1960) volume is provided as further support for this idea. This reference suggests that Lehman-Wilzig (1981) was presumably aware of Putnam and some of the other contributors discussing artificial life and consciousness.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref21" id="ftnt21">[21]</a><span class="c5">&nbsp;Both elements were used to illustrate Lehman-Wilzig&rsquo;s &ldquo;four general categories of AI harmful behaviour.&rdquo; Science fiction from past centuries also features prominently in the title (&ldquo;Frankenstein Unbound&rdquo;) and introduction. The article contained a &ldquo;review of the actual (or theoretically proven) powers of artificially intelligent machine automata and the likely advances to be made in the future,&rdquo; with citations of contributors to the field of AI like Minsky and Wiener (1960). </span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Lehman-Wilzig (1981) also referred to a passage in Rorvik&rsquo;s (1979; first edition 1970, p. 156) forward-looking book that quoted &ldquo;Dr N. S. Sutherland, the computer expert who believes, nonetheless, that within fifty years we will be arguing over whether computers should be entitled to the vote.&rdquo; Comments with such direct relevance to moral consideration were relatively rare in Rorvik&rsquo;s book, however, which focused mostly developments in the capacities of artificial entities and various types of social interaction with machines, from robots assisting with domestic tasks, to robotic sexual partners, to symbiosis with machines.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref22" id="ftnt22">[22]</a><span class="c7">&nbsp;The article posed several open questions that seem adjacent to the idea of moral consideration, such as whether artificial entities might in the future have consciousness or free will. A footnote in the conclusion noted that, &ldquo;[t]he problem here is not merely how does one relate to the humanoid if it transgresses the law; even more delicate is the question of how the law will deal with those humans who injure a humanoid. Is shooting one to be considered murder?&rdquo; An earlier footnote noted that, </span><span class="c7">&ldquo;Dr N. S. Sutherland, Professor of Experimental Psychology at the University of Sussex (and a computer expert) </span><span class="c5">suggests that by the 21st century human society will be grappling with the problem of whether AI robots should be allowed to vote. From such enfranchisement it is but a small step to AI leadership.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref23" id="ftnt23">[23]</a><span class="c5">&nbsp;In the 21st century, Lehman-Wilzig began writing a few other articles about the societal implications of certain technologies, but most of his 20th-century work seems to have focused on politics and public protest in Israel.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref24" id="ftnt24">[24]</a><span class="c7">&nbsp;Of these, the first to cite Lehman-Wilzig (1981) for explicit discussion of legal futures for artificial entities seems to have been Hu&rsquo;s (1987) brief advice to &ldquo;software engineers and managers&rdquo; about the possible &ldquo;Establishment of New Computer Criminal Laws,&rdquo; though this is not linked to moral consideration. With more relevance, McNally and Inayatullah (1988) cited Lehman-Wilzig (1981) for discussion about robot rights, as summarized below. Wilks (1998) cited it in a discussion of various legal precedents relevant to &ldquo;computer science and artificial intelligence,&rdquo; but not to discuss granting greater legal rights or responsibilities to artificial entities. Besides, Wilks (1975) had already considered the topic many years previously. Bartneck </span><span class="c7">(2004)</span><span class="c7">&nbsp;discussed legal futures through the lens of science fiction and briefly asserted that, &ldquo;[t]he arrival of studies into the ethical (Dennet, 1997) and legal (Lehman-Wilzig, 1981) aspects of human-robot interaction shows that the integration of robots in our society in immanent.&rdquo; Bartneck et al. (2007) used a similarly brief reference as part of a discussion that seems more directly relevant to moral consideration; hesitation in switching off a robot. Spennemann (2007) cited Lehman-Wilzig (1981) as one of numerous references in a section explicitly about &ldquo;What Rights Do AI Robots Have?&rdquo; Levy (2009) cited it in discussion of &ldquo;The Ethical Treatment of Artificially Conscious Robots.&rdquo; The number of references increased </span><span class="c7">somewhat thereafte</span><span class="c7">r, although not all for the discussion of granting legal rights or moral consideration to artificial entities.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref25" id="ftnt25">[25]</a><span class="c7">&nbsp;Willick (1983) cites &ldquo;P McCorduck, </span><span class="c7 c14">Machines Who Think</span><span class="c5">&nbsp;(1979),&rdquo; an earlier edition of McCorduck (2004). McCorduck (2004) includes various discussion about the capabilities of AI and other machines, citing Minsky, Turing, and various others. McCorduck (2004) also offers some explicit moral commentary, such as that &ldquo;[f]aced with an uppity machine, we&rsquo;ve always known we could pull the plug as a last resort, but if we accept the idea of an intelligent machine, we&rsquo;re going to be stuck with a moral dilemma in pulling that plug, one we&rsquo;ve hardly worked out intraspecies&rdquo; (p. 198). However, the comment that Willick (1983) cites McCorduck for regarding &ldquo;recognition of artificially intelligent machines as persons&rdquo; appears in McCorduck (2004, p. 238) to actually be about &ldquo;intelligence&rdquo; and capacity for &ldquo;thinking,&rdquo; not about legal personhood specifically.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref26" id="ftnt26">[26]</a><span class="c5">&nbsp;Hu (1987) cited Willick (1983) for brief advice to &ldquo;software engineers and managers&rdquo; about the possible &ldquo;Establishment of New Computer Criminal Laws,&rdquo; though this was not linked to moral consideration. Boden (1984) cited Willick (1983) for the brief comment that, &ldquo;[w]hether computer-systems can truly be said to have intentions, the capacity to engage in frolic, or even rights [Willick, 1983] may thus be questions of more than merely academic interest&rdquo; in an article about &ldquo;artificial intelligence and social forecasting.&rdquo; Most relevantly, perhaps, Fields (1987), discussed below, cited Willick&rsquo;s (1985) conference presentation, which was &ldquo;largely abstracted from&rdquo; Willick&rsquo;s (1983) article.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref27" id="ftnt27">[27]</a><span class="c5">&nbsp;Willick has no Google Scholar page, but Google and Google Scholar searches reveal no other seemingly relevant papers, e.g. see the list of publications at lawyers.com (2022).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref28" id="ftnt28">[28]</a><span class="c7">&nbsp;The first citation seems to have been Petrina et al.&lsquo;s (2004) brief mention of Freitas (1985) as an example of robot rights in a broader discussion of &ldquo;Technology and Rights.&rdquo; Thereafter,</span><span class="c7">&nbsp;citations picked up</span><span class="c7">. Gunkel (</span><span class="c7">2018</span><span class="c5">) offers comments on why Freitas (1985) had, at that time, had &ldquo;less than twenty citations in the past thirty-five years. This may be the result of: the perceived status (or lack thereof) of the journal, which is not a major venue for peer-reviewed research, but a magazine published by the student division of the American Bar Association; a product of some confusion concerning the essay&rsquo;s three different titles; the fact that the article is not actually an article but an &lsquo;end note&rsquo; or epilogue; or simply an issue of timing, insofar as the questions Freitas raises came well in advance of robot ethics or even wide acceptance of computer ethics.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref29" id="ftnt29">[29]</a><span class="c5">&nbsp;Like Willick (1983), LaChat (1986) cited the 1979 edition of McCorduck (2004), which contains some brief moral commentary but focuses more on the development of and debates about the capacities of AI.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref30" id="ftnt30">[30]</a><span class="c5">&nbsp;Fields (1987) cited two earlier articles that explicitly &ldquo;raise the interesting possibility that intelligent artifacts may be considered non-tools, and perhaps persons.&rdquo; One of these is Wilks (1985), who in turn cited Lehman-Wilzig (1981). However, Wilks (1975) had addressed the topic before Lehman-Wilzig (1981), influenced primarily by the arguments of Hilary Putnam and J. J. Clarke. The other earlier article cited by Fields (1987) is Willick (1985); this paper contained no citations except to note that it is &ldquo;largely abstracted from&rdquo; Willick (1983), discussed above. Both Wilks (1985) and Willick (1985) were presented at the same conference (the Ninth International Joint Conference on Artificial Intelligence); given that both authors had addressed the topic before, Fields&rsquo; (1987) decision to cite these two particular papers suggests that Fields was influenced by attendance at that conference.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref31" id="ftnt31">[31]</a><span class="c5">&nbsp;Furthermore, most of the articles citing them seem not to be very relevant to the moral consideration of artificial entities. Dolby (1989) has been cited by a few more relevant articles after a couple of decades&rsquo; delay (e.g. Gunkel 2012), but the topic of moral consideration of artificial entities had become more prevalent by that time anyway, as discussed in the subsections below.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref32" id="ftnt32">[32]</a><span class="c7">&nbsp;</span><span class="c7">They also mention various ongoing legal and social questions (e.g. liability for damages and &ldquo;robots&hellip; in our houses).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref33" id="ftnt33">[33]</a><span class="c7">&nbsp;Another quoted contribution that seemingly discusses AI rights explicitly is an </span><span class="c7">article </span><span class="c7">in </span><span class="c7 c14">The Futurist </span><span class="c7">from </span><span class="c5">1986, though I was unable to find a copy of this.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref34" id="ftnt34">[34]</a><span class="c7">&nbsp;Dator (1990) cited the article, discussing the development of &ldquo;artificial life&rdquo; and AI rights as part of a broader &ldquo;review of recent work on future socioeconomic and scientific and technological developments.&rdquo; Dator and Inayatullah both continued to cite the article in a number of other publications. Sudia (2001) cited the article as part of an exploration of &ldquo;jurisprudence of artilects,&rdquo; with various relevant legal precedents and a proposed &ldquo;Blueprint for a synthetic citizen.&rdquo; McNally and Inayatullah (1988) is one of only four references (alongside Kurzweil, 1999), and Sudia also attributes one claim to &ldquo;S. Inayatullah, personal communication,&rdquo; suggesting that Inayatullah substantially influenced Sudia. Kim (2005) cited McNally and Inayatullah (1988), Inayatullah (</span><span class="c7">2001</span><span class="c7">b), Freitas (1985), as well as numerous publications about AI, artificial consciousness, rights, and computer programs in an examination of &ldquo;issues in artificial life and rights&hellip; through one of the most popular video game, </span><span class="c7 c14">The Sims</span><span class="c7">.&rdquo; Kim and Petrina (2006) and Jenkins (2006) also cited the article in discussion of the moral consideration of simulations. The article was also cited in Coeckelbergh (</span><span class="c7">2010</span><span class="c5">) and a handful of other publications specifically about robot rights since that point. Other citations were for a mixture of reasons, such as broader discussion of AI or of future studies. Indeed, a number of the citations relevant to robots rights were in journals explicitly dedicated to future studies.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref35" id="ftnt35">[35]</a><span class="c7">&nbsp;Inayatullah (</span><span class="c7">2001</span><span class="c5">a) touches on many of the same themes as McNally and Inayatullah&rsquo;s earlier (1988) article and cites similar streams of thought, albeit with a few updated specific references, such as Ray Kurzweil&rsquo;s predictions for the development of AI. Less formally, Inayatullah (2001b) again covers some similar themes, but focuses more on the criticism received by colleagues, historical trends in moral &ldquo;exclusion and inclusion,&rdquo; and &ldquo;scenarios of the future.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref36" id="ftnt36">[36]</a><span class="c7">&nbsp;Solum (1992) also cited Moravec (1988) and an early publication by Kurzweil; these two authors are discussed in the section below on &ldquo;Transhumanism, effective altruism, and </span><span class="c7">longtermism</span><span class="c7">.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref37" id="ftnt37">[37]</a><span class="c5">&nbsp;Citations began to accumulate rapidly from 1993, including articles addressing who or what should be granted legal standing, albeit not necessarily focusing specifically on artificial entities (e.g. Kester, 1993; White, 1993) and articles addressing various legal problems surrounding new technologies (e.g. Fiedler &amp; Reynolds, 1993). A number of publications cited Solum (1992) for discussions of intellectual property and liabilities relating to computers and other artificial entities (e.g. Vigderson, 1994; Clifford, 1996), some of which included explicit discussion of legal personhood (e.g. Allen &amp; Widdison, 1996; Herrick, 2002; Chopra &amp; White, 2004; Barfield, 2005; Calverley, 2008) and several of which attracted many citations themselves. A number of these articles explicitly touched on the moral aspect of the question, though citations of Solum (1992) in publications primarily focused on ethical rather than legal discussions about artificial entities seem quite rare and mostly from many years after the article was initially published (e.g. Levy, 2009; Lichocki et al., 2011). Indeed, citations of Solum (1992) have proliferated recently, with over half of the citations being from 2018-2021.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">The framing could help to explain the difference in scholarly attention. Freitas&rsquo; (1985) and McNally and Inayatullah&rsquo;s (1988) articles focused on &ldquo;rights of robots&rdquo; in the future, whereas perhaps &ldquo;legal personhood for artificial intelligences&rdquo; seemed to have more pressing implications. However, this explanation seems unlikely to have contributed much, if at all, to the difference: Lehman-Wilzig&rsquo;s (1981) choice of wording is closer to Solum (1992), and Solum&rsquo;s framing in the introduction is still very hypothetical and explicitly forward-looking. The detailed focus on legal precedent might distinguish Solum&rsquo;s article somewhat, though again, this feature is shared to some extent with Lehman-Wilzig (1981) and Willick (1983). Perhaps more plausibly influential are the effort that Solum makes in the final section to link the investigation back to fundamental and generalizable legal questions (such as developing &ldquo;a fully satisfactory theory of legal or moral personhood&rdquo;) and the inclusion of both legal personhood issues and more mundane and pressing questions of liability. By comparison, Karnow separated out these two topics into articles on personhood (Karnow, 1994) and liability (Karnow, 1996); the latter has nearly three times as many citations as the former at present (though Solum&rsquo;s article has about three times the combined total of Karnow&rsquo;s two articles)</span><span class="c7">.</span><span class="c7">&nbsp;Another potential contributing factor is simply that Solum (1992) was writing a little later than Lehman-Wilzig (1981), Willick (1983), or some of the others, though this would not explain why Solum attracted more attention than Karnow (1994). And of course it is possible that Solum&rsquo;s article was just written more engagingly (e.g. via the &ldquo;interlude&rdquo; quotes) or persuasively (e.g. via the detailed engagement with various legal and moral objections). Listed as an &ldquo;attorney at law,&rdquo; Willick (1983) was presumably the author with the most comparable legal credibility to Solum, though that article was published in </span><span class="c7 c14">AI Magazine </span><span class="c5">rather than a law journal.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref38" id="ftnt38">[38]</a><span class="c7">&nbsp;Many of the publications that cite </span><span class="c7">Karnow (1994) proceed with seemingly similar motivations</span><span class="c5">&nbsp;of concern about the adaptation of legal systems in order to protect human rights that are threatened by emerging technologies (e.g. Krogh, 1996). The same is true for numerous other publications at this time, such as Allen and Widdison&rsquo;s (1996) article that cites Solum (1992). There do not appear to be any publications citing Karnow (1994) that focus primarily on moral rather than legal issues.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref39" id="ftnt39">[39]</a><span class="c7">&nbsp;The website does not explicitly cite its intellectual influences, apart from a single reference to &ldquo;Marvin Minsky, noted AI scientist&rdquo; (ASPCR, </span><span class="c7">1999</span><span class="c7">b), so it is possible that the overlap is entirely coincidental. For example, both the ASPCR and the academics writing about legal rights for artificial entities might have been influenced to adopt this focus and terminology by science fiction. Similarly, Brooks (2000) wrote in </span><span class="c7 c14">Time </span><span class="c5">magazine of &ldquo;robots to which we will want to extend the same inalienable rights that humans enjoy.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref40" id="ftnt40">[40]</a><span class="c5">&nbsp;Minsky (1994) stopped short of giving explicit predictions about dates, but argued that AI would rapidly exceed various human capabilities. Bostrom (2005) discusses some precedent for such predictions as early as 1965.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Of course, many of the ideas associated with these authors have a history that predates 1988; see Bostrom (2005) and Miah (2009). David M. Rorvik&rsquo;s (1979; first edition 1970) </span><span class="c7 c14">As Man Becomes Machine</span><span class="c5">, which discussed the idea of cyborgs and various types of social interaction with artificial entities. The conversational style is somewhat similar to Kurzweil (1999), and the book lacks formal references. </span></p></div><div><p class="c4 c6"><a href="#ftnt_ref41" id="ftnt41">[41]</a><span class="c7">&nbsp;Moravec&rsquo;s (1988) chapter on &ldquo;Mind in Motion&rdquo; discussed various developments in intelligence and consciousness in machines. </span><span class="c7">Unlike Kurzweil&rsquo;s (1999) chapter &ldquo;Of Minds and Machines&rdquo; and subsequent commentary interspersed through that book, Moravec (1988) made little explicit comment on the ethical implications of artificial consciousness. </span><span class="c5">Asaro (2001) criticized Moravec&rsquo;s (2000; first edition 1999) later book for giving only cursory and unconvincing discussion of the moral consideration of artificial entities, noting that Moravec &ldquo;argues that we should keep the robots enslaved... yet also makes the point that robots will be just as conscious and sensitive as humans.&rdquo; Moravec (1988) appears to have been quickly referenced by numerous publications discussing artificial life and consciousness (e.g. Farmer &amp; Belin, 1990).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref42" id="ftnt42">[42]</a><span class="c5">&nbsp;Kurzweil&rsquo;s prediction for 2019 was that, &ldquo;[t]he subjective experience of computer-based intelligence is seriously discussed, although the rights of machine intelligence have not yet entered mainstream debate.&rdquo; An updated prediction for 2029 was that, &ldquo;[d]iscussion of the legal rights of machines is growing, particularly those of machines that are independent of humans (those not embedded in a human brain). Although not yet fully recognized by law, the pervasive influence of machines in all levels of decision making is providing significant protection to machines.&rdquo; By 2099, &ldquo;[t]he rights and powers of different manifestations of human and machine intelligence and their various combinations represent a primary political and philosophical issue, although the basic rights of machine-based intelligence have been settled.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref43" id="ftnt43">[43]</a><span class="c5">&nbsp;Bostrom (2003) argued that, with &ldquo;enormous amounts of computing power,&rdquo; future generations might run many conscious simulations, such that &ldquo;it could be the case that the vast majority of minds like ours do not belong to the original race but rather to people simulated by the advanced descendants of an original race.&rdquo; Bostrom (2003) briefly discussed some moral implications of this, assuming that the conscious simulations would be capable of suffering and warranting moral consideration. However, the issue of shutting down a simulation was more explicitly discussed as a brief mention in his 2002 paper, which cited the forthcoming manuscript of the 2003 article.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Although only </span><span class="c7">hinting </span><span class="c5">at the idea in his 1988 book, Moravec had explicitly discussed in an interview the idea that our current world is more likely to be a simulation than the original, biological world (Platt, 1995). Bostrom (2003) cited Moravec (1988), but not for this specific idea, and later (2008) did not mention Moravec when asked &ldquo;How did you come up with this?&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref44" id="ftnt44">[44]</a><span class="c5">&nbsp;Bostrom (2001) had written a short note about &ldquo;Ethical Principles in the Creation of Artificial Minds&rdquo; which included comments such as that &ldquo;Substrate is morally irrelevant. Whether somebody is implemented on silicon or biological tissue, if it does not affect functionality or consciousness, is of no moral significance.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref45" id="ftnt45">[45]</a><span class="c7">&nbsp;Though the transhumanist writers </span><span class="c7">often mentioned</span><span class="c5">&nbsp;sentience or consciousness as part of their commentary on why artificial entities might warrant moral consideration, they tended not to explain their motivation. This may stem from transhumanists subscribing to a broadly utilitarian ethical system where, as argued by Singer (1995, pp. 7-8), following Jeremy Bentham, &ldquo;[t]he capacity for suffering and enjoyment is, however, not only necessary, but also sufficient for us to say that a being has interests.&rdquo; For example, Bostrom (2005) noted that, &ldquo;[d]espite some surface-level similarities with the Nietzschean vision, transhumanism &ndash; with its Enlightenment roots, its emphasis on individual liberties, and its humanistic concern for the welfare of all humans (and other sentient beings) &ndash; probably has as much or more in common with Nietzsche&rsquo;s contemporary J.S. Mill, the English liberal thinker and utilitarian.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref46" id="ftnt46">[46]</a><span class="c5">&nbsp;Kurzweil (1999) briefly mentions &ldquo;Animal rights,&rdquo; but no citations are provided. There does not appear to be any citation of work in environmental ethics, either.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref47" id="ftnt47">[47]</a><span class="c5">&nbsp;Bostrom (2003) also credited him in the acknowledgements.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref48" id="ftnt48">[48]</a><span class="c7">&nbsp;Of course, it is possible that similar discussions might have arisen without the contributions by the authors associated with transhumanism and effective altruism. For example, some researchers had discussed the moral consideration of simulations previously to (e.g. Elton, 2000) or seemingly independently of (e.g. Kim, 2004) Bostrom&rsquo;s work, although other contributors seem to have been partly inspired by Bostrom (e.g. </span><span class="c7">Jenkins</span><span class="c5">, 2006).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref49" id="ftnt49">[49]</a><span class="c5">&nbsp;Hall (2000) appears to have been an influence on David Gunkel (see the section on &ldquo;Social-relational ethics&rdquo;), and may also be the origin of the term &ldquo;Machine Ethics&rdquo; (Gunkel, 2012, pp. 102-3). However, Hall had numerous influences beyond the Transhumanist writers (see footnote 69).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">As another example of an item that was directly influenced by these contributions, see Walker (2006), who cited publications by Moravec, Searle, and Turing. Whitby (1996) cited Moravec, LaChat, Singer, and a few others. Barfield (2005) cited each of Moravec, Kurzweil, and Bostrom for claims about the potential trajectory of AI developments as context for discussion about a number of legal issues, including legal personhood, though also cited a wide range of other influences, including Solum (1992).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref50" id="ftnt50">[50]</a><span class="c5">&nbsp;A Google Scholar search for &ldquo;(&quot;Mindcrime&quot; OR &quot;mind crime&quot; OR &quot;mind-crime&quot;) AND Bostrom&rdquo; identified 33 items, of which at least half appeared to be from writers associated with the effective altruism community (Google Scholar, 2021a). See also Table 7.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref51" id="ftnt51">[51]</a><span class="c7">&nbsp;MacAskill (2019) has defined effective </span><span class="c7">altruism </span><span class="c5">as the research field and social movement using &ldquo;evidence and careful reasoning to work out how to maximize the good with a given unit of resources&rdquo; and using the findings &ldquo;to try to improve the world.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref52" id="ftnt52">[52]</a><span class="c7">&nbsp;MacAskill (2022) has defined </span><span class="c7">longtermism</span><span class="c5">&nbsp;as &ldquo;the view that positively influencing the longterm future is a key moral priority of our time.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref53" id="ftnt53">[53]</a><span class="c5">&nbsp;Moravec&rsquo;s views on the topic seem to have been more ambivalent; see Asaro (2001) for discussion.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref54" id="ftnt54">[54]</a><span class="c5">&nbsp;Six of the seven references in Tomasik (2011) were from individuals associated with the transhumanism and effective altruism communities, as were both named individuals in the acknowledgements. Tomasik&rsquo;s (2014) article includes a far wider array of references, though it is unclear whether or not the cited writers influenced Tomasik&rsquo;s initial thinking on the topic.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Tomasik (2013) noted that he &ldquo;coined the phrase &lsquo;suffering subroutines&rsquo; in a 2011 post on Felicifia. I chose the alliteration because it went nicely with &lsquo;sentient simulations,&rsquo; giving a convenient abbreviation (SSSS) to the conjunction of the two concepts&hellip; It appears that Meghan Winsby (coincidentally?) used the same &lsquo;suffering subroutines&rsquo; phrase in an excellent 2013 paper: &ldquo;Suffering Subroutines: On the Humanity of Making a Computer that Feels Pain.&rdquo; It seems that her usage may refer to what I call sentient simulations, or it may refer to general artificial suffering of either type.&rdquo; A Google Scholar search for &ldquo;&quot;suffering subroutines&quot;&rdquo; identified 20 items, of which at least half appeared to be from Tomasik or other writers associated with the Center on Long-Term Risk (Google Scholar, 2021d).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref55" id="ftnt55">[55]</a><span class="c7">&nbsp;Also in 1998, Floridi had presented some of the same ideas at a Computer Ethics: Philosophical Enquiry conference (Floridi, 1998a), though this conference presentation gained far fewer citations than Floridi (</span><span class="c7">1999</span><span class="c7">) </span><span class="c7">(Google Scholar, 2021b)</span><span class="c5">.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref56" id="ftnt56">[56]</a><span class="c7">&nbsp;</span><span class="c7">Floridi (</span><span class="c7">1999</span><span class="c7">, p. 50) </span><span class="c7">clarifie</span><span class="c5">d and explicitly noted that some types of AI could warrant high moral consideration: &ldquo;All entities have a moral value&hellip; from the point of view of the infosphere and its potential improvement, responsible agents (human beings, full-AI robots, angels, gods, God) have greater dignity and are the most valuable information entities deserving the highest degree of respect.&rdquo; Floridi (1999, p. 54) encouraged the reader to &ldquo;[i]magine a boy playing in a dumping-ground&hellip; The boy entertains himself by breaking [abandoned car] windscreens and lights, skilfully throwing stones at them.&rdquo; With information ethics, &ldquo;we know immediately why the boy&rsquo;s behaviour is a case of blameworthy vandalism: he is not respecting the objects for what they are, and his game is only increasing the level of entropy in the dumping-ground, pointlessly. It is his lack of care, the absence of consideration of the objects&rsquo; sake, that we find morally blameable. He ought to stop destroying bits of the infosphere and show more respect for what is naturally different from himself and yet similar, as an information entity, to himself.&rdquo; Floridi&rsquo;s example would presumably hold if the boy had instead been inflicting harm on robots or AIs. In another example on pages 54-5, Floridi (1999) imagines that &ldquo;one day we genetically engineer and clone non-sentient cows,&rdquo; which could be seen as a type of artificial entity, and objects to the idea of &ldquo;carving into&rdquo; their flesh.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">In a subsequent article, Floridi (2002) sought to &ldquo;clarify and support&rdquo; the &ldquo;second thesis&rdquo; of information ethics, &ldquo;that information objects </span><span class="c7 c14">qua </span><span class="c5">information objects can have an intrinsic moral value, although possibly quite minimal, and hence that they can be moral patients, subject to some equally minimal degree of moral respect.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Floridi had previously published about the internet and information, but mostly did not argue in these articles that information possesses intrinsic value; Floridi expressed concern about &ldquo;an unrestrained, and sometimes superfluous, profusion of data&rdquo; (</span><span class="c7">Floridi, 1996</span><span class="c7">b) and the spread of misinformation (</span><span class="c7">Floridi, 1996</span><span class="c7">a). Floridi (</span><span class="c7">1996</span><span class="c5">b) did comment briefly that destroying paper records is &ldquo;unacceptable, as would have been the practice of destroying medieval manuscripts after an editio princeps was printed during the Renaissance. We need to preserve the sources of information after the digitalization in order to keep all our memory alive&hellip; The development of a digital encyclopedia should not represent a parricide.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref57" id="ftnt57">[57]</a><span class="c7">&nbsp;Floridi (</span><span class="c7">1999</span><span class="c5">, p. 41) noted that &ldquo;Medical Ethics, Bioethics and Environmental Ethics&hellip; attempt to develop a patient-oriented ethics in which the &lsquo;patient&rsquo; may be not only a human being, but also any form of life. Indeed, Land Ethics extends the concept of patient to any component of the environment, thus coming close to the object-oriented approach defended by Information Ethics.&rdquo; Floridi (2013) repeatedly referred to Information Ethics as &ldquo;e-nvironmental ethics or synthetic environmentalism.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref58" id="ftnt58">[58]</a><span class="c7">&nbsp;Floridi (</span><span class="c7">1999</span><span class="c5">, p. 42) noted that &ldquo;Bioethics and Environmental Ethics fail to achieve a level of complete universality and impartiality, because they are still biased against what is inanimate, life-less or merely possible (even Land Ethics is biased against technology and artefacts, for example). From their perspective, only what is alive deserves to be considered as a proper centre of moral claims, no matter how minimal, so a whole universe escapes their attention. Now this is precisely the fundamental limit overcome by CE, which further lowers the condition that needs to be satisfied, in order to qualify as a centre of a moral concern, to the minimal common factor shared by any entity, namely its informationstate.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref59" id="ftnt59">[59]</a><span class="c5">&nbsp;&ldquo;If one tries to pinpoint exactly what common feature so many case-based studies in CE share, it seems reasonable to conclude that this is an overriding interest in the fate and welfare of the action-receiver, the information.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref60" id="ftnt60">[60]</a><span class="c5">&nbsp;When exploring how &ldquo;artificial agents&rdquo; can &ldquo;not only&hellip; perpetrate evil&hellip; but conversely&hellip; &lsquo;receive&rsquo; or &lsquo;suffer&rsquo; from it,&rdquo; Floridi and mathmetician Jeffrey W. Sanders (2001) drew on a mixture of previous explorations of the concept of evil, their own previous writings on information ethics and entropy, environmental ethics (specifically deep ecology), and CE. They cited several articles that had focused on moral questions about animals, but to explore the idea of &ldquo;Artificial Agents&rdquo; rather than &ldquo;Artificial Patients.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Floridi and Sanders (2002) restated that IE is &ldquo;patient-oriented&rdquo; and cited &ldquo;Medical Ethics, Bioethics and Environmental Ethics&rdquo; as being &ldquo;among the best known examples of this non-standard approach.&rdquo; Almost all of the references in the paper were previous contributions to CE and information ethics. Floridi (2002) added a new dimension by drawing firstly on the framework provided by previous work in </span><span class="c7">&ldquo;Object Oriented Programming (OOP)&rdquo;</span><span class="c5">&nbsp;(a specific computer programming methodology, which Floridi, 1998 had also drawn on) in order to &ldquo;make precise the concept of &lsquo;information object&rsquo; as an entity constituted by a bundle of properties.&rdquo; Otherwise, the article mostly drew upon, analyzed, and extended previous contributions in information ethics, CE, environmental ethics, and Kant&rsquo;s writings.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Floridi (2006) drew on some of the references and ideas explored in Floridi and Sanders (2001), Floridi and Sanders (2002), and Floridi (2002), added in some additional references to other theorists such as Rawls, and addressed &ldquo;some standard objections to Information Ethics&hellip; that seem to be based on a few basic misunderstandings,&rdquo; </span><span class="c7">e.g. Himma (2004)</span><span class="c5">. Otherwise, however, the basic ideas were similar and most of the references were to previous writings on environmental ethics, CE, or IE.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Floridi&rsquo;s writings drew little on science fiction. Floridi and Sanders (2001) briefly cited </span><span class="c7 c14">The Matrix</span><span class="c7">&nbsp;as an example of how &ldquo;[s]ci-tech&hellip; creates a new form of evil, AE [artificial evil]&rdquo; and commented that &ldquo;something similar to Asimov&rsquo;s Laws of Robotics will need to be enforced for the digital environment (the infosphere) to be kept safe.&rdquo; However, science fiction was absent from Floridi&rsquo;s other early works (e.g. 1998; </span><span class="c7">1999</span><span class="c5">; 2002). Floridi (2002) referred to &ldquo;Putnam&rsquo;s twin earth mental experiment,&rdquo; but Floridi&rsquo;s writings usually referred little to work on artificial life and consciousness.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">More recently, Floridi (2013) has credited a broader range of philosophical influences. The preface (p. xv) also contains a brief joking reference to </span><span class="c7 c14">Battlestar Galactica</span><span class="c5">&nbsp;aimed at &ldquo;science-fiction fans,&rdquo; which suggests that he may share this self-identification.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref61" id="ftnt61">[61]</a><span class="c7">&nbsp;</span><span class="c7">Tavani (2002) summarizes several proponents of the &ldquo;computer ethics is unique&rdquo; thesis who, like Floridi, &ldquo;claim that a new system of ethics is needed to handle the kinds of moral concerns raised by ICT&rdquo; and that ICT introduces &ldquo;new objects of moral consideration.&rdquo;</span><span class="c5">&nbsp;Tavani&rsquo;s (2002) own view is that &ldquo;there is no compelling evidence to support the claim that computer ethics is unique in the sense that it: (a) introduces new ethical issues or new ethical objects, or (b) requires a new ethical theory or a whole new ethical framework.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref62" id="ftnt62">[62]</a><span class="c7">&nbsp;Floridi (2013, pp. xvii-xix) explicitly notes that, &ldquo;[a]ll the chapters were planned as conference papers or (sometimes inclusive or) journal articles&rdquo; and provides a list of the earlier publications. Floridi (2010b) has accrued 198 citations to date compared to 612 for Floridi (2013) </span><span class="c7">(Google Scholar, 2021b)</span><span class="c5">.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref63" id="ftnt63">[63]</a><span class="c5">&nbsp;If Floridi&rsquo;s (2013) more recent book is excluded, the average of the other four is only eight citations per year.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref64" id="ftnt64">[64]</a><span class="c7">&nbsp;For example, </span><span class="c7">Siponen (2000</span><span class="c7">) applied Floridi&rsquo;s information ethics to an ethical issue in computer security and interpreted Floridi (</span><span class="c7">1999</span><span class="c7">) as suggesting that &ldquo;anti-virus activity may be wrong&rdquo; because it grants insufficient respect to the virus as an information entity. However, another early reference (Rogerson, 2001) just cited Floridi (1999) for the brief comment that, &ldquo;[t]here has been remarkably little consideration of moral obligations with respect to the dead&rdquo; and a third (Tavani, 2001) cited Floridi (</span><span class="c7">1999</span><span class="c7">) in discussion about &ldquo;the proper computer ethics methodology.&rdquo; Others cited Floridi (</span><span class="c7">1999</span><span class="c5">) mainly for its explanations of certain concepts, such as the &ldquo;infosphere&rdquo; (Gandon, 2003) or Kantian ethics (Treiblmaier et al., 2004). Many cited various articles by Floridi for discussion of artificial moral agents, somewhat independently from their possible moral patiency (e.g. Sullins, 2009).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">York (2005) cited Floridi when advocating a &ldquo;universal ethics&rdquo; that &ldquo;regards all concrete material entities, whether living or not, and whether natural or artefactual, as inherently valuable, and therefore as entitled to the respect of moral agents.&rdquo; Capurro (2006) explicitly engaged with the implications of Floridi&rsquo;s ideas for &ldquo;the moral status of digital agents,&rdquo; though the focus was more on agency than patiency. Brey (2008) critiqued Floridi, arguing that, &ldquo;Floridi has presented no convincing arguments that everything that exists has some minimal amount of intrinsic value.&rdquo; Brey (2008) agreed with &ldquo;the necessity of expanding the class of moral patients beyond human beings&rdquo; but objected to Floridi&rsquo;s wider claims for various reasons, such as that, &ldquo;for an object to possess intrinsic value it must possess one or more properties that bestow intrinsic value upon it, such as the property of being rational, being capable of suffering, or being an information object.&rdquo; Similarly, Doyle (2010) argued that &ldquo;Floridi fails to show that the moral community should be expanded beyond beings capable of suffering or having preferences&rdquo; and defends consequentialism. Volkman (2010) examined Floridi and Sanders&rsquo; arguments about moral patiency of any and all information entities from the perspective of virtue ethics. Gunkel (2012) quoted Floridi and Sanders (2004) at length in distinguishing between agency and patiency; the focus of the book is then to examine these two concepts in the context of &ldquo;machines,&rdquo; including quite substantial discussion of Floridi&rsquo;s views.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">I have not read all of the items citing Floridi (</span><span class="c7">1999</span><span class="c5">) or Floridi&rsquo;s subsequent papers; this impression is based on scanning titles and checking up references that seemed potentially relevant to the moral consideration of artificial entities. However, this impression seems to have been shared by Siponen at the time: after noting that Floridi&rsquo;s work addresses &ldquo;how we should treat entities deserving moral respect,&rdquo; Siponen (2004) added that, &ldquo;[u]nfortunately, for whatever reasons, Floridi&rsquo;s work has not attracted much interest, which is odd, given the promising nature of this work. Even though I have reservations about Floridi&rsquo;s theory, I believe it deserves to be discussed and better known.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref65" id="ftnt65">[65]</a><span class="c7">&nbsp;Floridi has sought to engage public interest in his work, appearing on numerous podcasts, writing a book for the public-facing </span><span class="c7 c14">Very Short Introduction</span><span class="c5">&nbsp;series (Floridi, 2010a), and giving a TEDx talk (Floridi, 2011). However, the content of these efforts has tended to focus on Floridi&rsquo;s other interests within the &ldquo;philosophy of information,&rdquo; rather than on his ideas about information ethics and the moral patiency of all informational entities.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Viewership of Floridi&rsquo;s (2011) TEDx talk was 30,852 at the time of checking (November 10th, 2021), which is less than 2% of the TED talk average of about 1,698,297 (Crippa, 2017). However, Floridi&rsquo;s </span><span class="c7 c14">Very Short Introduction </span><span class="c7">(2010a) has 1,186 citations </span><span class="c7">(Google Scholar, 2021b)</span><span class="c7">,</span><span class="c7">&nbsp;the highest of any of the books in the series published that year and well above the average of 147.</span><span class="c5">&nbsp;Floridi (2013, p. x) notes that he is &ldquo;painfully aware that this [book] is not a page-turner, to put it mildly, despite my attempts to make it as interesting and reader-friendly as possible.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref66" id="ftnt66">[66]</a><span class="c7">&nbsp;Floridi </span><span class="c7">(2017a)</span><span class="c7">&nbsp;made a near identical point and </span><span class="c7">Floridi and Taddeo (2018) made a similar point very briefly.</span><span class="c5">&nbsp;It is possible that Floridi simply disagreed that legal personhood was the best way to protect the interests of artificial informational entities; possible that he did not think about the potentially important long-run effects of setting precedent for protecting such entities; possible that had changed his views on the moral consideration that they warrant; or possible that, all along, the &ldquo;intrinsic moral value&rdquo; he attributed to them always was really &ldquo;quite minimal&rdquo; (Floridi, 2002).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref67" id="ftnt67">[67]</a><span class="c5">&nbsp;They make only passing reference to &ldquo;a few people&rdquo; having been &ldquo;interested in how human beings ought to treat machines.&rdquo; Gunkel (2018, p.. 38-9) notes that Anderson and Anderson&rsquo;s subsequent writings explicitly exclude &ldquo;how human beings ought to treat machines&rdquo; from machine ethics.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref68" id="ftnt68">[68]</a><span class="c7">&nbsp;Gunkel (2012, p. 103) adds that Hall&rsquo;s &ldquo;exclusive focus on machine moral agency persists in Hall&rsquo;s subsequent book-length analysis, </span><span class="c7 c14">Beyond AI: Creating the Conscience of the Machine</span><span class="c5">&nbsp;(2007). Although the term &lsquo;artificial moral agency&rsquo; occurs throughout the text, almost nothing is written about the possibility of &lsquo;artificial moral patiency,&rsquo; which is a term Hall does not consider or utilize.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref69" id="ftnt69">[69]</a><span class="c5">&nbsp;Hall (2000) drew on previous philosophical discussions. There are no references to foregoing detailed discussion of how humans ought to treat machines, although Kurzweil (1999) and Moravec (2000) are both cited, as are Minsky and Dennett who had written about the capacities of AI long previously. Hall also cited Robert Freitas, though not for his (1985) article about robot rights.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref70" id="ftnt70">[70]</a><span class="c7">&nbsp;In what is mostly an extension of Veruggio (2006), Veruggio and Operto (2006) emphasize in their article in the </span><span class="c7 c14">International Review of Information Ethics</span><span class="c5">&nbsp;that &ldquo;[r]oboethics shares many &lsquo;sensitive areas&rsquo; with Computer Ethics, Information Ethics and Bioethics.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref71" id="ftnt71">[71]</a><span class="c5">&nbsp;Torrance had written on this topic previously (e.g. briefly in 1984 and 2000) and continued to write on it subsequently (e.g. Torrance et al., 2006; Torrance, 2008). Calverley presented similar work at another conference in the same year (2005), and went on to publish additional relevant research (e.g. 2006; 2008). Torrance&rsquo;s work seems to have primarily stemmed out of artificial consciousness research but sometimes cites work by Floridi or, more regularly, Calverley; Calverley draws heavily on previous writings on both artificial consciousness and legal rights for artificial entities.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref72" id="ftnt72">[72]</a><span class="c5">&nbsp;Friedman et al.&rsquo;s (2003) Table 1 noted that 7% of participants&rsquo; responses suggested that AIBO &ldquo;Engenders moral regard,&rdquo; 4% that it is a &ldquo;Recipient of moral care,&rdquo; and 3% that it has or should have &ldquo;Rights.&rdquo; Although less relevant, 3% suggested that AIBO &ldquo;Deserves Respect,&rdquo; 1% that it is &ldquo;Morally Responsible,&rdquo; and 1% that it is &ldquo;Morally Blameworthy.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref73" id="ftnt73">[73]</a><span class="c5">&nbsp;Of course, many other factors could have influenced the authors to be open to including this category in their analysis. For example, Friedman et al. (2003) cited a number of publications about human interactions with animals, some of which may contain some ethical discussion. Friedman had also previously published a number of items that addressed ethical issues in computing.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref74" id="ftnt74">[74]</a><span class="c7">&nbsp;One seemingly relevant publication, Nomura et al. (2006), developed the &ldquo;Negative Attitude toward Robots Scale (NARS),&rdquo; though their motivation was to investigate &ldquo;how humans are mentally affected&rdquo; by robots, such as developing anxiety towards robots. </span><span class="c7">None of the included items in the scale were about moral attitudes towards robots.</span><span class="c5">&nbsp;The references were to other papers on anxiety and HRI but not to moral consideration. MacDorman and Cowley (2006) presented a paper about the potential criteria for robot personhood &mdash; including consciousness, appearance, and &ldquo;the ability to sustain long-term relationships&rdquo; &mdash; at a conference about &ldquo;robot and human interactive communication.&rdquo; Their references included Kahn et al. (2006), which in turn drew heavily on Friedman et al. (2003) for its discussion of moral standing. Scheutz and Crowell (2007) addressed a number of &ldquo;Social and Ethical Implications of Autonomous Robots,&rdquo; though the discussion of the possibility of robot rights is very brief and described as not &ldquo;of pressing urgency, since such questions may only be relevant for robots much more advanced than those available at present.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">It is possible that, like Friedman et al.&rsquo;s (2003) own article, publications would have titles implying a focus on social interaction but include some discussion of moral consideration; in such cases, I would likely have missed relevant discussion.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref75" id="ftnt75">[75]</a><span class="c5">&nbsp;Freier (2008) cited numerous publications by Friedman and Kahn (including Friedman et al., 2003) and &ldquo;thanks Batya Friedman and Peter H. Kahn, Jr., for their guidance in developing and conducting this work.&rdquo; Otherwise, none of the publications referenced by Freier (2008) seem to focus explicitly on moral (as opposed to social) consideration of artificial entities. The paper is presented in the context of people having &ldquo;frequent interactions&rdquo; with artificial entities that are &ldquo;routinely designed to mimic not only animate but also social and even moral entities in the world&rdquo; and is motivated by &ldquo;a general concern with the role that human values play in the design of technology.&rdquo; Freier builds on literature about social interaction with &ldquo;personified technology&rdquo; and literature about the moral development of children.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref76" id="ftnt76">[76]</a><span class="c5">&nbsp;None of the papers at either conference cited any empirical research by Friedman, Kahn,or Hagman.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref77" id="ftnt77">[77]</a><span class="c5">&nbsp;For example, the computer is described as &ldquo;a medium for emotion&rdquo; and concern is expressed that &ldquo;negative behaviors that are directed not only towards the machine but also towards other people.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref78" id="ftnt78">[78]</a><span class="c5">&nbsp;For example, Zancanaro and Leonardi (2005) conducted a qualitative study to provide &ldquo;initial insights on how groups can reduce the cognitive effort of using a co-located interface.&rdquo; Brahnam (2005) addressed customer abuse of &ldquo;embodied conversational agents&rdquo; (ECAs) but noted that &ldquo;ECAs are not people and thus not capable of being harmed&rdquo; and listed various human-focused reasons for concern with the abuse, such as degrading &ldquo;the business value of using ECAs.&rdquo; Other papers addressed topics such as user frustrations, cyberbullying, cybersex, the moral development of children, and &ldquo;rudeness in email.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref79" id="ftnt79">[79]</a><span class="c5">&nbsp;For example, De Angeli and Carpenter (2005) stated that their paper was &ldquo;a preliminary attempt&rdquo; to address the lack of research on &ldquo;negative outcomes&rdquo; of HRI, including &ldquo;moral and ethical issues.&rdquo; They highlighted &ldquo;an urgent need to explore the requirements for the establishment and negotiation of a cyber-etiquette to regulate the interaction between humans and artificial entities&rdquo; and asked whether &ldquo;respect for &lsquo;machines&rsquo; [will] grow along with their abilities, or will the abuse spiral upward thanks to a perception of a developing risk of inter-&lsquo;species&rsquo; conflict?&rdquo; The paper&rsquo;s references are to publications about user experiences or human-computer interaction. De Angeli&rsquo;s (2006) paper in the second conference explicitly noted that, ordinarily, the concept of &ldquo;verbal abuse&hellip; should not apply to unanimated objects, as they cannot suffer any pain,&rdquo; and that machines &ldquo;cannot feel any pain&hellip; they are inferior, unanimated objects.&rdquo; These comments suggest that De Angeli and Carpenter&rsquo;s (2005) paper was not likely motivated by concern about the artificial entities themselves that are abused.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Explicitly following up on De Angeli and Carpenter (2005), Brahnam (2006) explored &ldquo;the effect gendered embodiment has on user verbal abuse.&rdquo; The motivations are not stated, but given Brahnam&rsquo;s (2005) comment that &ldquo;ECAs are not people and thus not capable of being harmed,&rdquo; it seems likely that Brahnam&rsquo;s (2006) focus was on shedding light on human gender issues. Krenn and Gstrein (2006) studied &ldquo;an online dating community where users are represented by avatars&rdquo; and found evidence that &ldquo;in peer-to-peer contexts abusive behaviour is rare.&rdquo; They noted that they were inspired by De Angeli and Carpenter (2005), &ldquo;where verbal abuse of a chatterbot by human users is explained by an asymmetrical power distribution between the human user and the dumb computer generated conversational system,&rdquo; but otherwise do not clarify their motivations for the study. Horstmann et al.&rsquo;s (2018) paper examining hesitation when &ldquo;switching off a robot which exhibits lifelike behavior,&rdquo; cited De Angeli and Carpenter (2005), though relatively few of the other articles citing this paper seem to have focused explicitly on the moral consideration of artificial entities.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Another ambiguous contribution comes from Ruzich (2006), who explores how and why, when computers crash, &ldquo;those who stare in horror at blank screens and error messages frequently frame their experiences as if they represent compressed experiences with the stages of grief as identified by Elisabeth Kubler-&shy;Ross: the initial denial of loss, bargaining, rising anger, depression, and acceptance of the loss.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref80" id="ftnt80">[80]</a><span class="c7">&nbsp;Most of the papers citing Bartneck et al. (</span><span class="c7">2005</span><span class="c5">b) seem to focus on human-robot interaction rather than moral consideration per se, although some do touch on this topic. One of the earliest was Misselhorn&rsquo;s (2009) paper on &ldquo;empathy with inanimate objects.&rdquo; Next was Rosenthal-von der P&uuml;tten et al.&rsquo;s (2013) &ldquo;experimental study on emotional reactions towards a robot.&rdquo; The rest of the decade saw numerous others, such as Horstmann et al.&rsquo;s (2018) paper examining hesitation when &ldquo;switching off a robot which exhibits lifelike behavior.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">At the following conference, Bartneck (2006) described the motivation and method of Bartneck et al.&rsquo;s forthcoming (2007) experiment. The motivation seems similar to Bartneck et al.&rsquo;s (2005b) paper, though Bartneck (2006) explicitly notes that &ldquo;[i]t is unclear if [robots] might remain &lsquo;property&rsquo; or may receive the status of sentient beings.&rdquo;</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Br&scaron;&#269;i&#263; et al. (2015) cite Bartneck et al. (</span><span class="c7">2005</span><span class="c7">b) as </span><span class="c7">having &ldquo;first used the term &lsquo;robot abuse,&rsquo;&rdquo;</span><span class="c5">&nbsp;which matches my own impression, at least among HRI research.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref81" id="ftnt81">[81]</a><span class="c7">&nbsp;Bartneck (</span><span class="c7">2004</span><span class="c7">) and Bartneck et al. (2007) make passing reference to Lehman-Wilzig and Bartneck et al. (2005b) </span><span class="c7">note that &ldquo;this discussion eventually leads to legal considerations of the status of robots in our society,&rdquo; citing Calverley (</span><span class="c7">2005</span><span class="c7">b</span><span class="c7">) as a study having addressed such considerations.</span><span class="c5">&nbsp;The same paper is cited in Bartneck et al. (2007).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref82" id="ftnt82">[82]</a><span class="c5">&nbsp;Bartneck (2003) began by noting that, &ldquo;[m]any companies, universities and research institutes are working on the home of the future&hellip; A key component of ambient intelligence is the natural interaction between the home and the user.&rdquo; The study measured users&rsquo; enjoyment of the interaction; the tendency to cooperate with the character was also measured, though there was no explicit discussion of the implications for the moral consideration of artificial entities. Similarly, Bartneck et al. (2004) noted that &ldquo;[t]he ability to communicate emotions is essential for a natural interaction between characters and humans&rdquo; and did not express any concern for the interests of artificial entities themselves.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Although they framed the experiment as testing the idea that &ldquo;humans treat computers as social actors,&rdquo; Bartneck et al. </span><span class="c7">(2005b)</span><span class="c5">&nbsp;seem to consider this to imply a moral dimension too, noting that their study explores the borderline of when &ldquo;we treat [robots] again like machines that can be switched off, sold or torn apart without a bad consciousness.&rdquo; They also present as context the idea that robots are increasingly ubiquitous and cite some previous research suggesting that humans treat computers as social actors.</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Bartneck et al. (</span><span class="c7">2005</span><span class="c5">a) began by noting the proliferation of robots, then commenting that, &ldquo;[w]ith an increasing number of robots, robot anxiety might become as important as computer anxiety is today.&rdquo; No mention is made of concern for negative treatment of the robots themselves.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref83" id="ftnt83">[83]</a><span class="c7">&nbsp;Bartneck et al. (2007) measured whether a robot&rsquo;s intelligence and agreeableness influenced &ldquo;hesitation to switch it off,&rdquo; with the paper&rsquo;s introduction explicitly mentioning the idea that this might constitute murder. Bartneck and Hu (2008) expanded on the Bartneck et al. (</span><span class="c7">2005</span><span class="c7">b) paper with a follow-up study; this was published in a &ldquo;Special Section on Misuse And Abuse Of Interactive Technologies&rdquo; in the journal </span><span class="c7 c14">Interaction Studies</span><span class="c5">, which followed up on the &ldquo;agent abuse&rdquo; workshops (Bartneck et al., 2008).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref84" id="ftnt84">[84]</a><span class="c5">&nbsp;Bartneck explained that &ldquo;[t]hat was the basic structure of the experiment, where people would see either a human or a robot being abused and then we would ask them then, well, what is the ethical aspect, how do you feel about this? And it turned out that people did not distinguish a human or a robot, so the abusive behavior to either of them was equally dismissive. Which is interesting because&hellip; it doesn&rsquo;t matter if the robot has no emotions, it has no pride, it doesn&rsquo;t even have tiny understanding of abuse, it doesn&rsquo;t know, it doesn&rsquo;t care; you can rip out its arm, it wouldn&rsquo;t care, so it makes absolutely no sense to feel sorry for it&hellip; A robot is a representation&hellip; of humans&hellip; and if we act towards it, we act towards a representation, and if we act poorly, that reflects also poorly on us, so from a virtue ethics point of view, we&rsquo;re not doing so great if we do this, we&rsquo;d actually be a much better human if we treat other humans and other representations of humans well and so I think we should be gentle to robots.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref85" id="ftnt85">[85]</a><span class="c5">&nbsp;The stated goals, however, were somewhat different, with Slater et al. (2006) noting their aim as being &ldquo;to investigate how people would respond to such a dilemma within a virtual environment, the broader aim being to assess whether such powerful social-psychological studies could be usefully carried out within virtual environments.&rdquo; They concluded that &ldquo;in spite of the fact that all participants knew for sure that neither the stranger nor the shocks were real, the participants who saw and heard her tended to respond to the situation at the subjective, behavioural and physiological levels as if it were real.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref86" id="ftnt86">[86]</a><span class="c5">&nbsp;Slater et al. (2006) has been widely cited (Google Scholar, 2021c). Many of the most prominent citations seem to focus on various aspects of human interaction, whether in virtual environments or not.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref87" id="ftnt87">[87]</a><span class="c5">&nbsp;Gunkel (2006) took established philosophical lines of thinking, such as by Ren&eacute; Descartes and Immanuel Kant, and applies them to machines. This sometimes draws in particular on discussion of the comparable &ldquo;animal question.&rdquo; Gunkel noted increasing social capabilities and indistinguishability from humans, drawing on Turing, science fictions, and a number of writings.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref88" id="ftnt88">[88]</a><span class="c7">&nbsp;Of course, this was not the only influence. For example, Gunkel (</span><span class="c7">2016</span><span class="c5">) noted that he has been interested in the philosophy of technology since he was in high school. </span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c7">Gunkel (</span><span class="c7">2018</span><span class="c7">) was apparently motivated by &ldquo;some very real and pressing challenges concerning emerging technology and the current state of and future possibilities for moral reasoning&rdquo; (p. xi). The preface highlighted the ongoing &ldquo;robot invasion&rdquo; &mdash; the increasing ubiquity of robots and smart devices and their increasing social interaction with humans (pp. ix-x). </span></p></div><div><p class="c4 c6"><a href="#ftnt_ref89" id="ftnt89">[89]</a><span class="c5">&nbsp;Many of the earlier writers on legal rights for artificial entities situated their writings in the context of increasing capacities of artificial entities. They sometimes argued explicitly that it was these capacities, such as the ability to suffer, that might lead the entities to merit moral consideration (e.g. McNally &amp; Inayatullah, 1988), though others had focused more on updating the legal systems regulating human interaction in the light of new technologies (e.g. Karnow, 1994).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">The transhumanist writers likewise seemed concerned by suffering or death of sentient artificial beings for their own sake (e.g. Bostrom, 2002; Bostrom, 2005; Hughes, 2005). Regarding information ethics, Floridi (2002) had explicitly defended the view that, &ldquo;[t]he moral value of an entity is based on its ontology. What the entity is determines the degree of moral value it enjoys, if any, whether and how it deserves to be respected and hence what kind of moral claims it can have on the agent.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref90" id="ftnt90">[90]</a><span class="c7">&nbsp;Gunkel (2012) cited Coeckelbergh only on a single page within the section on &ldquo;moral agency&rdquo; (p. 87). Although Coeckelbergh&rsquo;s (2009; 2010) early papers on the topic had not initially referenced Gunkel, Coeckelbergh (2013) reviewed </span><span class="c7 c14">The Machine Question</span><span class="c7">&nbsp;and Gunkel reciprocated by reviewing (2013) Coeckelbergh&rsquo;s </span><span class="c7 c14">Growing Moral Relations: Critique of Moral Status Ascription </span><span class="c7">(2012), which he praised as &ldquo;a significant paradigm shift in moral thinking&hellip; a real game changer,&rdquo; highlighting its &ldquo;relational, phenomenological, and transcendental&rdquo; approach. In contrast to his earlier writings, Gunkel mentioned Coeckelbergh&rsquo;s name 40 times in </span><span class="c7 c14">Robot Rights </span><span class="c5">(2018), including in the acknowledgements (p. xiv) as a &ldquo;brilliant &lsquo;sounding board&rsquo; for bouncing around ideas, and it was due to these interactions in Vienna [at a conference the pair attended] that it first became clear to me that this book needed to be the next writing project.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref91" id="ftnt91">[91]</a><span class="c5">&nbsp;Coeckelbergh (2009) argued for &ldquo;an approach to ethics of personal robots that advocates a methodological turn from robots to humans, from mind to interaction, from intelligent thinking to social-emotional being, from reality to appearance, from right to good, from external criteria to good internal to practice, and from theory to experience and imagination.&rdquo; Coeckelbergh&rsquo;s earlier papers (e.g. 2007) had addressed ethical issues relating to artificial entities, but focused less on moral consideration of those entities. Coeckelbergh has expanded on or revisited the topic a few times subsequently (2012; 2014) and published many times on partly overlapping topics (e.g. Coeckelbergh 2011; Stahl &amp; Coeckelbergh, 2016).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref92" id="ftnt92">[92]</a><span class="c7">&nbsp;For example, Coeckelbergh (2009) prominently cited Floridi &amp; Sanders (2004). Coeckelbergh (2009; 2010) also cited books by Moravec and Kurzweil, some HRI research, and numerous contributions explicitly discussing the moral consideration of artificial entities (e.g. McNally &amp; Inayatullah, 1988; Brooks, 2000; Asaro, 2006; Calverley, 2006; </span><span class="c7">Torrance, 2008</span><span class="c7">; </span><span class="c7">Whitby, 2008; Levy, 2009)</span><span class="c5">.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref93" id="ftnt93">[93]</a><span class="c7">&nbsp;S&oslash;raker (</span><span class="c7">2006</span><span class="c5">b) did the same in a publication from the same year that focused on computer ethics. That article also cited a publication co-authored by Lawrence Solum, so S&oslash;raker may have been aware of the arguments about legal rights for artificial entities in Solum (1992).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref94" id="ftnt94">[94]</a><span class="c5">&nbsp;Duffy&rsquo;s (2006) contained relatively few references; several seem to focus on social interaction with robots, but none on moral consideration of them. Duffy had written about anthropomorphism and perceptions of social robots before (e.g. Duffy, 2003), but does not seem to have explicitly linked the topic to rights for robots. Gunkel (2018) cited Duffy (2006), though Gunkel and Coeckelbergh&rsquo;s earliest writings on the topic do not seem to have done so.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref95" id="ftnt95">[95]</a><span class="c5">&nbsp;Indeed, Whitby (2008) and Goldie (2010) explicitly cited the proceedings of the second agent abuse workshop that had been held in Canada in 2006, with Goldie (2010) also citing a wider range of HRI research. In contrast, Levy (2009) cited extensively the research on artificial consciousness and on &ldquo;legal rights of robots,&rdquo; but not HRI research. The final chapter in Levy&rsquo;s (2005) book had addressed some similar themes, with many of the same citations, alongside others, such as writing by Floridi and Sanders. Coeckelbergh (2010) cited Levy (2009) but not Whitby (2006).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref96" id="ftnt96">[96]</a><span class="c5">&nbsp;For discussion, see Gunkel (2018, pp. 133-58). Harris and Anthis (2021) note a number of additional publications adopting a social-relational perspective.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref97" id="ftnt97">[97]</a><span class="c5">&nbsp;Of course, subsequent authors can easily manipulate these scales to add in artificial entities themselves. </span></p></div><div><p class="c4 c6"><a href="#ftnt_ref98" id="ftnt98">[98]</a><span class="c5">&nbsp;Gray et al. (2007) did not directly assess moral consideration, but they noted that their two identified dimensions of &ldquo;agency&rdquo; and &ldquo;experience&rdquo; relate &ldquo;to Aristotle&rsquo;s classical distinction between moral agents (whose actions can be morally right or wrong) and moral patients (who can have moral right or wrong done to them). Agency is linked to moral agency and hence to responsibility, whereas Experience is linked to moral patiency and hence to rights and privileges.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref99" id="ftnt99">[99]</a><span class="c5">&nbsp;In a subsequent paper, Gray and Wegner (2009) examined the distinction between moral agency and moral patiency more fully, briefly citing Floridi and Sanders (2004) for the idea that &ldquo;moral agency [could] be ascribed to&hellip; mechanical agents, such as robots or computers.&rdquo; They also cited other possible sources of interest in artificial entities, such as Haslam&rsquo;s (2006) &ldquo;integrative review&rdquo; of &ldquo;dehumanization&rdquo; which included brief discussion of &ldquo;[t]echnology in general and computers in particular.&rdquo; However, it is unclear whether knowledge of these papers influenced the earlier Gray et al. (2007) publication.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref100" id="ftnt100">[100]</a><span class="c7">&nbsp;Ward et al. (2013) found through four experiments that &ldquo;observing intentional harm to an unconscious entity&mdash;a vegetative patient, a robot, or a corpse&mdash;leads to augmented attribution of mind to that entity.&rdquo; One of the co-authors on this paper (who was also </span><span class="c7">the lead author&rsquo;s PhD supervisor</span><span class="c5">), Daniel M. Wegner, had been a co-author of the Gray et al. (2007) paper, which is cited prominently in the introduction.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref101" id="ftnt101">[101]</a><span class="c5">&nbsp;Piazza et al. (2014) were certainly aware of Gray et al. (2007), since they cited that paper, other papers by its co-authors, and personal correspondence with Heather Gray. Nevertheless, they chose to test their hypothesis that &ldquo;harmfulness&hellip; is an equally if not more important determinant of moral standing&rdquo; than moral patiency or agency through &ldquo;four studies using non-human animals as targets.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref102" id="ftnt102">[102]</a><span class="c5">&nbsp;This literature appears to have at least some precedent that was several decades older. For example, Haslam (2006) discussed contributions from 1983 and 1984 in support of the comment that &ldquo;[t]echnology in general and computers in particular are a common theme in work on dehumanization.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref103" id="ftnt103">[103]</a><span class="c5">&nbsp;See, for example, Starmans and Friedman&lsquo;s (2016) paper exploring whether &ldquo;autonomy makes entities less ownable,&rdquo; which cited Haslam (2006) and Loughnan and Haslam (2007) alongside Gray et al. (2007) in the introduction. Their third experiment compared vignettes describing a human, an alien, and a robot, asking participants to judge whether someone could own the entity in question, and whether behavior relating to owning the entity was morally acceptable. See also Swiderska and K&uuml;ster (2018), who &ldquo;investigated if the presence of a facial wound enhanced the perception of mental capacities (experience and agency) in response to images of robotic and human-like avatars, compared to unharmed avatars.&rdquo; They cited Haslam (2006) briefly after discussion of the implications of Epley et al. (2007), Waytz et al. (2010), Gray et al. (2007), and Gray and Wegner (2009).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref104" id="ftnt104">[104]</a><span class="c7">&nbsp;John Danaher&rsquo;s (2020) exposition and defense of &ldquo;ethical behaviourism&rdquo; is a good example. This theory &ldquo;holds that robots can have significant moral status if they are </span><span class="c7 c14">roughly performatively equivalent</span><span class="c7">&nbsp;to other entities that have significant moral status.&rdquo; Danaher notes that &ldquo;[v]ariations of this theory are&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hinted at in the writings of others&hellip; but it is believed that this article is the first to explicitly name it, and provide an extended defence of it.&rdquo; Danaher acknowledges not only the precursors of this specific line of thinking (e.g. Sparrow, 2004; Levy, 2009), but also other authors who have &ldquo;already defended the claim that we should take the moral status of robots seriously&rdquo; (e.g. Gunkel, </span><span class="c7">2018</span><span class="c5">).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref105" id="ftnt105">[105]</a><span class="c7">&nbsp;For example, Sparrow (2004) proposed a test for when machines have achieved &ldquo;moral standing comparable to a human,&rdquo; referencing Putnam, Kurzweil, Moravec, and Floridi. Magnani (2005) argued that technological &ldquo;things&rdquo; are better construed as &ldquo;moral mediators&rdquo; than as moral agents or patients, drawing primarily on Kantian ethics, but referencing literature on animal rights, legal personhood for artificial entities, and information ethics</span><span class="c7">.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref106" id="ftnt106">[106]</a><span class="c5">&nbsp;For further discussion, see &ldquo;Dismissal of the Importance of Moral Consideration of Artificial Entities&rdquo; in Harris and Anthis (2021) and note that few of the items given a score lower than three out of five for &ldquo;Argues for moral consideration?&rdquo; in Table 7 were published before 2010. Gunkel&rsquo;s (2018) section on &ldquo;Robot Rights or the Unthinkable&rdquo; includes a number of references from prior to 2010, but the contributions summarized in the chapter &ldquo;S1 !S2: Although Robots Can Have Rights, Robots Should Not Have Rights&rdquo; are almost all from 2010 or later.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref107" id="ftnt107">[107]</a><span class="c7">&nbsp;The next paragraph quotes Bill Gates, providing an alternative possible current affairs spark for interest in the topic, so it is unclear what the primary cause was. Gunkel (</span><span class="c7">2018</span><span class="c7">, pp. 193-4) claims that, &ldquo;[l]ike the April 2007 Dana Centre event and preceding press conference at the Science Media Center (see chapter 1), this BioCenter (</span><span class="c0 c7"><a class="c11" href="https://www.google.com/url?q=http://www.bioethics.ac.uk/&amp;sa=D&amp;source=editors&amp;ust=1660576307725883&amp;usg=AOvVaw2fg_H3ic_-KNxLBZFig3iP">http://www.bioethics.ac.uk/</a></span><span class="c5">) symposium was also developed in direct response to the Ipsos MORI document that was commissioned and published by the UK Office of Science and Innovation&rsquo;s Horizon Scanning Centre.&rdquo; No citation for this claim is provided.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref108" id="ftnt108">[108]</a><span class="c5">&nbsp;See the section on &ldquo;Artificial life and consciousness,&rdquo; especially footnote 15.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref109" id="ftnt109">[109]</a><span class="c5">&nbsp;Lehman-Wilzig (1981), Willick (1983), Freitas (1985), and Solum (1992) did not cite one another, Slater et al. (2006) did not cite previous relevant HCI and HRI works by Bartneck, Friedman, or Kahn, and Gunkel, Coeckelbergh, Duffy, and S&oslash;raker developed somewhat similar ideas without any citation of each other in their earliest writings (though S&oslash;raker and Coeckelbergh may have had communication). See the relevant subsections above.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref110" id="ftnt110">[110]</a><span class="c7">&nbsp;It is not, however, the only one; see the point below on </span><span class="c7">&ldquo;The growth in research on this topic reflects wider trends in academic research.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref111" id="ftnt111">[111]</a><span class="c5">&nbsp;Of the many figures in Zhang et al.&rsquo;s (2021) report, there is one that seems somewhat close to the trend in publications identified in Harris and Anthis (2021): &ldquo;U.S. government total contract spending on AI, FY 2001-20.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref112" id="ftnt112">[112]</a><span class="c7">&nbsp;In contrast, Google Scholar searches limited by year to each of 1990, 1991, 1992 and so on until 2021 suggest a publication pattern that looks quite unlike the growth in research on AI rights (see the spreadsheet &ldquo;</span><span class="c0 c7"><a class="c11" href="https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/18vzt9NvkNgZCq6-1CQPJe_8sQjxY9Or9nlGM9lNnNaQ/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1660576307734490&amp;usg=AOvVaw1dW7PlevoVu9T7-VMionlA">Google Scholar searches for &quot;e&quot;, &quot;robot&quot;, and &quot;&quot;artificial intelligence&quot;&quot;</a></span><span class="c7">&rdquo;).</span><span class="c5">&nbsp;The search for &ldquo;e&rdquo; was used as a proxy for all items in Google Scholar, although this does seem to focus on items that have &ldquo;e&rdquo; as a standalone word in the title or authors&rsquo; names; it may be a fairly random selection of publications but not the full set of publications on Google Scholar in that year. It is also not clear whether the &ldquo;About X results&rdquo; comments provided by Google Scholar do indeed represent all items on the database for each year.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref113" id="ftnt113">[113]</a><span class="c5">&nbsp;Consider by analogy that any number of unusual and niche topics might make it through the peer review process, but that their success in doing so does not guarantee the emergence of a research field around that topic.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref114" id="ftnt114">[114]</a><span class="c5">&nbsp;Floridi and the early writers on legal rights for artificial entities both seem to have drawn heavily on environmental ethics and, to a lesser extent, animal ethics, while the early transhumanist writers drew on ideas and research about artificial life and consciousness. Machine ethics then drew on transhumanism, and social-relational ethics in turn drew on machine ethics and several other previous streams of discussion. Many drew on science fiction. Some of the earliest relevant contributions from HCI and HRI may have arisen more independently. Relevant contributions from moral and social psychology seem to have been influenced by research on artificial life and consciousness and by HCI and HRI. See the relevant subsections of the &ldquo;Results&rdquo; section for further detail.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref115" id="ftnt115">[115]</a><span class="c5">&nbsp;Of course, it may not be a necessary condition. For example, their success in research relevant to AI rights and in other topics may both just be attributable to underlying factors such as high intelligence or persuasive writing style. See footnote 37 for a discussion of factors potentially contributing to Solum&rsquo;s success.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref116" id="ftnt116">[116]</a><span class="c7">&nbsp;See the spreadsheet &ldquo;</span><span class="c0 c7"><a class="c11" href="https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1CZUZasVAB5QHmWhOdO0CA1EOPRr37l5Gg2m23GhAJAo/edit?usp%3Dsharing&amp;sa=D&amp;source=editors&amp;ust=1660576307736481&amp;usg=AOvVaw1kQ1TWnWcLTwJBu3HPbH6p">Google Scholar citations for key authors</a></span><span class="c5">.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref117" id="ftnt117">[117]</a><span class="c5">&nbsp;For example, Harris and Anthis (2021) included four publications co-authored by Dennis K&uuml;ster and Aleksandra Swiderska with a combined total of only 10 Google Scholar citations at the time of checking in mid-2020, as well as five publications by Robin Mackenzie that had a combined total of 23 citations. Some reasonably high-quality contributions to the field that have adopted neither strategy seem to have attracted little attention (e.g. S&oslash;raker, 2006a).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref118" id="ftnt118">[118]</a><span class="c5">&nbsp;See footnote 30.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref119" id="ftnt119">[119]</a><span class="c5">&nbsp;Floridi (2002) summarizes Kant&rsquo;s ethical views as following this pattern. See also Wareham (2013) and Laukyte (2017).</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref120" id="ftnt120">[120]</a><span class="c7">&nbsp;Gunkel (2018, pp. xiii-xiv) credits the &ldquo;form and configuration&rdquo; of </span><span class="c7 c14">Robot Rights</span><span class="c7">&nbsp;to the </span><span class="c7 c14">Robophilosophy/TRANSOR 2016: What Social Robots Can and Should Do? </span><span class="c7">conference at the University of Aarhus, Denmark. </span><span class="c7">Despite the conference&rsquo;s focus on what </span><span class="c7 c14">robots </span><span class="c7">can and should do, Gunkel and Coeckelbergh were not the only contributors to consider how </span><span class="c7 c14">humans </span><span class="c7">should treat robots: other presentations included Dennis K&uuml;ster and Aleksandra &#346;widerska&rsquo;s (2016) &ldquo;Moral Patients: What Drives the Perceptions of Moral Actions Towards Humans and Robots?&rdquo; and Maciej Musial&rsquo;s (2016) &ldquo;Magical Thinking and Empathy Towards Robots.&rdquo; This is a fairly similar spread of topics to the first conference of the series (Seibt et al., 2014), which had included contributions from Gunkel and Coeckelbergh and discussions ranging from robots&rsquo; agency to social interaction to emotional capacities. Although the third conference of the series (Coeckelbergh et al., 2018) was more bereft of discussion of this topic, the fourth conference (N&oslash;rskov et al., 2021) contained a workshop entitled &ldquo;Should Robots Have Standing? The Moral and Legal Status of Social Robots,&rdquo; with six contributing presentations (two by Gunkel).</span></p><p class="c4 c6 c2"><span class="c5"></span></p><p class="c4 c6"><span class="c5">Floridi (2013, p. xviii) notes that, &ldquo;[t]he CEPE (Computer Ethics Philosophical Enquiries) meetings, organized by the International Society for Ethics and Information Technology, and the CAP (Computing and Philosophy) meetings, organized by the International Association for Computing and Philosophy, provided stimulating and fruitful venues to test some of the ideas presented in this and in the previous volume.&rdquo;</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref121" id="ftnt121">[121]</a><span class="c5">&nbsp;Gunkel (2012) is a borderline earlier example, with the moral patiency of artificial entities being the focus of about half of the book, and their moral agency being the focus of the other half. Floridi (2013) includes discussion of moral patiency when explaining information ethics, but also addresses many other topics.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref122" id="ftnt122">[122]</a><span class="c5">&nbsp;From the items included in Harris and Anthis&rsquo; (2021) systematic searches whose &ldquo;Primary framework or moral schema used&rdquo; was categorized as &ldquo;Legal precedent,&rdquo; the median number of citations tracked by Google Scholar (at the time of checking, in mid-2020), was 9, which compares to a median of 4 from the full sample of 294 items. Six out of 33 &ldquo;legal precedent&rdquo; (18%) articles had 50 citations or more, compared to 34 out of 294 (12%) in the full sample.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref123" id="ftnt123">[123]</a><span class="c5">&nbsp;See footnote 43.</span></p></div><div><p class="c4 c6"><a href="#ftnt_ref124" id="ftnt124">[124]</a><span class="c7">&nbsp;For example, Floridi (2013, p. xiv) notes that, &ldquo;[a]pparently, there are also some spiritual overtones and connections to Confucianism, Buddhism, Taoism, and Shintoism&rdquo; in his book. &ldquo;They were unplanned and they are not based on any intended study of the corresponding sources. I was made aware of such connections by other philosophers, while working on the articles that led to this book.&rdquo; Gunkel (2012) makes no mention of Asian philosophy in his (2012) book The Machine Question. However, his (2018a) </span><span class="c7 c14">Robot Rights</span><span class="c7">&nbsp;includes some discussion when citing other contributors and their ideas, such as Robertson (2014) and McNally and Inayatullah (1988). A subsequent article (Gunkel, 2020) responds to a recent article published in </span><span class="c7 c14">Science and Engineering Ethics</span><span class="c5">&nbsp;(Zhu et al., 2020) by exploring the implications of Confucianism for &ldquo;AI/robot ethics&rdquo; in more depth, especially the overlap between Confucianism and the social-relational perspective that Gunkel has developed.</span></p></div></body></html>
