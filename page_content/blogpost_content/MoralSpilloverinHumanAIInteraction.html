<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLUXza5VhXqVC6o75Eld_V98');ol.lst-kix_list_1-3{list-style-type:none}ol.lst-kix_list_1-4{list-style-type:none}.lst-kix_list_2-6>li:before{content:"\0025cf  "}.lst-kix_list_2-7>li:before{content:"\0025cb  "}ol.lst-kix_list_1-5{list-style-type:none}ol.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-4>li:before{content:"\0025cb  "}.lst-kix_list_2-5>li:before{content:"\0025a0  "}.lst-kix_list_2-8>li:before{content:"\0025a0  "}ol.lst-kix_list_1-1{list-style-type:none}ol.lst-kix_list_1-2{list-style-type:none}.lst-kix_list_1-1>li{counter-increment:lst-ctn-kix_list_1-1}ol.lst-kix_list_1-8.start{counter-reset:lst-ctn-kix_list_1-8 0}ol.lst-kix_list_1-5.start{counter-reset:lst-ctn-kix_list_1-5 0}ol.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_1-7>li{counter-increment:lst-ctn-kix_list_1-7}ol.lst-kix_list_1-8{list-style-type:none}ol.lst-kix_list_1-7.start{counter-reset:lst-ctn-kix_list_1-7 0}.lst-kix_list_1-2>li{counter-increment:lst-ctn-kix_list_1-2}.lst-kix_list_1-5>li{counter-increment:lst-ctn-kix_list_1-5}.lst-kix_list_1-8>li{counter-increment:lst-ctn-kix_list_1-8}ol.lst-kix_list_1-4.start{counter-reset:lst-ctn-kix_list_1-4 0}ol.lst-kix_list_1-1.start{counter-reset:lst-ctn-kix_list_1-1 0}.lst-kix_list_1-4>li{counter-increment:lst-ctn-kix_list_1-4}ol.lst-kix_list_1-6.start{counter-reset:lst-ctn-kix_list_1-6 0}ol.lst-kix_list_1-3.start{counter-reset:lst-ctn-kix_list_1-3 0}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_1-2.start{counter-reset:lst-ctn-kix_list_1-2 0}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"" counter(lst-ctn-kix_list_1-0,decimal) ") "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"" counter(lst-ctn-kix_list_1-1,lower-latin) ". "}.lst-kix_list_1-2>li:before{content:"" counter(lst-ctn-kix_list_1-2,lower-roman) ". "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"" counter(lst-ctn-kix_list_1-3,decimal) ". "}.lst-kix_list_1-4>li:before{content:"" counter(lst-ctn-kix_list_1-4,lower-latin) ". "}ol.lst-kix_list_1-0.start{counter-reset:lst-ctn-kix_list_1-0 0}.lst-kix_list_1-0>li{counter-increment:lst-ctn-kix_list_1-0}.lst-kix_list_1-6>li{counter-increment:lst-ctn-kix_list_1-6}.lst-kix_list_1-7>li:before{content:"" counter(lst-ctn-kix_list_1-7,lower-latin) ". "}.lst-kix_list_1-3>li{counter-increment:lst-ctn-kix_list_1-3}.lst-kix_list_1-5>li:before{content:"" counter(lst-ctn-kix_list_1-5,lower-roman) ". "}.lst-kix_list_1-6>li:before{content:"" counter(lst-ctn-kix_list_1-6,decimal) ". "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_2-0>li:before{content:"\0025cf  "}.lst-kix_list_2-1>li:before{content:"\0025cb  "}.lst-kix_list_1-8>li:before{content:"" counter(lst-ctn-kix_list_1-8,lower-roman) ". "}.lst-kix_list_2-2>li:before{content:"\0025a0  "}.lst-kix_list_2-3>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c23{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:150.4pt;border-top-color:#000000;border-bottom-style:solid}.c17{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:161.2pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:104.2pt;border-top-color:#000000;border-bottom-style:solid}.c31{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117.8pt;border-top-color:#000000;border-bottom-style:solid}.c29{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:146.2pt;border-top-color:#000000;border-bottom-style:solid}.c32{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:74.2pt;border-top-color:#000000;border-bottom-style:solid}.c0{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:90.8pt;border-top-color:#000000;border-bottom-style:solid}.c57{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:235.5pt;border-top-color:#000000;border-bottom-style:solid}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:200.2pt;border-top-color:#000000;border-bottom-style:solid}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c3{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-family:"Arial"}.c40{padding-top:18pt;padding-bottom:4pt;line-height:1.0;page-break-after:avoid;text-align:justify}.c30{padding-top:20pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;text-align:left}.c39{padding-top:20pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;text-align:justify}.c42{padding-top:18pt;padding-bottom:4pt;line-height:1.0;page-break-after:avoid;text-align:left}.c45{padding-top:20pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;text-align:left}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;height:12pt}.c44{padding-top:20pt;padding-bottom:6pt;line-height:1.1500000000000001;page-break-after:avoid;text-align:justify}.c55{padding-top:12pt;padding-bottom:12pt;line-height:1.0;text-align:justify}.c11{color:#000000;font-weight:400;text-decoration:none;font-family:"Calibri"}.c49{border-spacing:0;border-collapse:collapse;margin-right:auto}.c25{color:#000000;font-weight:700;text-decoration:none;font-family:"Arial"}.c34{-webkit-text-decoration-skip:none;color:#0563c1;text-decoration:underline;text-decoration-skip-ink:none}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:justify}.c26{padding-top:12pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c10{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c6{vertical-align:baseline;font-size:12pt;font-style:normal}.c47{background-color:#ffffff;max-width:451.3pt;padding:72pt 72pt 72pt 72pt}.c18{text-decoration:none;vertical-align:baseline;font-style:normal}.c53{text-decoration:none;vertical-align:baseline;font-size:12pt}.c7{orphans:2;widows:2}.c8{font-weight:400;font-family:"Arial"}.c4{color:inherit;text-decoration:inherit}.c43{background-color:#f5f5f5;font-size:15pt}.c52{font-family:"Arial";font-weight:700}.c46{width:33%;height:1px}.c35{padding:0;margin:0}.c14{font-size:11pt;color:#000000}.c36{height:118.8pt}.c41{vertical-align:super}.c15{font-size:11pt}.c28{font-style:italic}.c13{margin-left:36pt}.c50{height:388.8pt}.c33{height:0pt}.c37{color:#000000}.c16{font-size:10pt}.c51{height:31pt}.c22{padding-left:0pt}.c19{margin-left:18pt}.c27{height:22pt}.c56{background-color:#f5f5f5}.c48{height:145.8pt}.c54{height:280.8pt}.c21{height:12pt}.c58{color:#1155cc}.c38{font-size:18pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Calibri"}p{margin:0;color:#000000;font-size:12pt;font-family:"Calibri"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.1500000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:14pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:4pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:12pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:11pt;color:#000000;font-weight:700;font-size:11pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:10pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:2pt;font-family:"Calibri";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c47 doc-content"><div><p class="c5 c7 c21"><span class="c11 c6"></span></p></div><p class="c5 c7"><span class="c8 c28">Edited by Jacy Reese Anthis. </span><span class="c53 c8 c28 c37">Many thanks to Michael Dello-Iacovo, Merel Keijsers, Ali Ladak, and Brad Saad for their thoughtful feedback.</span></p><h1 class="c7 c30" id="h.jgmqlghwo066"><span class="c2">Summary</span></h1><p class="c5 c7"><span class="c8 c28">Moral spillover</span><span class="c8">&nbsp;is the transfer of moral attitudes or behaviors from one setting to another</span><span class="c8 c58">&nbsp;</span><span class="c8">(e.g., from one being to another, from one behavior to a related behavior, from one group today to related groups in the future). Examples include the </span><span class="c8">transfer of</span><span>&nbsp;</span><span class="c8">anti-slavery activism to animal rights activism (</span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sentienceinstitute.org/british-antislavery&amp;sa=D&amp;source=editors&amp;ust=1684459462349999&amp;usg=AOvVaw1bJ_s8nL1rFhQIVsL1lS6q">Anthis and Anthis 2017</a></span><span class="c8">)</span><span class="c8">,</span><sup class="c8 c41"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c8">&nbsp;children&rsquo;s </span><span class="c8">moral consideration of a biological dog to a robot dog (</span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2016-48422-007&amp;sa=D&amp;source=editors&amp;ust=1684459462350714&amp;usg=AOvVaw05qbpBBWi9Q5Por67pl4Ct">Chernyak and Gary 2016</a></span><span class="c8">)</span><span class="c8">, and household </span><span class="c8">energy conservation to water conservation (</span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0921344921001774&amp;sa=D&amp;source=editors&amp;ust=1684459462350995&amp;usg=AOvVaw1YvZKW4vPhBwFZpiYr9kHI">Liu et al. 2021</a></span><span class="c8">)</span><span class="c8">. Moral spillover seems to be an important driver of </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0016328721000641&amp;sa=D&amp;source=editors&amp;ust=1684459462351246&amp;usg=AOvVaw06xznyN9ydafmX9GkvJyme">moral circle expansion</a></span><span class="c8">. Here, we review moral spillover research with a focus on human-AI interaction.</span><span>&nbsp;</span><span class="c8">P</span><span class="c8">sychological factors, such as pre-existing attitudes towards AIs, as well as AI attributes, such as human-likeness and social group membership, could influence moral spillover between humans and AIs. Spillover of moral consideration to AIs might be hindered by factors such as the </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265&amp;sa=D&amp;source=editors&amp;ust=1684459462351563&amp;usg=AOvVaw1XYTVbhw1XNCxhTcaUJEjf">intention-action gap</a></span><span class="c8">&nbsp;and might be facilitated by interventions such as human-AI contact and promoting a core belief that the moral consideration of AIs is important. We conclude with future research suggestions to examine how pre-existing attitudes affect moral spillover, the potential backfiring of spillover interventions, how spillover affects AIs on a spectrum of similarity to humans, and how temporal spillover functions to shape moral consideration of future AIs, especially based on core beliefs about AI. </span></p><h1 class="c7 c44" id="h.6aevp7z97h5u"><span class="c2">Table of contents</span></h1><p class="c10"><span class="c3"><a class="c4" href="#h.jgmqlghwo066">Summary</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.6aevp7z97h5u">Table of contents</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.z0ay041xj2ze">Introduction</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.8ub9rsi830s">Types of spillover</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.9ifkekxvmwxm">What factors shape the likelihood of moral spillover?</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.lmixdc2v5jt">What are the implications of human-AI interaction research for moral spillover?</a></span></p><p class="c10 c19"><span class="c3"><a class="c4" href="#h.c7f2trae5117">&ldquo;Computers are social actors&rdquo;</a></span></p><p class="c10 c19"><span class="c3"><a class="c4" href="#h.uwtlrinq1eh6">Social group membership</a></span></p><p class="c10 c19"><span class="c3"><a class="c4" href="#h.uq2qw6mlkpih">Human and AI features</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.bdv2dry64m6k">What is the difference between the spillover of actions and intentions?</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.qc42eymkb64l">What interventions can we use to induce the spillover of moral consideration?</a></span></p><p class="c10"><span class="c3"><a class="c4" href="#h.jxrvu05vmabb">What future research is needed?</a></span></p><h1 class="c45 c7" id="h.z0ay041xj2ze"><span class="c2">Introduction</span></h1><p class="c5 c7"><span class="c8">The well-being of future </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience&amp;sa=D&amp;source=editors&amp;ust=1684459462353707&amp;usg=AOvVaw1GRhAvrV4xsW_EydtY72fd">sentient artificial intelligences</a></span><sup class="c8"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c8">&nbsp;(AIs) depends in part on whether moral consideration transfers to them from consideration already extended to other beings, such as humans, nonhuman animals, and AIs who already exist.</span><sup class="c8"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c8">&nbsp;The</span><span class="c8">&nbsp;transfer of moral attitudes and behaviors, such as moral consideration, from one setting to another can be defined as </span><span class="c8 c28">moral spillover</span><span class="c8">.</span><sup class="c8"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c8">&nbsp;Moral spillover may be an important part of </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0016328721000641&amp;sa=D&amp;source=editors&amp;ust=1684459462354188&amp;usg=AOvVaw2LlYJpujarG5-tCF6sD2S9">moral circle expansion</a></span><span class="c8">, both for the circles of individual humans and of human societies. For example, a 2017 Sentience Institute </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sentienceinstitute.org/british-antislavery&amp;sa=D&amp;source=editors&amp;ust=1684459462354399&amp;usg=AOvVaw1SGrMJBvrIl37n7QmNaqIC">report</a></span><span class="c8">&nbsp;on the 1800s anti-slavery movement found that the consideration anti-slavery activists had for humans transferred to animals, making them some of the first animal rights activists. </span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Given the </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ai100.stanford.edu/2021-report/standing-questions-and-responses/sq2-what-are-most-important-advances-ai&amp;sa=D&amp;source=editors&amp;ust=1684459462354777&amp;usg=AOvVaw2ke91tInNC8gnyB2RZRhbW">rapid growth</a></span><span class="c8">&nbsp;of AI application and sophistication and an increasing likelihood that the number of future sentient beings will be </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.overcomingbias.com/p/a-galaxy-on-earthhtml&amp;sa=D&amp;source=editors&amp;ust=1684459462354989&amp;usg=AOvVaw0sPjQ-Ytky20-FtPDGzYEb">vast</a></span><span class="c8">, here we analyze whether the spillover of moral consideration is feasible or likely between beings that are granted (at least some) consideration (e.g., humans, animals) and future AIs. In psychology and human-robot interaction (HRI) studies, AIs are often used to improve the moral treatment of humans, suggesting that moral consideration can transfer from AIs to humans. For example, positive emotions from being hugged by a robot </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/document/8172336/&amp;sa=D&amp;source=editors&amp;ust=1684459462355211&amp;usg=AOvVaw2jlGpW9hpQrkLNIKt4szR6">spilled over</a></span><span class="c8">&nbsp;to increase donations to human-focused charities. Some research suggests that the transfer of moral consideration from humans or nonhuman animals to AIs is also possible. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2016-48422-007&amp;sa=D&amp;source=editors&amp;ust=1684459462355445&amp;usg=AOvVaw1kkAfou556NeQq0IQkjx-b">A 2016 study</a></span><span class="c8">&nbsp;showed that 5- and 7-year-old children with biological dogs at home treated </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://robots.ieee.org/robots/aibo2018/&amp;sa=D&amp;source=editors&amp;ust=1684459462355746&amp;usg=AOvVaw0HEtUXhfWSkGVUobFalra_">robot dogs</a></span><span class="c8">&nbsp;better than children without biological dogs. This suggests that moral spillover to AIs might occur </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S2352250X21001378&amp;sa=D&amp;source=editors&amp;ust=1684459462355992&amp;usg=AOvVaw2Pd476HefEy8YtA3qICORH">incidentally or automatically</a></span><span class="c8">&nbsp;as part of humans&rsquo; </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00822/full&amp;sa=D&amp;source=editors&amp;ust=1684459462356231&amp;usg=AOvVaw0IlZK3sOQ6lxGjUv_7Mwsa">social relationships</a></span><span class="c8">. </span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">We do not know whether or not moral consideration will reliably transfer to and across the diverse range of AIs with different appearances, inner features, or mental capacities who are </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://longtermrisk.org/risks-of-astronomical-future-suffering/&amp;sa=D&amp;source=editors&amp;ust=1684459462356702&amp;usg=AOvVaw1f3bUDPybX44yL-d_TITps">likely to proliferate</a></span><span class="c8">&nbsp;in the future. There is little evidence on whether or not moral consideration would transfer from very different beings to AIs who display very few or none of the same features. For instance, moral consideration of a biological dog might spill over to moral consideration of a robot dog but it may not spill over to moral consideration of a disembodied large language model like </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/OpenAI%23Generative_models&amp;sa=D&amp;source=editors&amp;ust=1684459462356963&amp;usg=AOvVaw39edXjdjYJBRDLeU-3qJ5y">GPT-n</a></span><span class="c8">. This might be especially significant if arguably superficial features such as appearance, substrate, or purpose override the effects of </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s43681-023-00260-1&amp;sa=D&amp;source=editors&amp;ust=1684459462357262&amp;usg=AOvVaw2J9AEJdhU-SIRtBq1RyG0U">features that grant moral standing</a></span><span class="c1">&nbsp;(e.g., sentience). For example, a sentient disembodied algorithm or a sentient cell-like robot, who could theoretically benefit from the transfer of moral consideration based on their sentience, might not. </span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">This post reviews research on moral spillover in the context of AIs and examines factors that might influence its occurrence. We suggest that spillover might foster the moral consideration of AIs and call for more research to investigate spillover effects on a range of current and future AIs.</span></p><h1 class="c7 c45" id="h.8ub9rsi830s"><span class="c2">Types of spillover</span></h1><p class="c5 c7"><span class="c8">In economics, </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Spillover_(economics)&amp;sa=D&amp;source=editors&amp;ust=1684459462357789&amp;usg=AOvVaw2_bjI_2RbTiFV3fVpjJfkL">spillovers</a></span><span class="c8">, also known as externalities, are a natural part of structural theories in which a transaction affects non-participants, such as if an event in an economy affects another&mdash;usually more dependent&mdash;economy. In epidemiology, a </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Spillover_infection&amp;sa=D&amp;source=editors&amp;ust=1684459462358028&amp;usg=AOvVaw3XnF-Ywx5KoOJgpnuY3REE">spillover event</a></span><span class="c8">&nbsp;occurs when a pathogen transfers from its </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Natural_reservoir&amp;sa=D&amp;source=editors&amp;ust=1684459462358410&amp;usg=AOvVaw3EZCvEVAOpelfEcOZkvwow">reservoir population</a></span><span class="c8">, such as animals, to a novel host, such as humans. In psychology, a </span><span class="c8 c37"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2017-08484-007&amp;sa=D&amp;source=editors&amp;ust=1684459462358689&amp;usg=AOvVaw0NXemVizPRgWVFsSIh7tFl">spillover</a></span><span class="c8">&nbsp;occurs when the adoption of</span><span>&nbsp;</span><span class="c8">an attitude or behavior transfers to other related attitudes or behaviors. Based on the latter definition, moral spillover occurs when moral attitudes or behaviors towards a being or group of beings transfer to another setting (e.g., to another being or group of beings). </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2017-08484-007&amp;sa=D&amp;source=editors&amp;ust=1684459462358953&amp;usg=AOvVaw2o7hY50Vyft847jcKkfO1p">Nilsson et al. (2017</a></span><span class="c8"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2017-08484-007&amp;sa=D&amp;source=editors&amp;ust=1684459462359152&amp;usg=AOvVaw1W_WI_rzgYVeCibtyKde-Y">) </a></span><span class="c8">suggested</span><span class="c8">&nbsp;a distinction between three types of spillover:</span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8 c28">Figure 1</span><span class="c1">: Types of Spillover</span></p><p class="c5 c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 218.67px;"><img alt="" src="images/moral-spillover-in-human-ai-interaction/image1.png" style="width: 601.70px; height: 218.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ol class="c35 lst-kix_list_1-0 start" start="1"><li class="c5 c7 c13 c22 li-bullet-0"><span class="c52 c37">Behavioral</span><span class="c8 c37">: Behavior A increases the probability of behavior B. </span><span>&nbsp; &nbsp; &nbsp;</span></li></ol><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7 c13"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0095069620300486&amp;sa=D&amp;source=editors&amp;ust=1684459462360117&amp;usg=AOvVaw2p-AVgGE6KH0THwBfyL7h7">Carlsson and colleagues (2021)</a></span><span class="c8">&nbsp;showed that pro-environmental behaviors, such </span><span class="c8 c37">as conserving </span><span class="c8">water</span><span class="c8 c37">, can spill over to other pro-environmental behaviors, such as conserving </span><span class="c8">electricity</span><span class="c8 c37">. </span><span class="c8">In the context of AI, researchers could ask questions like, can a prosocial</span><sup class="c8 c41"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c1">&nbsp;behavior towards AIs lead to other prosocial behaviors towards AIs? For example, could greeting an AI assistant spill over to protecting this assistant from mistreatment?</span></p><p class="c5 c7 c21"><span class="c1"></span></p><ol class="c35 lst-kix_list_1-0" start="2"><li class="c5 c7 c13 c22 li-bullet-0"><span class="c37 c52">Contextual</span><span class="c8 c37">: A behavior or attitude in context A increases the probability of this behavior or attitude in context B.</span></li></ol><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7 c13"><span class="c8 c37">Research on contextual AI moral spillover could address questions like, can the moral consideration of sentient beings such as </span><span class="c8">animals</span><span class="c8 c37">&nbsp;spill over to AIs (e.g., </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2016-48422-007&amp;sa=D&amp;source=editors&amp;ust=1684459462361251&amp;usg=AOvVaw1pQL94vm9fRtTB2NTkJt5d">Chernyak and Gary 2016</a></span><span class="c1">)? </span></p><p class="c5 c7 c21"><span class="c8 c14 c18"></span></p><ol class="c35 lst-kix_list_1-0" start="3"><li class="c5 c7 c13 c22 li-bullet-0"><span class="c52 c37">Temporal</span><span class="c8 c37">: A behavior or attitude at </span><span class="c8">time point</span><span class="c8 c37">&nbsp;A increases the frequency of the same or similar behavior or attitude at (a temporally distant) </span><span class="c8">time point</span><span class="c8 c37">&nbsp;B.</span><sup class="c8 c41 c37"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c1">&nbsp;</span></li></ol><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7 c13"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02699/full&amp;sa=D&amp;source=editors&amp;ust=1684459462362298&amp;usg=AOvVaw2VBXMjL--839GITzmTLmnj">Elf et al. (2019)</a></span><span class="c1">&nbsp;showed that the frequency of pro-environmental behaviors, such as buying eco-friendly products, increased a year after their initial adoption. Temporal spillover may be especially important for AIs given that they are likely to proliferate in the future. Increasing the moral consideration of AIs now might increase the moral and social inclusion of sentient AIs hundreds of years in the future.</span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">Behavioral, contextual, and temporal spillovers can occur at the same time. These kinds of spillovers can also occur at multiple levels (e.g., from individual to individual, from individual to group, from group to group). Below are some examples.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c24 c7"><span class="c8 c28">Table 1:</span><span class="c1">&nbsp;Examples of Spillover Types at Different Levels</span></p><p class="c24 c7 c21"><span class="c1"></span></p><a id="t.4d3699aa403dd55bd8dbd5d2d2461ab22847bee5"></a><a id="t.0"></a><table class="c49"><tr class="c33"><td class="c20" colspan="1" rowspan="1"><p class="c5"><span class="c25 c6">Type</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c25 c6">Level</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c25 c6">Example</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Behavioral</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Turning off an unused electronic device increases the probability of riding a bike to work</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Caring for a sick animal increases the probability of becoming vegan</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Volunteering at a refugee center increases the probability of advocating for human rights</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Contextual</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Saving water at home increases the probability of saving water at work</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Positive attitudes toward a biological dog increase the probability of positive attitudes toward robot dogs</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Donating to animal charities increases the probability of donating to AI charities</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Temporal</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">A donation to a charity increases the frequency of donations to this charity in the future </span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Purchasing an energy-saving lamp increases the frequency of purchasing energy-saving lamps in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Giving blood to patients in need increases the frequency of giving blood again in the future</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Behavioral x Contextual</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Saving electricity at work increases the probability of limiting food waste at home</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Caring for a pet at home increases the probability of acting compassionately toward animals in the wild</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Giving blood to patients in need increases the probability of volunteering to help flood victims</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Behavioral x Temporal</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Using a recycled product increases the frequency of recycling in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Treating a wounded animal increases the frequency of using animal-free products in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Voting for animal rights increases the frequency of volunteering at an animal shelter in the future</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Contextual x Temporal</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Helping an AI household assistant increases the frequency of helping an AI workplace assistant in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Donating for the treatment of a sick dog increases the frequency of donating to an animal charity in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Trying vegan food increases the frequency of trying other animal-free products in the future</span></p></td></tr><tr class="c27"><td class="c20" colspan="1" rowspan="3"><p class="c5"><span class="c1">Behavioral x Contextual x Temporal</span></p></td><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to individual</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Conserving electricity at home increases the frequency of cycling to work in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Individual to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Positive attitudes towards a biological pet increase the frequency of voting for wild animal rights in the future</span></p></td></tr><tr class="c27"><td class="c29" colspan="1" rowspan="1"><p class="c5"><span class="c1">Group to group</span></p></td><td class="c12" colspan="1" rowspan="1"><p class="c5"><span class="c1">Voting for AI rights increases the frequency of being kind to AI assistants at work in the future</span></p></td></tr></table><p class="c24 c7 c21"><span class="c8 c28 c37 c53"></span></p><p class="c5 c7"><span class="c8">The examples outlined in Table 1 involve the transfer of positive or prosocial attitudes and behaviors. However, negative attitudes and behaviors can also spill over. This transfer can also be behavioral (e.g., ignoring an AI&rsquo;s plea for help leads to turning off an AI without their consent), contextual (e.g., being rude to a household AI increases the probability of being rude to a workplace AI), or temporal (e.g., intentionally damaging one AI now leads to frequently damaging AIs at a later point). For instance, </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S1071581916301768&amp;sa=D&amp;source=editors&amp;ust=1684459462378401&amp;usg=AOvVaw1ndoouXWeuXZ9J5HTt9VYi">previous research</a></span><span class="c1">&nbsp;has shown that feeling threatened by a highly autonomous AI increases negative attitudes toward all AIs. See Figure 2 for a possible taxonomy of how these spillover types might intersect. </span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c24 c7"><span class="c8 c28">Figure 2: </span><span class="c1">Possible Taxonomy of Spillover Types</span></p><p class="c24 c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 559.06px; height: 385.50px;"><img alt="A picture containing text

Description automatically generated" src="images/moral-spillover-in-human-ai-interaction/image2.png" style="width: 559.06px; height: 385.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">In the moral domain, the transfer of positive attitudes and behaviors can be associated with increased moral consideration between different groups and settings, like when the moral consideration of a biological dog increased the moral consideration of a robot dog. The transfer of negative attitudes and behaviors might lead to decreased moral consideration. </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0010027712000820&amp;sa=D&amp;source=editors&amp;ust=1684459462379349&amp;usg=AOvVaw2sYw_L3UwNG7se5MoEVtAU">Uhlmann et al. (2012)</a></span><span class="c8">&nbsp;showed that negative evaluations of a criminal spill over to their biological relatives, who are then more likely to be punished by law than non-biological relatives. The transfer of negative attitudes and behaviors can pose a significant risk to the well-being of sentient AIs, especially if they are</span><span>&nbsp;</span><span class="c8">held to different standards than other entities. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.degruyter.com/document/doi/10.1515/pjbr-2020-0017/html?lang%3Den&amp;sa=D&amp;source=editors&amp;ust=1684459462379788&amp;usg=AOvVaw3CbC-3vU0gX0Pf91uZK2NJ">Bartneck and Keijsers (2020)</a></span><span class="c1">&nbsp;showed that a mistreated robot who fights back is perceived as more abusive than a mistreated human who fights back. The transfer of negative attitudes towards one AI to all AIs could decrease the moral consideration of AIs and obstruct their inclusion in the moral circle.</span></p><h1 class="c45 c7" id="h.9ifkekxvmwxm"><span class="c2">What factors shape the likelihood of moral spillover?</span></h1><p class="c5 c7"><span class="c1">Whether or not spillover occurs depends on factors such as personality traits and social context. The impact of these factors on spillover has been studied largely in the context of environmental behavior. The same factors are likely valuable for understanding when and how moral spillover applies to AIs. Table 2 summarizes some factors identified in previous research.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c24 c7"><span class="c8 c28">Table 2</span><span class="c1">: Factors Influencing the Occurrence of Spillover</span></p><p class="c24 c7 c21"><span class="c1"></span></p><a id="t.db38eee223e85e74f635c56392f98cb3eb79c4b6"></a><a id="t.1"></a><table class="c49"><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c5"><span class="c25 c6">Factor</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5"><span class="c25 c6">Example</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5"><span class="c6 c25">Source(s)</span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c24 c7"><span class="c8">Pre-existing attitudes</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">Contact with a robot spilled over to more negative attitudes toward all robots for people who already had negative attitudes towards robots. &nbsp;</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/7745228&amp;sa=D&amp;source=editors&amp;ust=1684459462382800&amp;usg=AOvVaw0xL7o_M8P9GJzQ5rvYhquT">Wullenkord et al., 2016</a></span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c24 c7"><span class="c8">P</span><span class="c8">ersonality traits</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">Trusting an individual robot spilled over to trusting robots in general for people who were more extroverted and open to experience. </span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s12369-016-0357-8&amp;sa=D&amp;source=editors&amp;ust=1684459462383911&amp;usg=AOvVaw3CllA9WLVfnFmOGmq_Kr0B">Ivaldi et al., 2017</a></span><span class="c1">;</span></p><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.frontiersin.org/articles/10.3389/fpsyg.2020.568256/full&amp;sa=D&amp;source=editors&amp;ust=1684459462384251&amp;usg=AOvVaw3k3zeta7G47sDzkcyjNcre">Oksanen et al., 2020</a></span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c24 c7"><span class="c8">Self-identity</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">A pro-environmental behavior spilled over to other pro-environmental behaviors more so for people who identified with caring about the environment. </span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.tandfonline.com/doi/abs/10.1080/13504622.2016.1250148?journalCode%3Dceer20&amp;sa=D&amp;source=editors&amp;ust=1684459462385148&amp;usg=AOvVaw2OTJZGnFJcmxwS_l40zIz9">Nilsson, Bergquist &amp; Schulz, 2017</a></span><span class="c1">;</span></p><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://journals.sagepub.com/doi/abs/10.1177/0013916512475209&amp;sa=D&amp;source=editors&amp;ust=1684459462385466&amp;usg=AOvVaw1WC6jC-PLhY-8k-8d5oePn">Van der Werff, Steg &amp; Keizer, 2014</a></span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c24 c7"><span class="c8">Social learning</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">Interacting with a robot that acted prosocially towards humans spilled over to increased prosocial treatment of other humans.</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000340&amp;sa=D&amp;source=editors&amp;ust=1684459462386280&amp;usg=AOvVaw3EUG7VCjE-BHmyugiNSFc-">Peter, </a></span><span class="c3 c56"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000340&amp;sa=D&amp;source=editors&amp;ust=1684459462386495&amp;usg=AOvVaw2eDJZNWh-Mprho6WIEq-8R">K&uuml;hne</a></span><span class="c3 c43"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000340&amp;sa=D&amp;source=editors&amp;ust=1684459462386700&amp;usg=AOvVaw3CMl4EKuiFxwLJ6Kg85Z7W">&nbsp;</a></span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000340&amp;sa=D&amp;source=editors&amp;ust=1684459462386912&amp;usg=AOvVaw3lJg74V5vQuM9L3MYlprqw">&amp; Barco, 2021</a></span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c24 c7"><span class="c8">Perceived value/impact</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">Purchasing an ethical good spilled over to purchasing another ethical good more so for people who considered ethical purchases socially impactful.</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.frontiersin.org/articles/10.3389/fpsyg.2021.668399/full&amp;sa=D&amp;source=editors&amp;ust=1684459462387968&amp;usg=AOvVaw3U2E3kz92tEbwFYWnshIPY">Schumann et al., 2021</a></span></p></td></tr><tr class="c33"><td class="c23" colspan="1" rowspan="1"><p class="c7 c24"><span class="c8">Similarity</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c1">A pro-environmental behavior spilled over to other pro-environmental behaviors more when these behaviors were similar (e.g., turning off a light and turning of a computer when leaving a room).</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0272494414000516&amp;sa=D&amp;source=editors&amp;ust=1684459462388816&amp;usg=AOvVaw3YObdNkHKJzs5E7JFKfFhM">Littleford, Ryley &amp; Firth, 2014</a></span><span class="c1">;</span></p><p class="c5 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.tandfonline.com/doi/abs/10.1080/13504622.2016.1250148?journalCode%3Dceer20&amp;sa=D&amp;source=editors&amp;ust=1684459462389157&amp;usg=AOvVaw0525M270zWsKF_0yq7_RM-">Nilsson, Bergquist &amp; Schulz, 2017</a></span></p></td></tr></table><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">One of the more studied factors shaping whether or not spillover occurs is pre-existing attitudes. If pre-existing attitudes towards a spillover target are negative, then the transfer of negative attitudes and behaviors is more likely. If pre-existing attitudes are positive, then the transfer of positive attitudes and behaviors is more likely. Below are three notable studies:</span></p><ul class="c35 lst-kix_list_2-0 start"><li class="c5 c7 c13 c22 li-bullet-0"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0272494419306218?via%253Dihub&amp;sa=D&amp;source=editors&amp;ust=1684459462389776&amp;usg=AOvVaw3XEo3Q9EkzQdVIljMNlsMR">Henn et al. (2020)</a></span><span class="c3">&nbsp;</span><span class="c1">showed that pre-existing attitudes were the driving force behind spillovers in pro-environmental behavior across two separate cohorts: pre-existing positive attitudes towards the environment led to greater spillover between different kinds of pro-environmental behaviors (e.g., saving electricity and saving water). </span></li><li class="c5 c7 c13 c22 li-bullet-0"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/7745228&amp;sa=D&amp;source=editors&amp;ust=1684459462390200&amp;usg=AOvVaw3-pV5BK9h_LDcy-hn5fh0K">Wullenkord et al. (2016)</a></span><span class="c1">&nbsp;showed that pre-existing negative emotions towards robots spilled over to feeling more negative emotions for robots in general following contact with a robot, compared to a control condition that involved no contact.</span></li><li class="c5 c7 c13 c22 li-bullet-0"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/5598679&amp;sa=D&amp;source=editors&amp;ust=1684459462390496&amp;usg=AOvVaw0AvBs-wM2B-QhhRkfE0sYi">Stafford et al. (2010)</a></span><span class="c1">&nbsp;showed that pre-existing positive attitudes towards robots became more pronounced after meeting a robot; suggesting that the transfer of positive attitudes from one robot to all robots is easier when positive attitudes towards robots are already present.</span></li></ul><h1 class="c45 c7" id="h.lmixdc2v5jt"><span class="c2">What are the implications of human-AI interaction research for moral spillover? </span></h1><p class="c5 c7"><span class="c1">General themes in human-AI interaction have emerged from HRI research: &ldquo;computers are social actors,&rdquo; the importance of social group membership, and how human and AI features affect interactions. In this section, we consider what these themes imply for moral spillover between humans and AIs, focusing on their implications for the moral consideration of AIs.</span></p><h2 class="c7 c40" id="h.c7f2trae5117"><span class="c18 c8 c37 c38">&ldquo;Computers are social actors&rdquo;</span></h2><p class="c5 c7"><span class="c8">The </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/191666.191703&amp;sa=D&amp;source=editors&amp;ust=1684459462391239&amp;usg=AOvVaw2DVSumC3yYcN-5uyQiCtXl">&ldquo;computers are social actors&rdquo; (CASA) framework</a></span><span class="c8">&nbsp;suggests that machines with human-like capacities, such as verbal or written communication, interactivity (e.g., a response when a button is pressed), and the ability to perform traditional human tasks elicit an automatic attribution of social capacities. This affects responses to them. For example, </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.mdpi.com/2414-4088/3/1/20&amp;sa=D&amp;source=editors&amp;ust=1684459462391506&amp;usg=AOvVaw2BlkuS2de5ze6fGPZCpnNe">Lee et al. (2019)</a></span><span class="c1">&nbsp;showed that people felt more positively towards autonomous vehicle voice agents who conformed to social role stereotypes (i.e., informative male voice and social female voice) compared to agents who did not.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">What does CASA imply for moral spillover? Moral spillover might be automatic between humans and AIs because of humans&rsquo; propensity to think of AIs as social actors.</span><span>&nbsp;</span><span class="c8">Moral spillover might occur more for AIs with human-like capacities, given that such</span><span>&nbsp;</span><span class="c8">features are blatant CASA cues. Some studies using the CASA framework have shown the transfer of consideration from AIs to humans. For example, </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563221000340&amp;sa=D&amp;source=editors&amp;ust=1684459462392026&amp;usg=AOvVaw0OVtO2d6QHhZaY6T77v0t-">Peter et al. (2021)</a></span><span class="c8">&nbsp;showed that people who interacted with a prosocial robot, compared to a less prosocial robot, gave more resources to other humans.</span><span>&nbsp;</span><span class="c1">Whether or not similar spillover effects emerge from humans towards AIs is an open question. For instance, the spillover of consideration from humans to AIs could be hindered for AIs who do not display enough human-like capacities (e.g., communication, emotional expression) to trigger CASA attributions.</span></p><h2 class="c40 c7" id="h.uwtlrinq1eh6"><span class="c18 c8 c37 c38">Social group membership</span></h2><p class="c5 c7"><span class="c8">Social group membership will likely impact moral spillover from humans to AIs since AIs are increasingly coexisting with humans in </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.techtarget.com/iotagenda/answer/What-is-human-robot-teaming-and-what-are-its-benefits&amp;sa=D&amp;source=editors&amp;ust=1684459462392675&amp;usg=AOvVaw1e7HdoUUX9xVIm8DCc6HVo">various group settings</a></span><span class="c8">. Generally, people tend to favor </span><span class="c8 c28">ingroups </span><span class="c8">(i.e., members of the same social group) over </span><span class="c8 c28">outgroups</span><span class="c8">, (i.e., members of another social group). </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2014-37732-001&amp;sa=D&amp;source=editors&amp;ust=1684459462392982&amp;usg=AOvVaw3IoW7lh0MkTyGBSNa8bUlP">Ingroup favoritism</a></span><span class="c1">&nbsp;increases cooperation with members of the same group, which can be based on features such as ethnicity, religion, gender, and ideology. </span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Moral consideration for ingroup humans could spill over onto ingroup AIs and consideration for ingroup AIs could spill on to other AIs. Shared social group membership can also translate into refusal to harm an ingroup AI. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/8172280&amp;sa=D&amp;source=editors&amp;ust=1684459462393355&amp;usg=AOvVaw2fE8GKOqnkhIrJSoyTFBl9">Sembroski et al. (2017)</a></span><span class="c8">&nbsp;found that people refused to turn off a robot teammate despite being instructed to do so. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://dl.acm.org/doi/abs/10.5555/3523760.3523851&amp;sa=D&amp;source=editors&amp;ust=1684459462393575&amp;usg=AOvVaw0R1txC6kbqrAxi6woaP55S">Preliminary research</a></span><span class="c8">&nbsp;has also shown that feelings of warmth towards a robot who was perceived as a friend spilled over to positive attitudes towards robots in general. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Ingroup-based spillover between humans and AIs likely has limits. </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0747563220303320&amp;sa=D&amp;source=editors&amp;ust=1684459462393950&amp;usg=AOvVaw2UkSeUBXX_hNinfh-wt3Ez">Savela et al. (2021)</a></span><span class="c8">&nbsp;showed that humans identified less with a team that was mostly composed of robots, suggesting an underlying &ldquo;us&rdquo; versus &ldquo;them&rdquo; distinction that threatens the identity,</span><span>&nbsp;</span><span class="c1">status, or control of the human minority. This could potentially inhibit the spillover of moral consideration between humans and AIs, at least in some cases. If humans and AIs coexist in social groups mostly composed of AIs (e.g., in the workplace), this could lead to the transfer of negative attitudes from the ingroup AIs to AIs in general, which might inhibit the inclusion of AIs in the moral circle. </span></p><h2 class="c7 c42" id="h.uq2qw6mlkpih"><span class="c8">Human and AI features</span></h2><p class="c5 c7"><span class="c1">Additional research has focused on how the features of AIs (e.g., autonomy, usefulness/ease of operation) and of humans (e.g., cultural values towards AIs, owning AI or robotic devices) shape spillover. This research is likely important to understanding whether or not moral spillover occurs in the context of AIs. This research is summarized below. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c24 c7"><span class="c8 c28">Table 3</span><span class="c1">: Human and AI Features Affecting Spillover</span></p><p class="c24 c7 c21"><span class="c1"></span></p><a id="t.addd2cab2c5c61322bbf67cc7e7d87ad91ecfccd"></a><a id="t.2"></a><table class="c49"><tr class="c51"><td class="c57" colspan="2" rowspan="1"><p class="c26 c7"><span class="c25 c6">Agent features</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c26 c7"><span class="c25 c6">Example</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c26 c7"><span class="c25 c6">Source(s)</span></p></td></tr><tr class="c48"><td class="c32" colspan="1" rowspan="2"><p class="c26 c7"><span class="c1">AI Features</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Autonomy</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Interacting with an AI with high perceived autonomy leads to a spillover of negative attitudes (e.g., higher perceived threat) towards AIs in general. </span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s12369-020-00672-7&amp;sa=D&amp;source=editors&amp;ust=1684459462396528&amp;usg=AOvVaw3dowJ84d5ipJwcKoZPlBt0">Zafari &amp; Koeszegi, 2020</a></span><span class="c1">;</span></p><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S1071581916301768&amp;sa=D&amp;source=editors&amp;ust=1684459462396969&amp;usg=AOvVaw00NJ4xzq7dL57sAaE0-2LP">Zlotowski, Yogeeswaran &amp; Bartneck, 2017</a></span></p></td></tr><tr class="c36"><td class="c17" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Use and Ease of Use</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Positive attitudes towards a service robot that is considered useful, enjoyable, and easy to operate spill over to robots in general. </span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.jbe-platform.com/content/journals/10.1075/is.12.3.04shi&amp;sa=D&amp;source=editors&amp;ust=1684459462397947&amp;usg=AOvVaw2UOS2Cx6R7kLw9SBpQEhJB">Shin &amp; Choo, 2011</a></span><span class="c1">;</span></p><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0378720618308437&amp;sa=D&amp;source=editors&amp;ust=1684459462398267&amp;usg=AOvVaw2HdoF6rVkojTwrttE5rnGZ">Turje et al., 2020</a></span><span class="c1">;</span></p><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s12369-013-0220-0&amp;sa=D&amp;source=editors&amp;ust=1684459462398589&amp;usg=AOvVaw0FTtAyXugxBpUpCYH2G5Ch">Smarr et al., 2014</a></span></p></td></tr><tr class="c50"><td class="c32" colspan="1" rowspan="2"><p class="c26 c7"><span class="c1">Human Features</span></p><p class="c26 c7"><span class="c1">&nbsp;</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c7 c26"><span class="c1">Cultural Attitudes Towards Technology </span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Culture might influence positive or negative perceptions of AIs based on a country&rsquo;s history, economy, and policies toward technology. For example, Chinese individuals tend to have more negative perceptions towards robots, whereas US individuals perceive robots as less threatening than humans. This could mean that the spillover of moral consideration towards AIs is dampened by cultural values in &nbsp;China but facilitated by cultural values in the US.</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s00146-008-0181-2&amp;sa=D&amp;source=editors&amp;ust=1684459462399746&amp;usg=AOvVaw1odakDHuSrEzllj7SzHlU4">MacDroman, Vasudevan &amp; Ho, 2009</a></span><span class="c1">;</span></p><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/5453165&amp;sa=D&amp;source=editors&amp;ust=1684459462400108&amp;usg=AOvVaw3lK5uIDlS_1SWp836sJbnc">Wang et al., 2010</a></span></p></td></tr><tr class="c54"><td class="c17" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Experience/Technological Self-Efficacy</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c26 c7"><span class="c1">Having a technology or engineering degree, prior exposure to robots, and higher self-efficacy with technology increases trust and moral consideration of AIs. This could facilitate moral spillover if moral consideration is transferred to AIs more readily for people with experience and self-efficacy with technology.</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.frontiersin.org/articles/10.3389/fpsyg.2020.568256/full&amp;sa=D&amp;source=editors&amp;ust=1684459462401027&amp;usg=AOvVaw2Ts7yNlcrxiJ2tPMYZdQp0">Oksanen et al., 2020</a></span><span class="c1">;</span></p><p class="c26 c7"><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0747563222001947%23!&amp;sa=D&amp;source=editors&amp;ust=1684459462401342&amp;usg=AOvVaw1M1nUs6Ae25_EmnOieN0c-">Pauketat &amp; Anthis, 2022</a></span></p></td></tr></table><p class="c7 c55"><span class="c1">&nbsp;</span></p><p class="c24 c7 c21"><span class="c1"></span></p><h1 class="c45 c7" id="h.bdv2dry64m6k"><span class="c2">What is the difference between the spillover of actions and intentions? </span></h1><p class="c5 c7"><span class="c8">The transfer of moral consideration could increase intentions to treat AIs morally, but this may not necessarily translate into action. For example</span><span>, </span><span class="c1">even if someone recognizes the importance of including sentient AIs in the moral circle, they may not act on it by voting for a ban on AI discrimination. There is no guarantee that AIs will be treated well, even if moral consideration transfers to them from other beings. In the short term, humans might not intervene to help a mistreated AI. In the long term, human societies might not implement legislative infrastructure that safeguards the well-being of sentient AIs.</span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">This phenomenon is known as the </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265&amp;sa=D&amp;source=editors&amp;ust=1684459462402148&amp;usg=AOvVaw2ysTamJCK6Ouig0YRa9C3y">intention-action gap</a></span><span class="c8">&nbsp;and has been extensively studied in the context of environmental behavior. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.nature.com/articles/s41893-019-0263-9&amp;sa=D&amp;source=editors&amp;ust=1684459462402355&amp;usg=AOvVaw2x-VU6g10I5FFlvjbT18LT">A recent meta-analysis</a></span><span class="c8">&nbsp;showed that a pro-environmental behavior spills over to the </span><span class="c8 c28">intention</span><span class="c8">&nbsp;to adopt similar behaviors but does not necessarily lead to action. For instance, taking shorter showers might spill over to an intention to start conserving electricity, but does not lead to turning off unused devices. In the context of human-AI interaction, most studies have focused on the spillover of intentions rather than behaviors. For example, </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/5598679&amp;sa=D&amp;source=editors&amp;ust=1684459462402652&amp;usg=AOvVaw2XP1etJw0LDI9bDQuUt9G5">previous studies</a></span><span class="c8">&nbsp;have found that intentions to engage in prosocial interactions with all AIs increased after interacting with a single AI. However, positive attitudes do not necessarily</span><span class="c8">&nbsp;transfer to behavior or even </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0747563218304825&amp;sa=D&amp;source=editors&amp;ust=1684459462402894&amp;usg=AOvVaw3Usqg4z0p9rskYTMShry84">behavioral intentions</a></span><span class="c1">. People might not seek out interactions with AIs even if they feel positively towards them or intend to engage in prosocial interactions.</span></p><p class="c5 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8 c34"><a class="c4" href="https://www.google.com/url?q=https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/spc3.12265&amp;sa=D&amp;source=editors&amp;ust=1684459462403245&amp;usg=AOvVaw3D2av4PoEy7NKlvAFAZrP-">A synthesis of studies</a></span><span class="c8">&nbsp;suggested that the extent to which an action is in line with core beliefs (i.e., strong, long-held beliefs about oneself and the world) may underpin the intention-action gap. Intentions that align with core beliefs </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-0992%2528200007/08%252930%253A4%253C533%253A%253AAID-EJSP6%253E3.0.CO%253B2-F&amp;sa=D&amp;source=editors&amp;ust=1684459462403490&amp;usg=AOvVaw0lGBS32wWVc16JhBHFMihl">are more likely</a></span><span class="c8">&nbsp;to consistently translate into action compared to intentions that are motivated by other factors (e.g., the need to conform to group norms), which might lead to performative or temporary behaviors. In the environmental conservation literature, promoting core beliefs to protect the environment </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.nature.com/articles/s41893-019-0263-9&amp;sa=D&amp;source=editors&amp;ust=1684459462403717&amp;usg=AOvVaw0vcjr0zRn6jXqThOfeIj6l">has been shown</a></span><span class="c1">&nbsp;to increase the spillover between pro-environmental behaviors. Promoting core beliefs about the importance of the moral consideration of AIs is likely important to closing the intention-action gap so that AIs can benefit from the transfer of moral consideration. </span></p><h1 class="c45 c7" id="h.qc42eymkb64l"><span class="c2">What interventions can we use to induce the spillover of moral consideration?</span></h1><p class="c5 c7"><span class="c8">A common technique used in HRI research is to examine attitude change towards robots in general after interaction with a single AI, usually a robot. This technique builds on a rich literature of human intergroup contact interventions that have been shown to effectively promote the spillover of moral consideration and reduction of prejudice between humans.</span><sup class="c8 c41"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">HRI research has shown that human-AI contact might facilitate the transfer of moral consideration in the context of AIs. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/5598679&amp;sa=D&amp;source=editors&amp;ust=1684459462404470&amp;usg=AOvVaw0N_B0OxZk8yxCBNa5iWevC">Stafford et al. (2010)</a></span><span class="c8">&nbsp;found that prosocial contact with a robot increased positive attitudes toward the robot and towards all robots. </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://dl.acm.org/doi/abs/10.5555/3523760.3523851&amp;sa=D&amp;source=editors&amp;ust=1684459462404693&amp;usg=AOvVaw1jfbNYTiAdcMRU9X_bUuKA">More recently</a></span><span class="c8">, researchers showed that mutual self-disclosure (e.g., sharing personal life details) with a robot increased positive perceptions of all robots. Additionally, contact with a robot caregiver </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S138650562030304X&amp;sa=D&amp;source=editors&amp;ust=1684459462404917&amp;usg=AOvVaw3_MKF5xnFQJyjLkhTatPMj">increased acceptance</a></span><span class="c8">&nbsp;of technology and AIs in general, and social interaction with a robot </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.liebertpub.com/doi/10.1089/cyber.2020.0162&amp;sa=D&amp;source=editors&amp;ust=1684459462405151&amp;usg=AOvVaw0mV-XVw-0KbbKXPyWfiWVu">has been shown</a></span><span class="c8">&nbsp;to increase positive perceptions of robots, regardless of their features. Positive attitudes after contact with a human-like robot </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/7451847&amp;sa=D&amp;source=editors&amp;ust=1684459462405517&amp;usg=AOvVaw105IjkE-t1Oh57hpC1on9q">have been shown</a></span><span class="c8">&nbsp;to spill over to non-human-like robots. In-person interactions with a robot are not required to produce the spillover of positive attitudes and behaviors. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/6926300&amp;sa=D&amp;source=editors&amp;ust=1684459462405768&amp;usg=AOvVaw3PoZFoclF5XA2HO-VGl8Tn">Wullenkord and Eyssel (2014)</a></span><span class="c8">&nbsp;</span><span class="c1">demonstrated that imagining a prosocial interaction with a robot leads to positive attitudes and willingness to interact with other robots.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Another possible intervention to promote the transfer of moral consideration is </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/abs/pii/S0272494419306218?via%253Dihub&amp;sa=D&amp;source=editors&amp;ust=1684459462406142&amp;usg=AOvVaw1lOEm_Ycr9bYOA4Moq2dEp">changing underlying negative attitudes</a></span><span class="c8">&nbsp;towards AIs. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s12369-016-0357-8&amp;sa=D&amp;source=editors&amp;ust=1684459462406356&amp;usg=AOvVaw227igr8NquI6UB9pliGI4g">Some research</a></span><span class="c8">&nbsp;has shown that pre-existing negative attitudes towards AIs can persist even after a positive interaction with an AI, highlighting the significance of promoting positive attitudes towards AIs in order to facilitate the transfer of moral consideration. However, even if an intervention is successful in changing pre-existing attitudes towards AIs, the effective scope of this intervention might be limited to attitudes and intentions rather than behavior because of the intention-action gap.</span><span>&nbsp; &nbsp; &nbsp;</span><span class="c1">&nbsp;</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">A </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://compass.onlinelibrary.wiley.com/journal/17519004&amp;sa=D&amp;source=editors&amp;ust=1684459462406776&amp;usg=AOvVaw0f8T5xYf1WKXMmM14JSC-7">more effective intervention</a></span><span class="c8">&nbsp;might be to target core beliefs. As discussed previously, such beliefs are more likely to overcome the intention-action gap and translate into behavior. </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://psycnet.apa.org/record/2000-00512-004&amp;sa=D&amp;source=editors&amp;ust=1684459462407004&amp;usg=AOvVaw2GjAKcfn3L9gmOXav5hWlm">Sheeran and Orbell (2000)</a></span><span class="c1">&nbsp;showed that individuals for whom exercising was part of their self-identity were better at translating intentions to exercise into action than &lsquo;non-exercisers&rsquo;. In the context of AIs, promoting the self-perception of being someone who cares about the well-being of all sentient beings might make it more likely for the transfer of moral consideration to produce positive behaviors towards sentient AIs. Likewise, holding a core belief that the moral consideration of AIs is important might improve the likelihood that moral consideration will transfer onto AIs.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Some interventions might be less effective in facilitating the transfer of moral consideration. Guilt interventions </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://www.nature.com/articles/s41893-019-0263-9&amp;sa=D&amp;source=editors&amp;ust=1684459462407359&amp;usg=AOvVaw3zB1hStvEQhVDVzEO5ticH">are ineffective</a></span><span class="c8">&nbsp;in producing spillover in the environmental domain, and may even backfire. Specifically, inducing guilt over failing to adopt a pro-environmental behavior decreased the likelihood of adopting other pro-environmental behaviors. Even though guilt increases initial intentions to perform a pro-environmental behavior, the </span><span class="c34 c8"><a class="c4" href="https://www.google.com/url?q=https://www.tandfonline.com/doi/abs/10.1080/13504622.2016.1250148?journalCode%3Dceer20&amp;sa=D&amp;source=editors&amp;ust=1684459462407603&amp;usg=AOvVaw0c_txURmHksxR4ko-sdDzg">feelings of guilt dissipate</a></span><span class="c1">&nbsp;after the first behavior has been performed and this undermines motivation to engage in similar future behaviors. There is currently no research on guilt interventions in the context of AIs, but the risk of backfiring seems high given these previous findings. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">Even though interventions designed around contact, pre-existing attitudes, and core beliefs might be effective in inducing the transfer of moral consideration, to date there is no evidence for a long-term change in the moral consideration of AIs. So far, interventions have focused on short-term behavioral and contextual spillover. It is unknown whether these interventions have long-lasting effects on the moral consideration of AIs. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Another limitation of existing spillover research in the context of AIs is that the interventions conducted so far have been small-scale (i.e., small samples with limited types of AIs), often focused on non-moral</span><span>&nbsp;</span><span class="c1">purposes (e.g., user experience), and disconnected from each other. Research on possible interventions with larger samples, for the purpose of studying moral spillover, and to track long-term effects (e.g., how moral consideration might transfer from current AIs to future AIs), would provide more insight into how spillover effects might facilitate or hinder the inclusion of AIs in the moral circle. </span></p><h1 class="c7 c39" id="h.jxrvu05vmabb"><span class="c2">What future research is needed?</span></h1><p class="c5 c7"><span class="c8">Research on moral spillover towards AIs is in its infancy. More empirical evidence is needed to understand how moral consideration may or may not</span><span>&nbsp;</span><span class="c1">transfer from humans to AIs and from existing AIs to future AIs. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Future research should investigate how and when positive and negative attitudes and behaviors transfer to AIs, and the consequences this might have for their inclusion in the moral circle. Prosocial interactions&mdash;</span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/7451847&amp;sa=D&amp;source=editors&amp;ust=1684459462408482&amp;usg=AOvVaw0OiECCOCU7pQTZt9R9GoAl">real</a></span><span class="c8">&nbsp;or </span><span class="c3"><a class="c4" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/6926300&amp;sa=D&amp;source=editors&amp;ust=1684459462408717&amp;usg=AOvVaw0q3JPZL8xfwBcqYrAuHzN8">imagined</a></span><span class="c8">&mdash;with individual robots have been shown to increase positive attitudes and behaviors towards similar and very different AIs. Developing this research in the moral domain with studies that examine precursors (e.g., pre-existing negative attitudes) to spillover could help us understand positive and negative attitude transfer and the potential backfiring effects of spillover interventions. This matters because interaction with AIs is likely to increase as they become more widespread in society. If</span><span>&nbsp;</span><span class="c1">positive interactions with existing AIs facilitate moral consideration for AIs in general, future AIs might have a better chance of being included in the moral circle.</span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">How future AIs will appear, think, feel, and behave is uncertain. They are likely to have diverse mental capacities, goals, and appearances. Future AIs could range from highly human-like robots to non-embodied sentient algorithms or minuscule cell-like AIs. The mental capacities of AIs are likely to vary on a spectrum (e.g., from minimally sentient to more sentient than humans). How these diverse future AIs will be affected by moral spillover is unknown. Future research could examine whether there is a minimal threshold of similarity with humans that AIs must meet for moral consideration to transfer. Future studies could also examine whether the inclusion of even one kind of AI in the moral circle spills over to all AIs regardless of their features. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c1">Furthermore, a neglected but important research direction is the examination of temporal spillover. A change in how AIs are treated in the present might shape the moral consideration of AIs in the future. One possible way of investigating temporal spillover would be to use longitudinal studies to examine how present attitudes and behaviors towards AIs affect attitudes and behaviors towards different future AIs. Further research on the effectiveness of interventions that change core beliefs towards AIs is likely also an important part of understanding temporal moral spillover. </span></p><p class="c24 c7 c21"><span class="c1"></span></p><p class="c5 c7"><span class="c8">Expanding the research on moral spillover in the context of AIs has the potential to identify boundaries that shape human-AI interaction and the moral consideration of present AIs. Future research is also likely to broaden our understanding of how the many diverse AIs of the future will be extended moral consideration. Facilitating the transfer of moral consideration to AIs may be</span><span>&nbsp;</span><span class="c8">critical</span><span>&nbsp;</span><span class="c1">to fostering a future society where the well-being of all sentient beings matters.</span></p><hr class="c46"><div><p class="c5 c7"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c16">&nbsp;</span><span class="c8 c15">See </span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://www.oah.org/tah/issues/2015/november/the-history-of-animal-protection-in-the-united-states/&amp;sa=D&amp;source=editors&amp;ust=1684459462409694&amp;usg=AOvVaw1nr0PKi7G1zkic8RSHZJa_">Davis (2015)</a></span><span class="c8 c15">&nbsp;and </span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://faunalytics.org/the-animal-rights-movement-history-and-facts-about-animal-rights/&amp;sa=D&amp;source=editors&amp;ust=1684459462409926&amp;usg=AOvVaw3B9233UwAdkgBfNE9ondLi">Orzechowski (2020)</a></span><span class="c18 c8 c14">&nbsp;for more on the histories of the animal rights and anti-slavery social movements.</span></p></div><div><p class="c5 c7"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c16">&nbsp;</span><span class="c8 c15">In humans and animals, sentience is usually defined as the capacity to have positive and negative experiences, such as happiness and suffering. However, we understand that sentience does not necessarily have to look the same in AIs. We outline how one might assess sentience in AIs in a </span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://www.sentienceinstitute.org/blog/assessing-sentience-in-artificial-entities&amp;sa=D&amp;source=editors&amp;ust=1684459462412459&amp;usg=AOvVaw2eMh2afxYZJUIq6p6JXpZB">previous post</a></span><span class="c18 c8 c14">. </span></p></div><div><p class="c5 c7"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c8 c15">&nbsp;We take the view that the well-being of an entity </span><span class="c8 c15"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Animal_Liberation_(book)&amp;sa=D&amp;source=editors&amp;ust=1684459462412751&amp;usg=AOvVaw2iTGBUTJrTsk-OplRqnpJn">is tied to</a></span><span class="c8 c15">&nbsp;judgments of </span><span class="c8 c15"><a class="c4" href="https://www.google.com/url?q=https://forum.effectivealtruism.org/topics/moral-patienthood&amp;sa=D&amp;source=editors&amp;ust=1684459462412957&amp;usg=AOvVaw3gxKAPV7jELai9Y1VABnnX">moral patiency</a></span><span class="c8 c15">, as opposed to </span><span class="c8 c15"><a class="c4" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Moral_agency&amp;sa=D&amp;source=editors&amp;ust=1684459462413168&amp;usg=AOvVaw0RuRMkpkHgqG9udWGk8caP">moral agency</a></span><span class="c8 c15">. Whether an AI is able to discern right from wrong, how the AI acts or how the AI is programmed </span><span class="c8 c15"><a class="c4" href="https://www.google.com/url?q=https://link.springer.com/article/10.1007/s10676-020-09540-4&amp;sa=D&amp;source=editors&amp;ust=1684459462413377&amp;usg=AOvVaw1NvTxe1D80QWJ9HFgMUsir">does not necessarily change</a></span><span class="c18 c8 c14">&nbsp;whether or not they should be considered a moral patient. That is, even if an AI who does not care about moral treatment is developed, we still ought to treat them morally on the basis that they are a moral patient.</span></p></div><div><p class="c5 c7"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c16">&nbsp;</span><span class="c8 c15">Moral spillover can also occur for the opposite of moral consideration (e.g., actively wishing harm upon someone). Negative moral spillover has been referred to as </span><span class="c8 c15 c28">moral taint</span><span class="c18 c8 c14">.</span></p></div><div><p class="c5 c7"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c8 c14">&nbsp;</span><span class="c8 c15">From a psychological perspective, t</span><span class="c8 c14">he term &ldquo;</span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://dictionary.apa.org/prosocial&amp;sa=D&amp;source=editors&amp;ust=1684459462410319&amp;usg=AOvVaw05PoD-kdh9ebhceCFu9DdR">prosocial</a></span><span class="c8 c14">&rdquo; refers to a behavior that benefits one or more other beings (e.g., offering one&rsquo;s seat to an</span><span class="c8 c15">&nbsp;older</span><span class="c8 c14">&nbsp;person on a bus).</span><span class="c8 c15">&nbsp;T</span><span class="c8 c14">he term &ldquo;</span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://dictionary.apa.org/moral&amp;sa=D&amp;source=editors&amp;ust=1684459462410769&amp;usg=AOvVaw0CEvOJ3i7cozH5Pwt_5P3c">moral</a></span><span class="c8 c14">&rdquo; refers to a behavior that is ethical or proper (e.g., right or wrong). </span><span class="c8 c15">These terms can overlap</span><span class="c8 c14">. A prosocial behavior can in some cases also be a moral behavior (an</span><span class="c8 c15">d vice versa)</span><span class="c8 c14">, insofar as both kinds of actions promote the interests of other beings and can be construed as right or wrong. Given that </span><span class="c8 c15">much </span><span class="c18 c8 c14">of the moral spillover research in HRI is framed as the study of prosocial behavior, we use the terms &ldquo;moral&rdquo; and &ldquo;prosocial&rdquo; interchangeably.</span></p></div><div><p class="c5 c7"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c8 c14">&nbsp;</span><span class="c8 c15">T</span><span class="c8 c14">emporal spillover </span><span class="c8 c15">could include a huge range of attitudes and behavior, such as simply having one attitude persist over time (e.g., after seeing a compelling fundraiser for a charity, you still feel compelled two weeks later). A narrower definition of spillover would exclude situations where moral consideration merely transfers to the same individual or group at a different time, rather than to different entities</span><span class="c8 c14">.</span></p></div><div><p class="c5 c7"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c8 c15">&nbsp;See </span><span class="c34 c8 c15"><a class="c4" href="https://www.google.com/url?q=https://spssi.onlinelibrary.wiley.com/doi/full/10.1111/josi.12419&amp;sa=D&amp;source=editors&amp;ust=1684459462411664&amp;usg=AOvVaw3DDRgUsr_kWhZPbUO7zWY7">Boin et al. (2021)</a></span><span class="c8 c15">&nbsp;and </span><span class="c3 c15"><a class="c4" href="https://www.google.com/url?q=https://www.annualreviews.org/doi/abs/10.1146/annurev-psych-071620-030619&amp;sa=D&amp;source=editors&amp;ust=1684459462411886&amp;usg=AOvVaw2pAZifPQqs3jZ58jFvbTQF">Paluck et al. (2021)</a></span><span class="c18 c8 c14">&nbsp;for recent reviews of the effectiveness of contact interventions.</span></p></div></body></html>
