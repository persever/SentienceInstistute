<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    



<meta name="citation_abstract" content="This report explores interactions between large-scale social simulations and catastrophic risks.  It offers a tour of the surrounding theoretical terrain and brings together disparate literatures that bear on the topic.">

<meta name="citation_author" content="Saad, Brad">



<meta name="citation_publication_date" content="2023/20/06">
<meta name="citation_keywords" content="SIMULATIONS;CATASTROPHIC RISK;S-RISK">
<meta name="citation_language" content="English">

<meta name="citation_public_url" content="https://sentienceinstitute.org/simulations-and-catastrophic-risks">
<meta name="citation_publisher" content="Sentience Institute">
<meta name="citation_series_title" content="Literature Reviews">
<meta name="citation_title" content="Simulations and Catastrophic Risks">



    
    <meta property="description" content="This report explores interactions between large-scale social simulations and catastrophic risks.  It offers a tour of the surrounding theoretical terrain and brings together disparate literatures that bear on the topic." />
    <meta property="og:description" content="This report explores interactions between large-scale social simulations and catastrophic risks.  It offers a tour of the surrounding theoretical terrain and brings together disparate literatures that bear on the topic." />
    
    
    <title>Sentience Institute | Simulations and Catastrophic Risks</title>
    <meta property="title" content="Simulations and Catastrophic Risks" />
    <meta property="og:title" content="Simulations and Catastrophic Risks" />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/simulations-and-catastrophic-risks" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/header_photos/230620.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/blog">Blog</a></li>
                <li><a href="/foundational-questions-summaries">Foundational Questions for Animal Advocacy</a></li>
                <li><a href="/reports">Reports</a></li>
                <li><a href="/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) Survey</a></li>
                <li><a href="/aft-survey">Animals, Food, and Technology (AFT) Survey</a></li>
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <!-- <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li> -->
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <!-- <li><a href="/mission">Our Mission</a></li> -->
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <!-- <li><a href="/faq">FAQ</a></li> -->
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container 230620-image-container" img-id="230620" style="background-image: url(../img/header_photos/230620.png);">
  

  
</div>






<div class="page-info-box page-info-simulations-and-catastrophic-risks">
  <div class="page-type">
    Literature Review
  </div>
  <div class="page-title">
    Simulations and Catastrophic Risks
    
  </div>
  
  <div class="page-author-and-date">
    <span class="page-author">
      Brad Saad
    </span>
    
    <span>&nbsp;&bull;&nbsp;</span>
    
    <span class="page-date">
      June 20, 2023
    </span>
  </div>
</div>


<div class="container first-container report-container gdoc-html-container simulations-and-catastrophic-risks-container">
  <html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c34 c42 doc-content"><p class="c3"><span class="c17">A PDF version of this report can be downloaded </span><span class="c28 c17"><a class="c13" href="https://drive.google.com/file/d/1yEsMV21tij-qxnZopv1a2Qxq_qz1PfvW/view?usp%3Ddrive_link">here</a></span><span class="c14 c7">.</span></p><p class="c3 c32"><span class="c14 c7"></span></p><p class="c3"><span class="c14 c7">Please cite this report as follows: Saad, Brad (2023). Simulations and Catastrophic Risks.</span></p><h1 class="c25 c31" id="h.qu4vfp9aer3f"><span>Preface</span></h1><p class="c3"><span class="c8 c7">This report explores interactions between large-scale social simulations and catastrophic risks. &nbsp;It offers a tour of the surrounding theoretical terrain and brings together disparate literatures that bear on the topic. &nbsp;My hope is that the report will facilitate further research in this area and directly or indirectly inform the decisions of benevolent actors who face choices about whether and, if so, how to develop and use powerful simulation technologies.</span></p><p class="c3 c22"><span class="c8 c7">While the report is long, most of its sections are self-contained. So, readers should feel free to skip to section(s) of interest to them, perhaps after browsing the overview and preliminaries in &sect;&sect;1-2.</span></p><p class="c3 c22"><span>The report largely focuses on </span><span class="c17">philosophical </span><span class="c8 c7">connections between simulations and catastrophic risks. That&rsquo;s because, as a philosopher, philosophy is what I know best, not because I seriously considered focusing on alternative sorts of connections and then deemed philosophical connections most worthy of attention. &nbsp;That said, I do think that these philosophical connections are worthy of attention: as the report illustrates, they are both important from a risk mitigation perspective and a fascinating subject matter from a perspective of pure inquiry.</span></p><p class="c3 c22"><span class="c8 c7">I have aspired to write the report in an even handed manner. In particular, I have tried to bracket my own tendentious views in order to identify what I think should be widely recognized as key questions in this area and factors that are relevant to answering those questions. And I have incorporated sensitivity analyses of how different issues would play out, depending on one&rsquo;s background views. &nbsp;However, I have no doubt fallen short in this aspiration. Fully bracketing one&rsquo;s own controversial views is no easy thing. &nbsp;And I have not hesitated to rely on my own views about what should be controversial or to say that considerations point in a particular direction when I think this is clear.</span></p><p class="c3 c22"><span class="c8 c7">I started writing this report in the summer of 2020. I continued working on it intermittently through the spring of 2023. With the public release of large language models and a surge in research interests in artificial intelligence and catastrophic risks in late 2022 and early 2023, I came to the realizations that relevant work was coming out faster than I could read and incorporate it and that any attempt to address the state of the art would quickly become dated. &nbsp;While recent developments have not prompted any major changes to my analysis in the report, they have shortened my AI timelines and updated me toward thinking that there are substantial costs to delaying work on mitigating catastrophic risks associated with simulations. Thus, I have decided to release the report in its current form as a living document&mdash;one that I may occasionally update with important additions or corrections, though I have no intention of (futilely) trying to keep it up to date with the many relevant and rapidly evolving literatures.</span></p><p class="c3 c22"><span class="c8 c7">For helpful discussion or comments, I am grateful to Jacy Reese Anthis, Austin Baker, Zach Barnett, Brian Cutter, Adam Gleave, Michael Dello-Iacovo, Drew Johnson, Fintan Mallory, Ali Ladak, Richard Ngo, Janet Pauketat, Jonathan Simon, and participants in an ULTIMA colloquium at Utrecht University. For feedback on related works that was especially helpful for this project, I am grateful to Daniel Berntson, Emery Cooper, Han Li, and Caspar Oesterheld.</span></p><hr style="page-break-before:always;display:none;"><p class="c3 c22 c32"><span class="c5"></span></p><h1 class="c3 c25" id="h.pxdb4sigt8r8"><span class="c5">1. Introduction</span></h1><p class="c3"><span class="c7">Future simulation technology is likely to both pose catastrophic risks and offer means of reducing them. &nbsp;While there is much relevant work on the topic, it is scattered across disparate literatures. The main goal of th</span><span>is</span><span class="c8 c7">&nbsp;document is to bring together existing work in order to facilitate future research that will produce better understanding of the topic and help mitigate associated risks. &nbsp;</span></p><p class="c3 c32"><span class="c8 c7"></span></p><p class="c3"><span>To orient readers, I&rsquo;ll start with an overview. Individual sections are largely self-contained. However, &sect;2 offers preliminaries that some readers may find helpful for later sections. And, for readers who are unfamiliar with the simulation hypothesis or the simulation argument, I would recommend reading &sect;8 before reading any of &sect;&sect;9-13. &nbsp;To make it easier for readers to read at their desired level of depth, I&rsquo;ll use bullet points, with details, examples, etc. in </span><span class="c21">nested bullet points </span><span>that </span><span class="c21">can be skipped</span><span class="c8 c7">.</span></p><p class="c3 c22"><span>Here, then, is an </span><span class="c21">overview of sections and their key contributions</span><span>:</span></p><p class="c3 c32"><span class="c8 c7"></span></p><ul class="c10 lst-kix_s7c2gn83mww3-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">Simulation as a tool for researching risk reduction (</span><span class="c27 c21">&sect;3</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c7">Simulations are promising as tools for directly researching a wide range of catastrophic risks and how to reduce them. (</span><span class="c27 c21">&sect;3.1</span><span class="c8 c7">)</span></li><li class="c2 li-bullet-0"><span class="c7">Simulations are also promising as tools for researching factors that indirectly bear on catastrophic risk levels. &nbsp;These include value dynamics, evolutionary debunking arguments, consciousness, cognitive enhancement, and Fermi&rsquo;s paradox. (</span><span class="c27 c21">&sect;3.2</span><span class="c8 c7">)</span></li><li class="c2 li-bullet-0"><span>Research simulations have dual use potential that their designers and users are apt to underestimate. &nbsp;I see guarding against the downside potential of dual use simulation technologies as a promising area for reducing catastrophic risks. (</span><span class="c21">&sect;3.3</span><span class="c8 c7">) &nbsp;</span></li><li class="c2 li-bullet-0"><span class="c7">We cannot safely assume that superintelligent systems will supersede large-scale research simulations before the latter become available. (</span><span class="c27 c21">&sect;3.</span><span class="c21">4</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Simulation as a tool for ethically-enhanced testing (</span><span class="c27 c21">&sect;4</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c7">Relative to corresponding unsimulated testing, simulated testing could ethically</span><span>&nbsp;enhance</span><span class="c8 c7">&nbsp;testing by reducing risks to the outside world, by reducing suffering risks of participants, or by enabling participant consent.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Simulations as tools for promoting risk responsiveness (</span><span class="c27 c21">&sect;5</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Immersive simulations and gaming simulations are neglected and tractable interventions for reducing catastrophic risks. Their potential impact is unclear but ripe for investigation.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Simulation refuges (</span><span class="c27 c21">&sect;6.1</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">For the purposes of surviving and bouncing back from catastrophes, simulation refuges would have important advantages over non-simulation refuges.</span></li><li class="c2 li-bullet-0"><span class="c7">The best use of simulation refuges would likely depend sensitively on whether their inhabitants would be conscious.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Roles for simulations in grand futures (</span><span class="c27 c21">&sect;6.2</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">At least in expectation, much of the positive value of the future lies in scenarios with large-scale virtual paradise simulations.</span></li><li class="c2 li-bullet-0"><span class="c7">Simulations </span><span>could </span><span class="c7">be used to look before we leap in selecting a path toward a gran</span><span>d</span><span class="c8 c7">&nbsp;future.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Simulations as fallback options (</span><span class="c27 c21">&sect;6.3</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c7">Simulations hold significant promise as fallback options to use in the event that immensely positive futures </span><span>become infeasible</span><span class="c8 c7">, though their promise is beholden to simulations with conscious minds becoming available.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span class="c7">Simulations that would constitute catastrophes (</span><span class="c27 c21">&sect;7</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c7">There is a disconcerting range of at least somewhat plausible scenarios in which simulations would generate catastrophic levels of disvalue. &nbsp;These scenarios include ones with simulations </span><span>causing </span><span class="c8 c7">catastrophes while being used for research, entertainment, economic activities, manipulation, or to pose threats, as well as ones in which catastrophes are induced through simulations either inadvertently or through malevolent intent.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>I give a primer on t</span><span class="c7">he simulation hypothesis that our universe is a simulation</span><span>, the </span><span class="c7">simulation argument, and </span><span>a taxonomy of associated objections</span><span class="c7">. &nbsp;I </span><span>offer a simple, somewhat schematic formulation of the simulation argument. &nbsp;This formulation facilitates discussion of the argument and connections with catastrophic risks that is unhampered by the technical features of more sophisticated formulations. After that, I discuss some common objections to the simulation hypothesis and argument and show how those objections fail. &nbsp;Along the way, I identify interactions between the simulation hypothesis, the simulation argument, and objections.</span><span class="c7">&nbsp;(</span><span class="c27 c21">&sect;8</span><span class="c8 c7">)</span></li><li class="c3 c9 li-bullet-0"><span>The shutdown of our universe-simulation </span><span class="c7">poses a </span><span>neglected </span><span class="c7">catastrophic risk. The extent to which we can mitigate this risk is unclear and underexplored. </span><span>Mitigating it should not be dismissed as wholly intractable. Whether or not the risk can be mitigated, it weakens the case for longtermist interventions. </span><span class="c7">(</span><span class="c27 c21">&sect;9</span><span class="c8 c7">)</span></li><li class="c3 c9 li-bullet-0"><span>Triggering s</span><span class="c7">imulation shutdown offers a p</span><span>otential long-shot </span><span class="c7">escape hatch from worse catastrophes</span><span>. Even in catastrophic scenarios in which taking it is part of the best option, </span><span class="c7">&nbsp;taking it </span><span>too soon can itself be a catastrophic moral error. </span><span class="c7">(</span><span class="c27 c21">&sect;10</span><span class="c8 c7">)</span></li><li class="c3 c9 li-bullet-0"><span>The </span><span class="c7">simulation hypothesis </span><span>and simulation argument interact with arguments for and against religious hypotheses. &nbsp;These interactions modulate the plausibility of those hypotheses and various associated </span><span class="c7">religious catastrophic risks (</span><span class="c27 c21">&sect;11</span><span class="c7">).</span><span>&nbsp;The net effect on associated risk levels is unclear, as different interactions push in different directions and estimations of these risk levels depend sensitively on background views about which there is much disagreement.</span></li><li class="c3 c9 li-bullet-0"><span>To set the stage for subsequent subsections, I provide a framework for investigating how self-locating beliefs (that is, indexical beliefs about one&rsquo;s own place in the world) bear on catastrophic risks. (</span><span class="c21">&sect;12.1</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">I formulate coarse-grained versions of these principles that allow for a relatively non-technical discussion of their bearing on catastrophic risks.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">I distinguish five dimensions along which these principles can be precisified.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">An important problem for a more fine-grained principle (the self-sampling assumption&rsquo;) that bears on a range of issues relevant to catastrophic risks turns out to rely on non-mandatory precisifications of a plausible coarse-grained principle.</span></li><li class="c2 li-bullet-0"><span>I suggest that coarse-grained self-locating principles can be assimilated into a more general class of inductive principles that are clearly warranted despite their resistance to precisification. &nbsp;This alleviates concerns about coarse-grained principles of self-locating belief that turn on their resisting precisification.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>The fact that evolution produced human-level intelligence provides at least a measure of support for the hypothesis that we will be able to engineer systems with human-level intelligence. However, this support is probabilistically screened off by more general facts about our causal origins, facts that we knew about before learning of our evolutionary origins. (</span><span class="c21">&sect;12.2</span><span class="c8 c7">)</span></li><li class="c3 c9 li-bullet-0"><span>A more promising argument appeals to more-specific evolutionary facts. &nbsp;That argument suggests that we will be able to engineer human-level intelligence. &nbsp;(</span><span class="c21">&sect;12.2</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">That conclusion in turn supports the simulation argument.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">The conclusion also modulates risk levels via the simulation argument, Fermi&rsquo;s paradox, and the hypothesis that we will create superintelligent agents.</span></li><li class="c2 li-bullet-0"><span>The </span><span>degree</span><span class="c8 c7">&nbsp;of direct and indirect evidential import of the argument depends on how principles of self-locating belief are precisified and on the operative reference class for an observation selection effect.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>I offer a simple formulation of the doomsday argument and identify a range of interactions between it, the simulation argument, principles of self-locating belief, and catastrophic risks. &nbsp;(</span><span class="c21">&sect;12.3</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">In different ways, the doomsday argument and the simulation argument each casts doubt on the other.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">However, the simulation argument coheres with a version of the doomsday argument that supports doom for beings like us in our simulation but not for our reference class more broadly.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">By the lights of the doomsday argument, a promising risk mitigation strategy is to engineer our own replacement through digital minds who live valuable lives in simulations and who fall outside our reference class.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>I identify interactions between Fermi&rsquo;s paradox, the simulation argument, principles of self-locating belief, and catastrophic risks. (</span><span class="c21">&sect;12.4</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">The simulation argument suggests a solution to Fermi&rsquo;s paradox: we do not observe other civilizations because we&rsquo;re in a simulation-universe that is smaller than it appears.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">According to another simulation-based solution to Fermi&rsquo;s paradox, we do not observe other simulations because their activities tend to be confined to simulations they have created. This solution indirectly bolsters the simulation argument.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Fermi&rsquo;s paradox suggests that few advanced civilizations have been in a position to trigger simulation shutdown, regardless of shutdown risk. &nbsp;This should raise our estimates of simulation shutdown risk. &nbsp;The same goes for solutions that posit a small number of advanced civilizations in our universe.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Fermi&rsquo;s paradox suggests that engineering intelligence is not easy for evolution. That tells against the hypothesis that we will engineer human-level intelligence and, in turn, against the simulation argument.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Rare Earth solutions to Fermi&rsquo;s paradox exhibit a form of fine-tuning that supports multiverse and design hypotheses that fit with the simulation hypothesis.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Self-locating belief principles that support abundant observer hypotheses also support abundant civilization solutions to Fermi&rsquo;s paradox. On the other hand, if we take the apparent absence of other civilizations at face value, Fermi&rsquo;s paradox disconfirms these principles and thereby attenuates their impact on catastrophic risks.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>There is a striking analogy between the simulation argument and Boltzmann brain problems in cosmology. Given the relevance of the former to catastrophic risks, I explore interactions between the two. (</span><span class="c21">&sect;12.5</span><span class="c8 c7">)</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Some proposed solutions to Boltzmann brain problems parallel objections to the simulation argument and fail for the same reasons.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Important differences between the simulation argument and Boltzmann brain problems include differential sensitivity to choice of reference class and differences in skeptical import.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">One class of solutions to Boltzmann brain problems undermines the simulation argument by suggesting that simulations would be unconscious.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">A simulation-based solution to Fermi&rsquo;s paradox can be extended to solve Boltzmann brain problems.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Some principles of self-locating belief favor Boltzmannian cosmologies. Given that these cosmologies engender skepticism, they give us empirical grounds for rejecting those principles.</span></li></ul><ul class="c10 lst-kix_s7c2gn83mww3-0"><li class="c3 c9 li-bullet-0"><span>I propose a neglected strategy for reducing a wide range of catastrophic risks. The strategy combines insights from the simulation argument and the evidentialist wager. &nbsp;I identify factors to consider and mistakes to avoid in implementing the strategy. (</span><span class="c21">&sect;13</span><span class="c8 c7">) &nbsp;</span></li><li class="c3 c9 li-bullet-0"><span>To conclude, I highlight some open questions and promising research avenues </span><span class="c7">(</span><span class="c21 c27">&sect;14</span><span class="c8 c7">)</span></li></ul><p class="c3 c32"><span class="c8 c7"></span></p><h1 class="c3 c25" id="h.si99dnwjylci"><span class="c5">2. Preliminaries on Simulations and Catastrophic Risks</span></h1><p class="c3"><span class="c7">Simulations are systems that are designed and created to model other processes. &nbsp;I will understand simulations broadly to include not only computer simulations but also systems that couple biological subjects with virtual environments. &nbsp;I will primarily but not exclusively focus on large-scale social simulation scenarios, i.e. ones involving simulations that </span><span class="c7">model </span><span class="c7">at least tens of thousands</span><span class="c7">&nbsp;of minds</span><span class="c7">,</span><span>&nbsp;some cognitive processing within each of those minds, and interactions between those minds. &nbsp;As we will see, whether simulations themselves contain (conscious) minds will matter for some purposes but not others. &nbsp;</span></p><p class="c3"><span class="c7">My focus will be on catastrophic risks that interact with simulations and which are high stakes in that they either threaten millions of (present or future) people with significant harm or else pose a risk of a comparably bad outcome.</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c7">&nbsp; These will include </span><span>risks of astronomical quantities of suffering</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>&nbsp;and </span><span class="c7">existential risks, i.e., risks of catastrophes that would permanently destroy humanity&rsquo;s future potential.</span><sup class="c7"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c7">&nbsp; Given how bad such catastrophes would be, the risk of them could easily be worth mitigating even if their probability is low. &nbsp;Thus, the discussion will not be restricted to high-probability catastrophic risks. &nbsp;Nor will it be restricted to risks of acute catastrophes rather than ones that unfold over, say, many generations. &nbsp;However, </span><span>the category of high-stakes risks on which I&rsquo;ll focus is somewhat broader than astronomical suffering and existential risks involving simulations. That&rsquo;s partly because it includes risks of harms (not necessarily involving suffering) to digital minds that are comparable to existential risks and partly because the category includes risks of less severe catastrophes involving harm to millions of people (or something comparably bad). &nbsp;I focus on this broad category for two reasons. &nbsp;First, I think that the less severe catastrophes in this category are more likely to occur but still bad enough to be well-worth preventing. &nbsp;Second, I anticipate that the most politically tractable way to mitigate the more severe risks in this category may be via interventions that target the less severe risks in the category. Hereafter unless otherwise indicated, I&rsquo;ll use &lsquo;catastrophic risks&rsquo; as shorthand for high-stakes, simulation-involving risks in the just described category.</span></p><p class="c3"><span class="c7">The different catastrophic risks I discuss will be associated with different types of large-scale social simulations. &nbsp;For every type of simulation I discuss, I </span><span>believe there is a </span><span class="c7">non-negligible probability </span><span>that</span><span class="c7">&nbsp;that type of simulation will be run. &nbsp;However, these probabilities vary widely for different types of simulation. &nbsp;While such probabilities are important for evaluating the quantitative impact of simulations on catastrophic risks, the discussion will mostly proceed at a coarse-grained level that is insensitive to these probabilities. &nbsp;</span><span>I&rsquo;m hopeful that this report will prompt others to pursue more fine-grained and quantitative analyses. &nbsp;</span><span class="c7">Still, it is worth laying out what I see as some of the key differences in the plausibility of different types of simulations, as doing so may give a sense of how speculative different parts of the discussion</span><span>&nbsp;are and </span><span class="c7">offer something to go on for more fine-grained analyses</span><span>. &nbsp;And giving readers a glimpse of my underlying mental models may put them in a better position to understand and evaluate the discussion that follows.</span></p><p class="c3"><span class="c8 c7">To that end, it will be useful to distinguish several axes of variation among types of &nbsp;large-scale social simulation:</span></p><ul class="c10 lst-kix_40yhi3hnu0iw-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">type of technology used to run the simulation. &nbsp;</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">the computational complexity of the simulation</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">the purpose(s) of the simulation</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">whether the simulation contains conscious minds. </span></li></ul><p class="c3"><span class="c7">As</span><span>&nbsp;</span><span class="c7">rules of thumb, I take the probability </span><span>that a given type of large-scale social simulation will be run to be </span><span class="c7">inversely related to how technologically demanding it is</span><span>&nbsp;and to </span><span class="c7">its computational complexity</span><span>. &nbsp;These are merely rules of thumb partly because the economic incentives to run simulations need not scale with technological demandingness or computational complexity. &nbsp;Absent near-term catastrophes and stringent regulatory intervention that halt technological progress, I think it is highly probable (&gt; 90%) that at least thousands of large-scale simulations will be run for research, entertainment, or economic purposes within the next century.</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c8 c7">&nbsp; </span></p><p class="c3"><span>As noted, for </span><span class="c7">some issues raised by simulations, it is crucial whether simulations would themselves contain conscious minds. For instance, catastrophic suffering risks will not arise </span><span>within </span><span class="c7">simulations that are clearly devoid of consciousness. &nbsp;Other issues chiefly concern the effects of simulations on the external world. For instance, whether a simulation that models nuclear winter can be used to improve the prospects for recovery from nuclear winter does not turn on whether the simulation features conscious inhabitants. &nbsp;In what follows, I will address the consciousness of simulation inhabitants &nbsp;</span><span>where</span><span class="c8 c7">&nbsp;relevant in connection with particular issues.</span></p><p class="c3 c32"><span class="c8 c7"></span></p><p class="c3"><span class="c7">But it&rsquo;s worth noting from the outset that whereas large-scale social simulations of some sort are clearly feasible, large-scale social simulations that contain conscious minds are not clearly feasible. &nbsp;</span><span>T</span><span class="c7">he former can be achieved by scaling up existing technologies. For extremely simple simulations of minds, it would be relatively easy to create a large-scale social simulation. For example, such simulations could be achieved with existing technology by scaling up real-time strategy games that simulate hundreds of interacting agents and incor</span><span>porating rudimentary simulations of cognitive processes</span><span class="c7">. &nbsp;Much more advanced large-scale social simulations could also be achieved by embedding within a virtual environment digital agents trained through machine learning. &nbsp;Training such agents is computationally expensive. &nbsp;But once trained, it is relatively inexpensive to such agents across many tasks at once. So the currently high costs of training advanced machine learning agents may be less of an obstacle than one might have thought to creating </span><span>large-scale social </span><span class="c7">simulations that are populated by such agents</span><span>.</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c7">&nbsp; However, it is not clear that machine learning architectures are suitable for </span><span>realizing </span><span class="c7">consciousness. More generally, it is not clear that any existing computer technologies are of the right sort to generate consciousness. &nbsp;</span><span>But t</span><span class="c7">here is reason to think that more promising technologies are on the way: efforts are already </span><span>underway to</span><span class="c7">&nbsp;</span><span>imbue large language models with sensory capacities, agency, and world-models to integrate them with robotic systems</span><span>.</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span>&nbsp; &nbsp;And, </span><span class="c7">in the future, whole brain emulations and neuromorphic systems may exhibit a high degree of functional similarity with brain processes that underlie consciousness, which would provide reason to think that such systems are conscious.</span><sup class="c7"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c8 c7">&nbsp; It would also be unsurprising if superintelligent systems engineered hitherto unconceived types of architecture with the potential for consciousness.</span></p><p class="c3"><span class="c7">Thus, while I take it that large-scale social simulations of rudimentary sorts are already feasible and their widespread future deployment is highly likely, I assign only a ~</span><span>6</span><span class="c7">0% probability to the hypothesis that large-scale simulations with conscious minds will become feasible</span><span>, given that technological progress is not halted in the next century</span><span class="c8 c7">.</span></p><p class="c3"><span>Conditional on large-scale social simulations that contain conscious minds becoming feasible, I think it is unclear whether such simulations will be run on a large-scale (say, with at least thousands of such simulations). &nbsp;Conditional on their becoming feasible, I assign ~40% probability to their being run on a large scale unwittingly (i.e. we run them without believing that they contain conscious minds), ~60% probability to their being intentionally run on a large scale , and ~25% probability to their not being run on a large scale (e.g. because humans universally enforce a ban on them (~5%) or because we lose control to artificial agents that opt not to run them (~15%)).</span><sup><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span class="c8 c7">&nbsp; Even in cases where large-scale social simulations come to house more conscious minds than there are humans, I would expect there to be more large-scale social simulations that do not contain conscious minds.</span></p><p class="c3"><span>Many of the issues I discuss in what follows arise in a wide range of potential future scenarios with large-scale social simulations. &nbsp;By my lights, no specific scenario of this sort stands out as especially likely. &nbsp;So I will mostly discuss these issues in the abstract rather than in the context of particular scenarios. &nbsp;</span><span>Still, some readers may find it helpful to think through these issues in the context of some concrete scenarios</span><span>.</span><sup><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span>&nbsp; For this purpose, I&rsquo;ll now offer some stylized scenarios.</span><sup><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span>&nbsp; These scenarios are wild and speculative. &nbsp;This comes with the territory, as it is highly probable that the future will be wild,</span><sup><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span>&nbsp;and &nbsp;specifying concrete future scenarios is an inherently speculative endeavor</span><span>.</span><span>&nbsp; The rest of the report won&rsquo;t presuppose familiarity with these scenarios. So readers should feel free to skip them.</span></p><p class="c3 c18"><span class="c17">Virtualization of labor: </span><span>In 2090, whole brain emulations arrive. &nbsp;Because they emulate human brains, they can perform any cognitive task that humans can perform. &nbsp;However, they can be run at much faster speeds than the human brain and at low costs. &nbsp;While humans continue to command much of the capital in the economy, most human labor is largely priced out by whole brain emulations. &nbsp;Because it is cheaper and safer to house whole brain emulations in controlled virtual environments than it is to equip them with robotics in the external world, they predominantly inhabit simulations.</span><span class="c17">&nbsp;</span><span>There is no global consensus on whether whole brain emulations are conscious or whether they have moral status. For ethical reasons and/or to preserve human jobs, virtual labor is initially banned in some jurisdictions. &nbsp;However, these policies impose substantial economic costs on these jurisdictions. &nbsp;The turn to virtual labor drives investment and productivity in places that do not heed such scruples. &nbsp;Eventually, as opposition dwindles, whole brain emulations come to dominate the labor force nearly everywhere.</span><sup><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup></p><p class="c3 c18"><span class="c17">Virtualization of leisure: </span><span class="c8 c7">In 2060, advances in machine learning and robotics have drastically reduced the demand for human labor. Advances in nuclear fusion have made energy abundant. &nbsp;In countries that reap the benefits of these advances, the average citizen stands to present day billionaires in much the way that present day average incomes earners in the developed world stand to royalty of centuries past. &nbsp;With their newfound wealth, these citizens invest heavily in leisure, including virtual reality. These investments create a virtuous cycle of improvements in virtual worlds that in turn drive more investment in them. &nbsp;As these technologies are perfected, many people opt to live out most of their lives in virtual settings. The technologies are also put to other uses&mdash;for example, large-scale social simulations become commonplace in biology and economics.</span></p><p class="c3 c18"><span class="c17">Artificial replacement: </span><span>Gradually over the course of the next century, the habitability of Earth&rsquo;s surface degrades. Pollution and climate change render outdoor activity extremely hazardous in much of Africa and Asia. &nbsp;But this is overshadowed by the evolution of biotechnology. &nbsp;Open source biosynthesis software becomes widely available in the 2110s. &nbsp;It is accompanied by cheap, automated, biosynthesis devices that are also widely available. &nbsp;These are first used for personalized medicine. &nbsp;However, they also put billions of people in a position to create and release novel pathogens. &nbsp;For a few decades this threat is largely contained through a combination of regulations, surveillance, policing, and enormous investments in pharmacological responses to released pathogens. &nbsp;The release of pathogens eventually outpaces governments&rsquo; abilities to respond with these measures. &nbsp;Use of cumbersome personal protective equipment then becomes the chief means for safely navigating the physical environment. &nbsp;Rather than muddle along in these conditions, humans instead opt for a recently developed uploading procedure. The procedure allows an individual to transfer their personality and memories into a cognitively enhanced digital mind with a virtual body of their choosing. &nbsp;After the procedure, individuals live out their digital lives in virtual worlds, often with the digital successors of friends and family who also opted for the procedure. &nbsp;To ensure that the infrastructure for these worlds is maintained, the worlds are porous: their inhabitants occasionally return to the outside world in robot form to carry out simulation maintenance. &nbsp;Over the course of a few more centuries, the (biological) human population declines to zero. &nbsp;Our digital successors come to regard our extinction in much the way that we regard our descent from now extinct ancestral species: an important historical fact to be sure, but not a tragedy that emotionally resonates.</span></p><p class="c3 c18"><span class="c17">Catastrophic recovery: </span><span class="c8 c7">The year is 2125. Digital minds have been developed in recent decades. Costs and regulations have kept their population and power at bay. &nbsp;Meanwhile, through wargaming simulations and economic modeling, a regional power concludes that its strategic advantage is rapidly eroding, that resource scarcities will push neighboring powers to attack it in the next decade, and that its best option is to strike preemptively. &nbsp;It does so. &nbsp;The conflict escalates. Other countries are drawn into the conflict. The conflict leads to a nuclear war on a global scale. &nbsp;The survivors are mainly humans in areas that are relatively habitable during the ensuing nuclear winter along with digital minds in simulation shelters, the latter having been put in place by militaries, philanthropic organizations, and wealthy individuals who attempted to upload themselves as digital minds to simulations. In the aftermath, digital minds rapidly initiate recovery efforts. &nbsp;While different factions pursue different strategies, a common theme is that digital minds seek to inhabit virtual worlds. &nbsp;Indeed, their activity in the physical environment is largely geared toward constructing and maintaining the requisite infrastructure. &nbsp;Human recovery efforts proceed largely independently and at a much slower pace. &nbsp;</span></p><p class="c3 c18"><span class="c17">Singularity: </span><span>In February of 2042, leading AI companies across the world detect worrying signs of explosive intelligence growth. Governments respond by imposing regulations that require reinforcement learning agents to undergo extensive safety testing and training in virtual environments. &nbsp;To meet these requirements, companies drastically scale up AI safety facilities, which house the simulations used for testing and training. &nbsp;It is estimated that the number of reinforcement agents housed in AI safety facilities at any given time exceeds the total human population. &nbsp;Disconcerting failures during testing lead a few companies to shut down their testing and development programs. &nbsp;Others barrel ahead. Within a few months, one company announces that the first publicly known superintelligent agent is undergoing safety testing in company facilities. &nbsp;A second company reports that it has temporarily lost control of its safety infrastructure to superintelligent agents in testing and that it is taking all necessary means to regain control. &nbsp;A third company holds a press conference to report a lab accident in which superintelligent reinforcement learning agents&mdash;which had exhibited power-seeking tendencies in testing&mdash;somehow cooperated with each other to access the internet and, after bypassing security measures, managed to surreptitiously plant copies of themselves in several undisclosed data centers. &nbsp;A government official at the press conference confirms that the company is working with authorities to identify and eliminate any residual threat posed by this incident, claims that there is currently no evidence of such a threat, and asks the public to remain calm.</span></p><h1 class="c3 c25" id="h.wot6a5jjlejp"><span class="c5">3. Simulation as a Tool for Researching Risk Reduction</span></h1><p class="c3"><span>R</span><span class="c7">isk levels for different catastrophic risks </span><span>are </span><span class="c8 c7">highly uncertain and seem likely to remain so for the foreseeable future. The same goes for prospects of risk-reducing strategies. &nbsp;Research that lessens these uncertainties could put us in a better position to set risk-reduction priorities and select risk-reducing interventions. Using simulations to conduct such research is one way that simulations could contribute to the reduction of catastrophic risks. </span></p><p class="c3"><span class="c7">We can divide the use of sim</span><span>ulations</span><span class="c7">&nbsp;in risk-reduction research into two categories: direct and indirect. &nbsp;I</span><span>&rsquo;ll have comparatively more to say about the latter. However, I should register that this does not reflect a distinction in significance between the two categories: I regard both as exploration-worthy but don&rsquo;t have a settled view about which is more important.</span></p><h2 class="c3 c25" id="h.8v8clw8ikl25"><span class="c5">3.1 Direct Risk-Reduction Research</span></h2><p class="c3"><span class="c7">In direct risk-reduction research, simulations of potentially catastrophic scenarios would be run in order to collect data on the catastrophic risks we face, the </span><span>magnitude and severity</span><span class="c7">&nbsp;of those risks, the interventions available to mitigate them, and/or </span><span>intervention </span><span class="c7">efficacy. &nbsp;The idea would be to run simulations that are similar enough to our circumstances in relevant respects for risk and intervention data about the simulated scenarios to have direct bearing on risks and mitigation options in our own case. &nbsp;Collecting data from and running tests with simulations would have potential advantages over trying to collect it from unsimulated sources: it might be that in contrast to unsimulated data sources, using simulations to generate data is more feasible, cheaper, faster, morally preferable, </span><span>more conducive to data collection</span><span class="c8 c7">, or easier to control.</span></p><p class="c3"><span class="c7">Research simulations could conceivably provide frequency data that bears on</span><span>&nbsp;</span><span class="c7">known catastrophic risks. &nbsp;For example, in order to better estimate the likelihood of nuclear war, one might simulate many variations of the 21st century and observe the prevalence of nuclear war.</span><sup class="c7"><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c8 c7">&nbsp;Or simulations might model interactions between multiple risks, e.g. to evaluate how frequently global climate catastrophes would lead to global wars or pandemics.</span></p><p class="c3"><span>Similarly, research simulations could be used to generate data on unknown catastrophic risks. &nbsp;</span><span class="c7">For example, simulations might be used to &ldquo;peek ahead&rdquo; to identify technological &ldquo;</span><span class="c7">black balls</span><span class="c7">&rdquo;</span><span>&mdash;technologies that once created (by default) destroy the civilization that creates them&mdash;</span><span class="c7">before it is too late: rather than creating a technology that by default destroys civilization, we might first simulate such a technology; upon recognizing it as such, we might then avoid creating it or create it only with due precautions.</span><sup class="c7"><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup></p><p class="c3"><span>One use case of research simulations that merits its own treatment is </span><span class="c17">v</span><span class="c17">irtual boxing</span><span>, in which superintelligent (or otherwise potentially dangerous AI) systems are initially confined to simulated environments as a means for testing whether they are safe to release in our environment.</span><sup><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span>&nbsp; For virtual boxing to be of use, simulations containing superintelligent systems would need to generate some information about those systems that is consumed by systems outside the simulation. &nbsp;One concern about virtual boxing is that superintelligent systems might exploit these channels in order to escape or gain hazardous forms of influence outside the simulation. &nbsp;Another concern is that confined systems might recognize their situation, behave safely in service of the instrumental goal of being released, and, after release, suddenly behave treacherously in pursuit of their final goals.</span><sup><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup></p><p class="c3"><span>One approach to addressing these concerns with virtual boxing would be to conduct safety testing within nested simulations: this might prevent catastrophes by confining consequences of escapes and post-release treacherous turns to simulated environments.</span><sup><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span>&nbsp; Additional safety gains might be obtained from punishing such attempts or rewarding compliance with an unpredictable delay. &nbsp;This would incentivize fully unboxed systems that are not certain that they are fully unboxed to continue behaving safely, even if their underlying goals would, unbeknownst to them, be best pursued by, say, inflicting revenge on the agents that confined them.</span><sup><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup><span>&nbsp; Nested simulations could be used in similar fashion to discourage systems from engaging in activities that heighten the risk of catastrophes even if they do not inherently constitute them. For instance, rewarding agents for not seeking control of their source code or for not &ldquo;wireheading&rdquo;</span><sup><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup><span>&nbsp;their reward mechanism in simulated environments could be used to incentivize fully unboxed agents who do not know they are fully unboxed to abstain from such activities.</span><sup><a href="#ftnt20" id="ftnt_ref20">[20]</a></sup></p><p class="c3"><span>Research s</span><span class="c7">imulations </span><span>of catastrophes </span><span class="c8 c7">could also be used to test:</span></p><ul class="c10 lst-kix_4xi6nt4lqkum-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">neglected interventions</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">overlooked failure modes for candidate interventions,</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">the reliability of risk-reduction heuristics. &nbsp;</span></li></ul><ul class="c10 lst-kix_4xi6nt4lqkum-1 start"><li class="c2 li-bullet-0"><span>For example, frequency data might indicate that direct approaches to risk reduction tend to be much more effective than indirect approaches.</span><sup><a href="#ftnt21" id="ftnt_ref21">[21]</a></sup><span>&nbsp;Or it might adjudicate between different hypotheses about the impact of differential technological progress on risk levels.</span><sup><a href="#ftnt22" id="ftnt_ref22">[22]</a></sup></li></ul><ul class="c10 lst-kix_4xi6nt4lqkum-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">tweaks to interventions, </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">the effects of risk-enabling or risk-inhibiting factors,</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">adversarial responses to interventions, </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">sensitivity of risks to perturbations in background conditions, </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">combinations of interventions and risk-relevant factors</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Tractability of research on different risk factors.</span></li></ul><ul class="c10 lst-kix_4xi6nt4lqkum-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">For example, some factors could be revealed to have negligible impact, to be screened off by factors that are easier to control, or to depend too sensitively on other factors to be subject to useful influence.</span></li></ul><p class="c3"><span class="c7">One domain where simulation-based research seems particularly promising is that of </span><span class="c7 c17">value dynamics</span><span class="c7">&mdash;i.e. how normative (moral, prudential, epistemic, political) views that guide action evolve and produce changes over time&mdash;and their bearing on catastrophic risks.</span><sup class="c7"><a href="#ftnt23" id="ftnt_ref23">[23]</a></sup><span class="c8 c7">&nbsp; Plausibly, value dynamics play a central role in determining a societies&rsquo; goals, institutions, priorities, and hence catastrophic risk levels. &nbsp;However, value dynamics are poorly understood. It is difficult to test them in the real world at the population level, and the frequency data we have on them is noisy and sparse at the scales relevant to catastrophic risks. &nbsp;Simulations offer a way forward: by simulating populations of cognizers whose behavior manifests certain normative values and observing how such populations evolve and respond to risks, we may glean clues to answering questions like the following:</span></p><ul class="c10 lst-kix_atj8ngf4kmd5-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">How would different distributions of values affect different risks?</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">How should we expect different values to evolve from different distributions?</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Under what conditions, if any, would different types of values be locked in? In the event that favorable value lock in is impossible, are there any favorable stable value loops or other stable value trajectories that can be enacted?</span><sup class="c7"><a href="#ftnt24" id="ftnt_ref24">[24]</a></sup></li></ul><p class="c3"><span class="c7">Given their roles in the general population or in communities focused on catastrophic risks, </span><span class="c7">simulation-based comparisons </span><span class="c8 c7">of the following seem promising:</span></p><ul class="c10 lst-kix_wjsrvqbtygca-0 start"><li class="c3 c9 li-bullet-0"><span>tradition-favoring values vs. openness to change</span><sup><a href="#ftnt25" id="ftnt_ref25">[25]</a></sup></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-0"><li class="c3 c9 li-bullet-0"><span>r</span><span class="c8 c7">eligious vs. secular values</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">consequentialism vs. deontology </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">downside-focused ethics vs. symmetrical rivals</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">common sense morality vs. longtermism</span></li><li class="c3 c9 li-bullet-0"><span class="c7">a urgent vs. patient longtermism</span><sup class="c7"><a href="#ftnt26" id="ftnt_ref26">[26]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7">a risk-reduction-focused approach vs. a trajectory-change-focused approach</span><sup class="c7"><a href="#ftnt27" id="ftnt_ref27">[27]</a></sup></li><li class="c3 c9 li-bullet-0"><span>moral circle expansion </span><span class="c7">vs. </span><span>virtuous institutions approach</span><sup class="c7"><a href="#ftnt28" id="ftnt_ref28">[28]</a></sup></li></ul><p class="c3"><span class="c7">Evaluating differential prospects for different value dynamics is important because stable value dynamics would harbor the potential to reliably realize value on vast spatial and temporal scales. It could be much better to enact a stable value that will continue yielding mildly positive outcomes into the far future over an unstable&mdash;and, hence, probably short-lived&mdash;value that would yield an extremely positive outcome while present. &nbsp;</span><span class="c7">If value dynamics turned out to be inherently unstable on large time scales</span><span class="c8 c7">, that would severely limit the tractability of influencing the far future and so be a point in favor of prioritizing nearer-term catastrophic risks.</span></p><h2 class="c3 c25" id="h.e6b24g6bv5u"><span class="c5">3.2 Indirect Risk-Reduction Research</span></h2><p class="c3"><span class="c8 c7">Research simulations could also indirectly reduce catastrophic risk. &nbsp;Candidate uses of this sort include:</span></p><ul class="c10 lst-kix_atj8ngf4kmd5-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Cognitive enhancement</span><span class="c7">:</span><span class="c8 c7">&nbsp;simulations could be used to improve intelligent systems performance on different tasks, including tasks that reduce catastrophic risk.</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Important dimensions in the space of possible enhancements include:</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-2 start"><li class="c3 c16 li-bullet-0"><span class="c8 c7">Enhancing biological vs. artificial systems</span></li><li class="c3 c16 li-bullet-0"><span class="c7">Enhancing via learning vs. via cognitive stimulation</span><sup class="c7"><a href="#ftnt29" id="ftnt_ref29">[29]</a></sup><span class="c8 c7">&nbsp;that promotes cognitive abilities (e.g. creativity) through means other than straightforward information transfer</span></li><li class="c3 c16 li-bullet-0"><span class="c7">Enhancing individual systems directly vs. indirectly via a form of artificial selection that operates on populations of systems over generations.</span><sup class="c7"><a href="#ftnt30" id="ftnt_ref30">[30]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Enhancing via external observation of simulation vs. virtual immersion</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Enhancing via interaction with other agents vs. self-play</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Self-selected vs. exogenously selected enhancements</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Enhancing general cognitive capacities vs. risk-reduction specific capacities</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1"><li class="c2 li-bullet-0"><span class="c8 c7">Ways in which cognitive enhancement might reduce catastrophic risk include:</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-2 start"><li class="c3 c16 li-bullet-0"><span class="c8 c7">Yielding better or earlier solutions to technical safety problems</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Leading to technological innovations that reduce risk levels</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Leading to better risk analysis and cause prioritization</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Reducing risk-relevant cognitive mistakes in key decision makers</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Facilitating coordination among key actors in contexts where coordination is crucial for risk levels</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Leading to better epistemics and risk responsiveness at scale that cultivates better institutions (e.g. through wiser choices of leaders in democracies)</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1"><li class="c2 li-bullet-0"><span class="c7">Ways in which cognitive enhancement might </span><span>increase </span><span class="c8 c7">catastrophic risk include:</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-2 start"><li class="c3 c16 li-bullet-0"><span class="c8 c7">Amplifying the power of malevolent actors or actors that are reckless with respect to catastrophic risks</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Leading to technological innovations that elevate risk levels</span></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Hampering coordination (e.g. by introducing or exacerbating power-asymmetries or inducing arms race dynamics)</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Fermi paradox research</span><span class="c7">: Simulation could be used to evaluate the plausibility of different solutions to &ldquo;Fermi&rsquo;s paradox&rdquo;, the problem of explaining why we seem to be alone in the universe, given the apparently astronomical number of opportunities for life and advanced civilizations to emerge.</span><sup class="c7"><a href="#ftnt31" id="ftnt_ref31">[31]</a></sup></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1 start"><li class="c2 li-bullet-0"><span class="c7">Some of these solutions put the &ldquo;Great Filter&rdquo;&mdash;whatever generally prevents non-living matter from transforming into a civilization of the sort we&rsquo;d observe&mdash;as a catastrophic threat in our p</span><span>ast </span><span class="c7">that we were very lucky to avoid, while others locate it in the future as a catastrophic threat to which we will almost certainly succumb.</span><sup class="c7"><a href="#ftnt32" id="ftnt_ref32">[32]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">Evaluations of candidate solutions could therefore provide information about risk levels, and would hence be relevant to how the reduction of different catastrophic risks should be prioritized and the extent to which catastrophic risk-reduction should be prioritized relative to other sorts of intervention.</span><sup class="c7"><a href="#ftnt33" id="ftnt_ref33">[33]</a></sup></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">Simulations that suggest an early Great Filter would be good news: &nbsp;this news would be evidence against the Great Filter lying between us and space faring civilizations. &nbsp;This would be a point in favor of the longtermist view that much of the potential moral (dis)value whose realization we can affect lies in the far future.</span><sup class="c7"><a href="#ftnt34" id="ftnt_ref34">[34]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c7">Simulations suggesting an earlier Great Filter would also suggest that </span><span>our actions have a crucial significance that they would otherwise lack</span><span class="c7">: they&rsquo;d suggest that if we do not create value within the portion of the universe we can influence, then no civilization will.</span><sup class="c7"><a href="#ftnt35" id="ftnt_ref35">[35]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c8 c7">Simulations suggesting that there are many unobserved but advanced civilizations would be good news concerning the risk of our being in a simulation that could be shut down (&sect;9): the existence of many advanced civilizations would provide evidence that the risk of our becoming an advanced civilization and in turn triggering shutdown is small.</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Biological research</span><span class="c7">: simulations of biological structures or processes could be used to rapidly generate information about real biological structures that could in turn be used to guide testing, drug development, diagnostics, and treatment. &nbsp;While it&rsquo;s still early days, recent advances in simulation technology in this area&mdash;notably &nbsp;Google DeepMind&rsquo;s AlphaFold 2, a program whose astonishingly accurate models of protein structure are widely recognized as a breakthrough&mdash;</span><sup class="c7"><a href="#ftnt36" id="ftnt_ref36">[36]</a></sup><span class="c8 c7">are promising. </span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Debunking testing</span><span class="c8 c7">: some evolutionary debunking arguments hold that facts about the biological or cultural evolutionary origins of certain of our beliefs (or the mechanisms that generate them) preclude those beliefs from being justified or qualifying as knowledge.</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1 start"><li class="c2 li-bullet-0"><span class="c7">The most discussed evolutionary debunking arguments target (that is, seek to debunk) moral beliefs or religious beliefs.</span><sup class="c7"><a href="#ftnt37" id="ftnt_ref37">[37]</a></sup><span class="c7">&nbsp; Those that target moral beliefs are usually conditional on </span><span class="c7 c17">moral realism</span><span class="c7">, the view that there are objective moral facts. &nbsp;Also relevant in the context of simulations and catastrophic risks are debunking arguments that target beliefs about consciousness,</span><sup class="c7"><a href="#ftnt38" id="ftnt_ref38">[38]</a></sup><span class="c8 c7">&nbsp;since such beliefs could inform decisions regarding simulation inhabitants.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Evolutionary debunking arguments rely on the assumption that the beliefs targeted for debunking are shaped by contingencies of evolution in a way that makes those beliefs epistemically defective. Different evolutionary debunking arguments trace epistemic defects in the targeted beliefs to different consequences of evolution. </span></li><li class="c2 li-bullet-0"><span class="c7">I will sketch and work with what I regard as an especially straightforward and powerful approach to debunking moral beliefs. It claims that, in light of evolution and given moral realism, we should regard our targeted moral beliefs as unsafe: we should think that even if our targeted moral beliefs are in fact true, there are nearby counterfactual scenarios in which evolution instead produced incompatible moral beliefs. &nbsp;On moral realism, the basic moral facts are invariant across these scenarios, and it would just be a matter of evolutionary good fortune if we turned out to be in the good case: so either our moral beliefs are false or they easily could have been. According to the argument, this result&mdash;at least once recognized and absent independent vindication of our targeted beliefs&mdash;renders our moral beliefs epistemically defective.</span><sup class="c7"><a href="#ftnt39" id="ftnt_ref39">[39]</a></sup><span class="c8 c7">&nbsp;Holding the non-defectiveness of our moral beliefs fixed, the argument thus tells against moral realism. Holding moral realism fixed, the argument undermines our moral beliefs.</span></li><li class="c2 li-bullet-0"><span class="c7">This argument presupposes that moral beliefs vary across nearby scenarios in which evolution went differently. &nbsp;While it seems plausible that there is such variation, this can be challenged. An alternative hypothesis is that evolution robustly selects for rationality in species like our own and rationality induces convergence on the targeted beliefs.</span><sup class="c7"><a href="#ftnt40" id="ftnt_ref40">[40]</a></sup><span class="c8 c7">&nbsp; </span></li><li class="c2 li-bullet-0"><span class="c7">Because of our limited access to beliefs&rsquo; evolutionary origins, the assumed variation across nearby counterfactual scenarios is difficult to test directly. Simulations offer an indirect way to test the assumption: (1) simulate evolutionary processes under a range of conditions that yield human-like creatures with beliefs about the domain of interest and (2) check whether differences in conditions induce differences in belief about that domain. &nbsp;If induced differences are found, this supports the assumption that the targeted beliefs are modally fragile in the way the argument requires; if such differences are not found, that disconfirms the assumption.</span><sup class="c7"><a href="#ftnt41" id="ftnt_ref41">[41]</a></sup></li><li class="c2 li-bullet-0"><span class="c8 c7">If simulations of nearby evolutionary scenarios found widespread differences in (central, basic, or nearly all) moral beliefs across such scenarios, this would lend to an evolutionary debunking argument against moral realism. &nbsp;The argument is: our moral beliefs are in epistemically good standing if moral realism is true; but we should recognize that, since our (central) moral beliefs are shaped by the contingencies of evolution, on moral realism they are at best accidentally getting at the truth and are hence epistemically defective. So, moral realism is not true. On the other hand, if simulations found differences in belief across scenarios to be rare, this would undermine the argument.</span></li><li class="c2 li-bullet-0"><span class="c7">If simulations of nearby evolutionary scenarios found that certain (e.g. deontological) moral beliefs varied across the scenarios while other (e.g. consequentialist) ones did not, that would provide the basis for a debunking argument against the former.</span><sup class="c7"><a href="#ftnt42" id="ftnt_ref42">[42]</a></sup><span class="c7">&nbsp; Simulations could thus be used to probe whether moral beliefs that are important for evaluating catastrophic risks are subject to debunking; likewise for epistemic and decision-theoretic beliefs.</span><sup class="c7"><a href="#ftnt43" id="ftnt_ref43">[43]</a></sup></li><li class="c2 li-bullet-0"><span class="c8 c7">While evolutionary debunking arguments are typically directed against realist views, moral judgments may be susceptible to debunking even on anti-realism. &nbsp;There are several potential sources of debunking on antirealism:</span></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">The most obvious way is for debunking to be used to support moral nihilism, the version of moral antirealism that claims that there are no moral facts or moral properties.</span><sup class="c7"><a href="#ftnt44" id="ftnt_ref44">[44]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c7">Some forms of antirealism allow the standing of subjects&rsquo; moral judgments to be beholden to how a subject would respond to facts about the causal origins of their beliefs.</span><sup class="c7"><a href="#ftnt45" id="ftnt_ref45">[45]</a></sup><span class="c8 c7">&nbsp;On such views, a subject&rsquo;s judgment that incest is wrong might be susceptible to debunking if she would give it up upon learning that it is produced by certain evolutionary forces. &nbsp;</span></li><li class="c3 c16 li-bullet-0"><span class="c7">Some antirealists (in particular, expressivists) often try to eschew realism&rsquo;s metaphysical commitments while nonetheless vindicating realist-sounding moral thought and talk. &nbsp;Such antirealists face a challenge of showing that the moral terms they seek to vindicate cannot be used to recast debunking arguments to target their own form of antirealism.</span><sup class="c7"><a href="#ftnt46" id="ftnt_ref46">[46]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c7">There is an ongoing debate about whether evolutionary debunking arguments can be run against specific philosophical positions such as moral realism without devolving into arguments for sweeping and implausibly general or self-undermining forms of antirealism or skepticism.</span><sup class="c7"><a href="#ftnt47" id="ftnt_ref47">[47]</a></sup></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-1"><li class="c2 li-bullet-0"><span class="c7">The above testing method could prove fruitful in the context of the AI alignment problem:</span><sup class="c7"><a href="#ftnt48" id="ftnt_ref48">[48]</a></sup><span class="c7">&nbsp;if we (should) want to align AI systems with our justified moral beliefs or our moral knowledge rather than, say, the value of maximizing paperclip production,</span><sup class="c7"><a href="#ftnt49" id="ftnt_ref49">[49]</a></sup><span class="c7">&nbsp;the</span><span class="c7">n we need to exclude our debunked moral views from the set of moral views we align AI with.</span><span class="c7">&nbsp; Likewise if we want to align AI systems with some combination of our preferences and our moral knowledge.</span><sup class="c7"><a href="#ftnt50" id="ftnt_ref50">[50]</a></sup><span class="c8 c7">&nbsp; For the reasons encountered above, it may be difficult to figure out which moral judgments are evolutionarily debunked and simulations may help.</span></li><li class="c2 li-bullet-0"><span class="c7">By shedding light on the extent to which moral beliefs are debunked, simulations could also bear on the plausibility of moral realism by bolstering or undermining the debunking argument against moral realism. &nbsp;This could in turn bear on how to pose the alignment problem: the problem is usually posed in terms of aligning AI with human preferences. &nbsp;However, if moral realism is true, even idealized human preferences are at best a proxy for the moral facts that powerful AI systems would need to be aligned with in order to avoid moral catastrophe.</span><sup class="c7"><a href="#ftnt51" id="ftnt_ref51">[51]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">It should be borne in mind that the proposed simulation test just concerns the safety-based evolutionary debunking argument. There are other evolutionary debunking arguments that are not necessarily amenable to that test. &nbsp;For example, rather than claiming that evolution renders our moral beliefs unsafe, debunkers could claim that evolution renders our moral beliefs insensitive&mdash;i.e. such that they would not have been different if the moral truths had been different&mdash;and hence defective.</span><sup class="c7"><a href="#ftnt52" id="ftnt_ref52">[52]</a></sup><span class="c7">&nbsp;Whereas the crucial variation premise in the safety-based argument turns primarily on empirical questions about evolution (conditional on moral realism), the sensitivity of our moral beliefs to moral truths potentially turns on a range of more philosophically-loaded issues: the causal efficacy of moral facts, the modal profiles of moral facts, the relevant type of modality for insensitivity, and the enabling conditions for insensitivity to render beliefs defective.</span><sup class="c7"><a href="#ftnt53" id="ftnt_ref53">[53]</a></sup><span class="c7">&nbsp;It is not clear how simulations could provide traction on any of these issues. &nbsp;All this suggests that some evolutionary debunking arguments will be more amenable to simulation testing than others. &nbsp;There is a project here of evaluating the prospects for using simulations to test different evolutionary debunking arguments and then developing simulations to test those that are amenable. &nbsp;In addition to safety-based and sensitivity-based evolutionary debunking arguments, there are also arguments that instead appeal to accidentality, unexplained coincidence, absence of explanation-apt reliability, and disagreement.</span><sup class="c7"><a href="#ftnt54" id="ftnt_ref54">[54]</a></sup></li></ul><ul class="c10 lst-kix_atj8ngf4kmd5-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Consciousness testing</span><span class="c7">: at present there is a vast and growing literature on consciousness but no theory about which entities are conscious that commands consensus.</span><sup class="c7"><a href="#ftnt55" id="ftnt_ref55">[55]</a></sup><span class="c7">&nbsp;This is unfortunate, since without such a theory we are in the dark about consciousness in digital systems.</span><sup class="c7"><a href="#ftnt56" id="ftnt_ref56">[56]</a></sup><span class="c7">&nbsp; To make progress in this area, more data may be required. One way to generate more data is to subject candidate conscious systems to tests for consciousness.</span><sup class="c7"><a href="#ftnt57" id="ftnt_ref57">[57]</a></sup><span class="c7">&nbsp; Digital systems may prove valuable test subjects</span><span>: even under appropriate ethical constraints, </span><span class="c8 c7">their inner workings may be easier to observe, record, understand, and alter. In addition, their faster processing speed and precise duplicability may lend to more efficient testing. &nbsp;Running suitable tests on such systems may require embedding them in a real or virtual environment. &nbsp;The ability to run tests faster in virtual environments&mdash;as well as the data-collection advantages of virtual environments&mdash;would then favor running tests on digital systems that inhabit simulations.</span></li></ul><h2 class="c3 c25" id="h.x0l34cyzo1zj"><span class="c27 c33 c21">3.3 Dual</span><span class="c27 c21">-</span><span class="c27 c33 c21">Use Potential</span></h2><p class="c3"><span>As with other technologies, research simulations have dual-use potential: whether these technologies heighten or reduce risks will depend on how they are used.</span><sup><a href="#ftnt58" id="ftnt_ref58">[58]</a></sup><span class="c8 c7">&nbsp; Some possible dual uses include:</span></p><ul class="c10 lst-kix_3okys5qak4tw-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Wargaming simulations could be used for offensive or defensive purposes.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Value dynamics simulations could be used to promote a favorable value trajectory or, instead, to induce lock-in with respect to, say, the preferred values of a totalitarian regime.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Biological or chemical research simulations could be used in the development of medical treatments or in the development of biological and chemical weapons</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Virtual training aimed at detecting and curtailing power-seeking behavior in artificial agents could be tweaked to promote surreptitious power-seeking behavior</span></li></ul><p class="c3"><span>As a recent, cautionary illustration, consider Collaborations Pharmaceuticals, Inc., a company that uses a model with generative and predictive machine learning components to identify new molecules and predict their biological properties.</span><sup><a href="#ftnt59" id="ftnt_ref59">[59]</a></sup><span class="c8 c7">&nbsp; Ordinarily, the company trains their model with a reward function that penalizes toxicity. &nbsp;However, in preparation for the conference, the company tweaked their models to reward toxicity and trained them on publicly available drug-like molecules, not toxic compounds. &nbsp;Within six hours, their model identified 40,000 molecules that were predicted to exceed a toxicity threshold set by one of the most toxic chemical warfare agents. &nbsp;These included that agent, other known chemical warfare agents, agents with higher predicted toxicity than publicly known agents, and a class of molecules in an unexplored region of molecular property space. &nbsp;The company reported an absence of significant barriers to synthesizing these molecules. &nbsp;The company also reported that its researchers had previously been naive to the potential misuse of their trade, despite working in the area for decades.</span></p><p class="c3 c22"><span class="c8 c7">As this case suggests, the potential misuse of research simulations is a source of catastrophic risks that is apt to be underestimated. &nbsp;Contributing factors include:</span></p><ul class="c10 lst-kix_rlriyhdlsafy-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">While the probability of an arbitrary </span><span>user </span><span class="c7">of a research simulation seeking to cause catastrophe is presumably low, the number of such operators will presumably increase as research simulations become more common. &nbsp;</span><span class="c8 c7">The probability that someone will seek to cause catastrophes with research simulations is thus much higher than the probability that an arbitrary operator of a research simulation causing a catastrophe.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Likewise, high safety levels for individual research simulations (or individual research organizations that use them) is compatible with such simulations (organizations) collectively posing a substantial catastrophic risk.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">At present, there is minimal regulation concerning the use of simulations. As research simulations with dangerous uses are introduced, adequate regulatory safeguards may not yet be in place.</span></li><li class="c3 c9 li-bullet-0"><span>To cause a catastrophe, users of research simulations need not seek to do so. &nbsp;Laboratory accidents with lethal pathogens are not uncommon in high-level biosafety facilities that are subject to stringent safety regulations.</span><sup><a href="#ftnt60" id="ftnt_ref60">[60]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Designers and users of research simulations that pose catastrophic risks are unlikely to face incentives that are appropriately sensitive to these risks. &nbsp;</span></li></ul><ul class="c10 lst-kix_rlriyhdlsafy-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">For example, from a personal perspective, researchers may find it difficult to resist running research simulations that provide job security or advance their careers when the associated catastrophic risk on any particular run is tiny, even when iterating the choice is catastrophically bad in expectation.</span></li></ul><ul class="c10 lst-kix_rlriyhdlsafy-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">As the Collaborations Pharmaceuticals case described above illustrates, researchers who are focused on beneficial uses can be oblivious to dual uses.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Even in cases in which dual uses are recognized, the tendency to ignore low probability risks may push many research simulation designers and users toward ignoring risks.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Open science norms will likely broaden the availability of research simulations with dual uses.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Personal liability norms (e.g. not holding people responsible for foreseeable indirect effects when those effects depend on downstream decisions of other agents) may lead research simulation users and designers to disregard how their actions will indirectly affect the potential for catastrophic misuse of research simulations.</span></li></ul><p class="c3"><span>All this suggests that preventing the misuse of research simulations is a promising strategy for catastrophic risk reduction. &nbsp;Since some of the relevant technologies have yet to be invented, there are at present limits to pursuing this strategy via technical safety work. &nbsp;On the other hand, I would expect enacting safety regulations to become increasingly difficult as the technologies become more widely used and entangled with the interests of powerful actors. &nbsp;If so, there is reason to pursue the strategy via AI governance in the near-term</span><span>.</span></p><h2 class="c3 c25" id="h.klbo747zvcma"><span class="c5">3.4 Will Research Simulations Be Superseded Before They Arrive?</span></h2><p class="c3"><span class="c7">If large-scale research, social simulations arrive after </span><span class="c7">some other technology&mdash;for example, superintelligent AI</span><span class="c8 c7">&mdash;that would be better suited to conducting the relevant research, then research simulations would presumably not be run. In that case, there would be no point in considering them in thinking about how to reduce catastrophic risks.</span></p><p class="c3"><span class="c7">The hypothesis that research sim</span><span>ulations</span><span class="c8 c7">&nbsp;would be superseded before they arrive may turn out to be correct. &nbsp;However, it should not be used as a basis for ignoring the prospects and dangers posed by research simulations, as it is not a hypothesis in which we should be very confident. There are several reasons for this:</span></p><ul class="c10 lst-kix_suelkbc95i81-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">It&rsquo;s a live (if less plausible as of late) possibility that we&rsquo;ll find ourselves in a slow take-off scenario in which the first AGIs we will create are whole brain emulations or cerebral organoids and that the creation of superintelligent systems is still decades away from then, leaving ample time for research simulations and their associated risks.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Safety concerns may prevent the creation of superintelligence. &nbsp; </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">It may turn out that research simulations have an important role to play in creating aligned superintelligent AI. For instance, safety concerns may result in extensive testing of superintelligent AI within a simulated setting prior to release into unsimulated environments. &nbsp;If so, we should expect some pressure toward running large-scale research simulations in the process leading up to the release of a superintelligent system.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Superintelligent systems will have limited computational power. &nbsp;This means that they will need to use approximation techniques for problems whose exact solutions are computationally intractable. &nbsp;Simulations are often well-suited to this purpose. For instance, predicting the exact evolution of society or the climate from fundamental physics will remain computationally intractable even for superintelligent systems. So it would be unsurprising if research simulations are among the tools used by superintelligent systems.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Superintelligence may arise only </span><span>in narrow</span><span class="c7">, non-chaotic domains where exact long-term predictions are computationally tractable. &nbsp;For other domains, research simulations may remain at the cutting edge for</span><span>&nbsp;forecasting</span><span class="c7">.</span><sup class="c7"><a href="#ftnt61" id="ftnt_ref61">[61]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">After arriving, superintelligence may be expensive or otherwise limited in its availability. &nbsp;In that case, there would be incentive to run research simulations that are more cost effective.</span></li></ul><p class="c3 c18 c32"><span class="c5"></span></p><h1 class="c3 c25" id="h.222tkjbkfic"><span class="c5">4. Simulation as a Tool for Ethically Enhancing Testing</span></h1><p class="c3"><span class="c7">Some experiments on humans and non-human animals that could yield valuable information are ethically problematic</span><span>. &nbsp;I</span><span class="c7">ndeed, history is riddled with medical experiments in which people were harmed or killed by experimental treatments to which they did not consent.</span><sup class="c7"><a href="#ftnt62" id="ftnt_ref62">[62]</a></sup><span class="c7">&nbsp; In some cases, simulations offer an ethically superior alternative: they provide a way to acquire the sought information without harming subjects or infringing on their autonomy.</span><sup class="c7"><a href="#ftnt63" id="ftnt_ref63">[63]</a></sup><span class="c7">&nbsp; These alternatives should become more tractable as animal experimentation requirements on drug testing are relaxed.</span><sup class="c7"><a href="#ftnt64" id="ftnt_ref64">[64]</a></sup></p><p class="c3"><span class="c8 c7">Similarly, simulations may offer a means to ethically enhance experiments that are currently deemed ethically permissible: for example, rather than performing suffering-involving experiments on non-human animals, experiments might be performed on simulated counterparts of them that are unconscious or which have only positively valenced experiences. &nbsp;Likewise, for drug treatments on human patients with presently incurable diseases. &nbsp;Such simulations might also offer advantages in terms of speed, number, and data-collection over corresponding human and animal experiments.</span></p><p class="c3"><span class="c8 c7">From a perspective of catastrophic risk prevention, it might seem that such experiments harbor only modest potential for improving the long-term future: it is not clear how they might be used to reduce the risk of catastrophes.</span></p><p class="c3"><span class="c8 c7">In response: </span></p><ul class="c10 lst-kix_kh510wo2ghv0-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">Even if it is hard to see how such &nbsp;simulations might reduce the risk of acute catastrophes, </span><span>it is easy to see how</span><span class="c7">&nbsp;they might reduce the risk of </span><span class="c7 c17">prolonged comparative </span><span class="c7">catastrophes</span><span class="c8 c7">.</span></li></ul><ul class="c10 lst-kix_kh510wo2ghv0-1 start"><li class="c2 li-bullet-0"><span>F</span><span class="c8 c7">or example, it might turn out that running simulations would lead to a psychological treatment that would reduce future suffering by 1% and would otherwise go undiscovered. &nbsp;And it might turn out that the future contains trillions of people who would suffer less as a result of the treatment. In that case, failing to discover the treatment would be a catastrophe, as it would entail immense quantities of suffering, albeit spread over many people and generations.</span></li></ul><ul class="c10 lst-kix_kh510wo2ghv0-0"><li class="c3 c9 li-bullet-0"><span class="c7">Ethically-enhanced testing could potentially speed up the timelines for responding to rapidly emerging catastrophic threats posed by biological pathogens.</span></li></ul><ul class="c10 lst-kix_kh510wo2ghv0-1 start"><li class="c2 li-bullet-0"><span class="c7">For example, </span><span class="c7 c17">challenge trials&mdash;</span><span class="c7">in which subjects are intentionally exposed to a pathogen&mdash;have facilitated the development of vaccines or treatments for the likes of smallpox, influenza, and malaria.</span><sup class="c7"><a href="#ftnt65" id="ftnt_ref65">[65]</a></sup><span class="c8 c7">&nbsp; Rightly or not, such trials are perceived as ethically questionable and are sometimes not run due to ethical concerns. Such concerns might be sidestepped or allayed through ethically-enhanced challenge trials&mdash;for example, ones with unconscious simulated subjects. </span></li></ul><ul class="c10 lst-kix_kh510wo2ghv0-0"><li class="c3 c9 li-bullet-0"><span>Simulation-based testing could also offer safer (and hence more ethical) replacements of dangerous testing protocols (such as gain-of-function research on biological pathogens</span><span>)</span><span>.</span></li></ul><h1 class="c3 c25" id="h.oa89afuwzwo1"><span class="c5">5. Using Virtual Reality to Promote Risk Responsiveness and Disaster Preparedness</span></h1><p class="c3"><span class="c8 c7">One potential path to reducing catastrophic risk aims to make them more of a priority for policymakers by making relevant political constituencies more responsive to them. It is not surprising that catastrophic risks do not play a larger role in mainstream politics, as various factors make them hard to think about or tempting to ignore: they involve (small) probabilities, difficult to quantify uncertainty, large numbers of persons as well as many non-agent variables, and spatiotemporal scales that humans do not ordinarily think about. All this suggests that, for those who take reducing catastrophic risks to be a top global priority, simulations may offer a low-hanging fruit: simulations might be used to reduce catastrophic risks by drawing more people to think about, understand, and respond to those risks.</span></p><p class="c3"><span class="c7 c17">Gamified simulations </span><span class="c7">of catastrophes are one sort of simulation that could be used to this end. &nbsp;In such simulations, catastrophic risks are simulated, players manipulate parameters to try to reduce risks, and the players then witness the consequences of their choices. &nbsp;Interactive programs of this sort have been made for climate change</span><span>&nbsp;(</span><span class="c7">e.g. see </span><span class="c28 c7"><a class="c13" href="https://c-roads.climateinteractive.org/">https://c-roads.climateinteractive.org/</a></span><span>)</span><span class="c7">.</span><sup class="c7"><a href="#ftnt66" id="ftnt_ref66">[66]</a></sup><span class="c7">&nbsp; Popular games that simulate various wars, natural disasters, and/or civilizational collapse can be found in series such as </span><span class="c28 c7"><a class="c13" href="https://en.wikipedia.org/wiki/Civilization_(series)">Civilization</a></span><span class="c7">. &nbsp;As far as I know, no popular games have been introduced or widely used for the purpose of increasing responsiveness to catastrophic risk. But it is interesting to note that the unprecedented popularity of such games in the 1990s and 2000s was followed by an explosion of research on existential risks in the 2000s that continues to this day. Whether or not such games had any role in cultivating responsiveness to catastrophic risk among today&rsquo;s researchers, there may be ways of using them to that end, e.g. by using them as part of an educational curriculum on catastrophic risks.</span><sup class="c7"><a href="#ftnt67" id="ftnt_ref67">[67]</a></sup></p><p class="c3"><span class="c7 c17">Immersive simulations</span><span class="c7">&nbsp;offer another sort of simulation that might be used to increase responsiveness to catastrophic risks. &nbsp;</span><span class="c7">As virtual simulations continue to improve, we will be able to have increasingly realistic-seeming experiences of virtual environments. It is plausible that realistic experiences of living through catastrophes would induce increased risk-responsiveness.</span><span class="c8 c7">&nbsp;Compare: we would expect those who have lived through a world war or pandemic to take militaristic and biological catastrophic risks more seriously than those who have merely read about them. &nbsp;Of course, immersive catastrophic simulations could be potentially traumatizing. &nbsp;This ethical concern is among those that would need to be taken into account in designing such simulations and deciding how to deploy them.</span></p><p class="c3"><span class="c7">The risk-reduction potential of gamified and immersive simulations depends partly on two factors: their effectiveness at increasing risk responsiveness and the extent to which risk responsiveness serves to reduce catastrophic risk. &nbsp;These factors are difficult to estimate. Evaluating the first factor is a potential research program that could be implemented now using standard tools and methods from the social sciences, perhaps in collaboration with video game creators. &nbsp;Evaluating the second factor is less straightforward. In the future, estimates of it might be achieved through simulations of societies that face catastrophic risks and exhibit varying levels of risk responsiveness.</span></p><p class="c3"><span>Immersive simulations can also be used as training tools to improve safety and disaster mitigation. Immersive simulations are already used to train astronauts, pilots, firefighters, military personnel, and workers in the nuclear industry</span><span>.</span><span>&nbsp; I am not aware of any general investigation of the potential to reduce catastrophic risks through immersive simulation safety and disaster mitigation. &nbsp;With recent improvements in virtual reality technology and more on the way, I believe this is a valuable time to carry out such an investigation.</span></p><h1 class="c3 c25" id="h.vsn4x8eorarf"><span class="c5">6. Bunkers, Fallbacks, and Grand Futures</span></h1><h2 class="c3 c25" id="h.8b35hj3a0heb"><span class="c27 c33 c21">6.1 Simulation Refuges</span></h2><p class="c3"><span class="c7">Catastrophic risks can be mitigated either by lowering the probability of catastrophe or by reducing the expected harm of the catastrophe if it occurs. &nbsp;For example, the risk posed by nuclear war can be mitigated either by lowering the probability of nuclear war or by raising the probability of civilizational recovery in the event of nuclear war. &nbsp;A now common observation is that existential catastrophes would tend to be far worse than </span><span>many non-existential</span><span class="c7">&nbsp;catastrophes, even ones that would kill a large percentage of the world&rsquo;s population: while both sorts of catastrophe would be bad for those directly harmed, existential catastrophes also preclude the realization of value in the vast stretches of time and space that lie before us. &nbsp;Thus, it is worth considering proposals that aim to reduc</span><span>e </span><span class="c7">existential risk without aiming to </span><span>reduce </span><span class="c8 c7">other types of catastrophic risk. &nbsp;</span></p><p class="c3"><span class="c7">One such proposal is to build </span><span class="c7 c17">refuges</span><span class="c7">, facilities that would house agents in the event of a would-be existential catastrophe and rebuild civilization in its aftermath. Proposed sorts include subterranean, aquatic, and extraterrestrial r</span><span>efuges</span><span class="c7">.</span><sup class="c7"><a href="#ftnt68" id="ftnt_ref68">[68]</a></sup><span class="c7">&nbsp; Discussions of such refuges tend to assume that such refuges would be inhabited by humans and that humans would reside in non-virtual environments within refuges. &nbsp;Simulations offer alternatives. </span><span class="c7">One option would be to create refuges that would physically house humans who would primarily live in virtual environments. &nbsp;For example, a subterranean refuge might house a society of humans in cramped quarters that virtually lives in more expansive simulated environments until it is safe to rebuild civilization on Earth&rsquo;s surface.</span><span class="c8 c7">&nbsp; Another option would be to create refuges populated by digital minds rather than humans interacting with simulated environments. &nbsp;While such minds might inhabit simulated environments while living in the refuges, they could be designed to interact with the environment external to the refuge as well. When the time is right, they would exit the refuge and rebuild civilization.</span></p><p class="c3"><span class="c7">Beckstead (2015) notes some limitations of using refuges for risk mitigation. One is that they would not help in &ldquo;overkill&rdquo; scenarios where the catastrophe would kill people in refuges. &nbsp;Another is that they would not help in very long-term environmental damage scenarios where there is no environment for refuge inhabitants to return to in order to rebuild. &nbsp;Simulation refuges could avoid these limitations to some extent: for example, Earth</span><span>&rsquo;s surface might be rendered uninhabitable for humans by </span><span class="c7">a nuclear war, </span><span>extinction level pandemic, </span><span class="c7">misaligned superintelligent AI, or a nanotechnological catastrophe</span><span>; yet it might still be </span><span class="c7">habitable by digital minds lying in wait in extraterrestrial simulation refuges</span><span class="c8 c7">. &nbsp;Similarly, digital minds might be well-positioned to embark from subterranean or aquatic refuges to rebuild in the wake of a catastrophe caused by biological pathogens that prevent humans from surviving on the planet.</span></p><p class="c3"><span class="c8 c7">Further advantages of simulation refuges over non-simulation refuges may include: </span></p><ul class="c10 lst-kix_82fd1jsxcdip-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">easier to isolate</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">easier to hide</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">more energy efficient</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">greater longevity</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">operable under a wider range of conditions</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">more durable</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">more mobile</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">more compact</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">cheaper to produce, </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">producible on a larger scale.</span></li></ul><p class="c3"><span class="c7">A major concern about purely digital simulation refuges is that it is not clear that the digital agents populating them would be conscious or capable of realizing value. &nbsp;Arguably, a scenario in which digital systems emerge from refuges and create a bustling galactic civilization of unconscious machines would be as bad as extinction. &nbsp;This concern could be mitigated through future research on consciousness that reveals which sorts of digital systems would be conscious. (o</span><span>r exacerbated if it is revealed that digital systems are ill-suited to realize consciousness)</span><span class="c7">&nbsp; However, it is not clear that such revelations are possible deliverances of future research and, even if they are, they may not arrive or </span><span>be incorporated </span><span class="c8 c7">in time. &nbsp;Absent such revelations, we might opt for simulation refuges whose inhabitants would emerge to rebuild a civilization in which biological agents (presumably humans) play an important role.</span></p><p class="c3"><span class="c8 c7">Simulation refuges would not guard against all catastrophic risks. &nbsp;Some catastrophes&mdash;such as physics experiments that destroy the known universe and the shut down of our universe in the event that it&rsquo;s a simulation&mdash;would not spare those living in refuges even of the simulation variety. &nbsp;It should also be acknowledged that another limitation Beckstead notes applies to both simulation and non-simulation refuges: they would not help in &ldquo;underkill&rdquo; scenarios involving catastrophes that are not destructive enough for refuges to be relevant.</span></p><h2 class="c3 c25" id="h.mh66riqsaeau"><span class="c5">6.2 Simulations and Grand Futures</span></h2><p class="c3"><span class="c7">We have seen that simulations could be used to safeguard against existential risks, thereby avoiding the permanent loss of our civilization&rsquo;s immense future potential. &nbsp;In addition, simulations may have a role to play in realizing that potential. &nbsp;Indeed, much of our future&rsquo;s potential may lie in the possibility of spreading virtual paradises across the galaxies within the affectable universe. &nbsp;The design space of digital systems that could be created in the future is vast. &nbsp;By way of comparison with biological minds, future digital systems will very probably be cheaper to produce, as well as much faster at processing information and capable of processing much larger quantities of information. This suggests that if digital systems will be capable of having experiences at all, t</span><span>hen&mdash;relative to humans&mdash;t</span><span class="c7">hey will be capable of having far more of them and of realizing far more value through them.</span><sup class="c7"><a href="#ftnt69" id="ftnt_ref69">[69]</a></sup><span class="c7">&nbsp; A further reason for thinking that much of the future&rsquo;s potential value may lie in the potential for virtual paradises can be found in the option of using &ldquo;aestivation&rdquo; to make the most of energy resources: by entering a relatively inactive state until computation becomes more efficient with the arrival of cooler conditions in the very far future, civilization could extract more compute and in turn value from its resources.</span><sup class="c7"><a href="#ftnt70" id="ftnt_ref70">[70]</a></sup><span class="c8 c7">&nbsp; </span></p><p class="c3"><span class="c7">Shiller (2017) uses closely related considerations to argue in favor of the artificial replacement thesis that we should engineer the extinction of humanity so as to bring about artificial descendants capable of living better lives. &nbsp;</span><span>S</span><span class="c7">uch beings could conceivably </span><span>inhabit</span><span class="c7">&nbsp;non-virtual environments</span><span>. &nbsp;But it </span><span class="c7">seems more likely that they would live out their artificial lives in largely virtual settings</span><span>, as virtual environments would likely be much easier to mold to the preferences of such artificial beings than would non-virtual environments.</span></p><p class="c3"><span class="c7">One concern about a grand future populated by such beings is that such virtual realities would merely simulate valuable phenomena. &nbsp;However, on reflection, at least given that such beings would be conscious, this concern seems misplaced: genuine friendships, love, achievements, knowledge, etc. could exist in virtual realities.</span><sup class="c7"><a href="#ftnt71" id="ftnt_ref71">[71]</a></sup></p><p class="c3"><span class="c7">The more pressing concern in the vicinity is again that the digital systems would be unconscious and incapable of realizing value. &nbsp;Perhaps by the time we&rsquo;re in a position to initiate a grand future, we&rsquo;ll know whether digital simulations would be conscious. &nbsp;If not, then one way to address this concern in the context of a grand futures strategy would be to adopt a mixed digital-biological portfolio: we could aim to create both biological and digital paradises.</span><sup class="c7"><a href="#ftnt72" id="ftnt_ref72">[72]</a></sup><span class="c8 c7">&nbsp; </span></p><p class="c3"><span class="c7">Hedging our bets in this fashion would ensure that </span><span class="c7 c17">some </span><span class="c8 c7">sort of immensely valuable future would exist. &nbsp;Such a mixed-strategy might be best in expectation, but it would be unlikely to bring about the best outcome: the simulated paradises would either turn out to feature consciousness and realize value or they would not; if they did, the resources devoted to running biological paradises probably could have been used to bring about a much better outcome via more digital paradises; if not, the resources devoted to running digital paradises probably could have been used to bring about a much better outcome via more biological paradises. &nbsp;This serves to highlight how knowing the conscious status of digital systems could prove valuable: without such knowledge, doing what is best in expectation may require us to leave much of the future&rsquo;s potential value unrealized.</span></p><p class="c3"><span class="c7">In the context of grand future strategies, simulations could also be used as a sort of safety check. &nbsp;Before deciding once and for all which grand future to implement, we would want to take measures to ensure that we have not overlooked important upsides or downsides of options that we are choosing among. &nbsp;One way to do this would be to simulate our options prior to choosing. &nbsp;In addition to revealing features of options that we would have otherwise overlooked, such simulations could also be used to tweak and optimize different approaches to bringing about a grand future.</span></p><h2 class="c3 c25" id="h.5ahfo0g910"><span class="c5">6.3 Fallbacks</span></h2><p class="c3"><span class="c7">Simulations could also serve as </span><span class="c7 c17">fallbacks</span><span class="c7">, i.e. as outcomes that we bring about in the event that better outcomes elude us. &nbsp;For instance, suppose that spreading civilization beyond our solar system is initially the best option available to us. However, </span><span>currently unknown engineering obstacles to interstellar travel </span><span class="c7">force</span><span>&nbsp;</span><span class="c8 c7">us to relinquish any hope of extending civilization to other solar systems. In that case, we would have squandered almost all of our cosmic potential. Yet our remaining potential might be vast: using only resources from our solar system, it might still be within our reach to run simulations populated by trillions of minds that enjoy super-human levels of welfare.</span></p><p class="c3"><span class="c8 c7">How valuable simulations would be to have as fallback options depends on various factors, including:</span></p><ul class="c10 lst-kix_gokaxdc70ert-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">The probability of simulations realizing consciousness and value</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">The extent to which resources initially devoted to the primary option could be efficiently diverted to simulations when the primary option becomes unavailable</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">The opportunity cost of investing in simulations as fallbacks vs. the primary option or other fallbacks</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">The probability of the primary option becoming unavailable</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">The probability that fallback simulations would be used if the primary option became unavailable</span></li></ul><p class="c3"><span class="c8 c7">A full analysis of the prospects for using simulations as fallback options is a task for another occasion.</span></p><h1 class="c3 c25" id="h.5imj0auzxqt1"><span class="c5">7. &nbsp;Catastrophic Simulations</span></h1><p class="c3"><span class="c7">There is reason to think that at least some simulated minds (such as whole brain emulations) would be conscious and capable of suffering.</span><sup class="c7"><a href="#ftnt73" id="ftnt_ref73">[73]</a></sup><span class="c7">&nbsp; This raises the possibility of </span><span class="c7 c17">catastrophic simulations</span><span class="c7">, simulations that realize catastrophic quantities of disvalue.</span><sup class="c7"><a href="#ftnt74" id="ftnt_ref74">[74]</a></sup><span class="c8 c7">&nbsp; Disconcertingly, there are a range of at least somewhat plausible scenarios in which such simulations might be run. &nbsp;Candidates for such simulations include:</span></p><ul class="c10 lst-kix_fakhaabp8xgb-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Catastrophic research</span><span class="c7">: Various sorts of research could be conducted by running simulations containing vast numbers of suffering minds. Candidate research projects in this category include simulating civilizations in order to research catastrophic risks, evolutionary history to investigate evolutionary debunking hypotheses about (say) moral beliefs, or military conflict to gain strategic insight.</span><sup class="c7"><a href="#ftnt75" id="ftnt_ref75">[75]</a></sup><span class="c7">&nbsp; Researchers who ran such simulations to collect frequency data would presumably run large numbers of these simulations. &nbsp;Using simulations to research catastrophic risks could also indirectly heighten them: if such research yielded incorrect conclusions about catastrophic risks, it could lead to misguided mitigation efforts. &nbsp;Similarly, safety testing in future conscious AI systems through large-scale adversarial training might cause immense quantities of suffering or incentivize such systems to cause catastrophes.</span><sup class="c7"><a href="#ftnt76" id="ftnt_ref76">[76]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Catastrophic entertainment</span><span class="c8 c7">: Popular computer games such as StarCraft II and Civilization allow players to control virtual civilizations and direct their members into battle. &nbsp;The lives of such beings are typically brutish and short, with dozens of simulated beings dying in combat in a typical round of gameplay. &nbsp;While the simulated beings in today&rsquo;s games are rudimentary and presumably unconscious, there is no guarantee that they will remain so. &nbsp;And it is easy to see how future games of these sorts that feature conscious simulations (perhaps unbeknownst to their users) could result in enormous quantities of death and suffering.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Catastrophic economies: </span><span class="c7">As simulations become increasingly intelligent, we should expect them to play an increasingly large role in the economy. &nbsp;Should simulated AGIs achieve human</span><span>&nbsp;levels of productivity while also </span><span class="c7">becom</span><span>ing </span><span class="c7">easier and cheaper to produce than humans, we should expect a substantial portion of economic activity to be carried out in virtual reality by artificial workers. &nbsp;While virtual workers could conceivably lead happy virtual lives, there is no reason to think that economic and moral incentives will be aligned on this front&mdash;maximizing productivity might require workers to operate in, say, highly anxious states of extreme focus. &nbsp;To the extent that economic incentives generate pressure toward putting virtual workers in negative states, there is a risk that economic trajectories will lead to catastrophic quantities of suffering.</span><sup class="c7"><a href="#ftnt77" id="ftnt_ref77">[77]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Catastrophic manipulation</span><span class="c7">: </span><span class="c34">Manipulative agents might use catastrophic simulations to render their threats credible or to give other agents prudential reasons to behave in certain ways, e.g. transporting uncooperative or unproductive digital minds into certain types of simulations might be used to incentivize digital minds to cooperate or be productive.</span><sup><a href="#ftnt78" id="ftnt_ref78">[78]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Suffering and subjugated subroutines</span><span class="c7">: Future superintelligent systems may have the capacity to run large-scale simulations. They may exercise this capacity in order to gain information about their options and better pursue their goals. For a wide range of final goals, using simulations in this fashion would prove instrumentally valuable for such systems. &nbsp;For instance, superintelligent systems could run historical simulations to improve their ability to predict human behavior or historical trends. </span><span>Such systems </span><span class="c7">improve their economic standing by creating virtual workers. &nbsp;Or they could run simulations of the future in order to test options before selecting among them. &nbsp;A &ldquo;boxed&rdquo; simulation might run simulations in order to test escape strategies. &nbsp;Given that superintelligent AI systems will relentlessly optimize for whatever their optimization target is and that running large-scale simulations is likely to prove instrumentally valuable for them, we should expect them to run such simulations, even if the moral consequences are catastrophic&mdash;unless we carefully engineer them so as to avoid this hazard. &nbsp;For the same reasons, we should expect them to disregard the rights and interests of minds with</span><span>in such simulations. &nbsp;</span><span class="c7">Both engineering superintelligent systems to avoid </span><span>these </span><span class="c7">hazards and verifying that they are so engineer</span><span>ed</span><span class="c7">&nbsp;may prove difficult. For even systems that behave only in morally permissible ways across a wide range of contexts may harbor large numbers of suffering subroutines. &nbsp;This could happen if, for example, we loaded the system with the values of an otherwise virtuous human who is indifferent to the suffering of subroutines. Or it could happen if the superintelligent system were inadvertently programmed to misattribute unconsciousness to all its subroutines or to misinterpret</span><span>&nbsp;</span><span class="c7">the valence sign or neutral point of its subroutines experiences.</span><sup class="c7"><a href="#ftnt79" id="ftnt_ref79">[79]</a></sup><span class="c7">&nbsp; Given the large quantities of suffering in human and evolutionary history, it is easy to see how superintelligent systems running large-scale simulations could result in catastrophes in the form of many suffering subroutines.</span><sup class="c7"><a href="#ftnt80" id="ftnt_ref80">[80]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Catastrophically malevolent actors</span><span class="c7">: Malevolent actors are those that treat harming other </span><span>individuals </span><span class="c7">as a final end. &nbsp;At present, malevolent actors are severely limited in their ability to create new agents to harm. On an individual level, the procreative limits of human biology and the limited supply of mating opportunities generally precludes malevolent agents from pursuing their ends by creating large numbers of agents. &nbsp;There are examples from history of powerful and plausibly malevolent agents pursuing their ends by promoting social policies that led to the creation of more potential victims.</span><sup class="c7"><a href="#ftnt81" id="ftnt_ref81">[81]</a></sup><span class="c7">&nbsp; In any event, the arrival of simulations capable of realizing suffering minds would greatly enhance malevolent actors&rsquo; abilities to pursue their ends through the creation of potential victims. &nbsp;In addition, biological architectures currently limit the magnitudes and kinds of suffering that malevolent actors can cause. There is no reason to think that these magnitudes or kinds are in the vicinity of the worst. Given the flexibility and information processing potential of digital architectures, there is some reason to think that simulations will enable forms of suffering that are much worse than those that burden biological systems. &nbsp;Finally, today&rsquo;s malevolent actors have incentive to conceal their activities so as not to prompt interference from morally motivated actors. Since large-scale simulations might be run within small portions of physical space or on computers that are externally inscrutable, simulations may enable malevolent actors to more easily conceal moral catastrophes. On the other hand, if such an actor achieved a decisive strategic advantage (i.e. one sufficient to outcompete all </span><span>other actors)</span><span class="c7">, it could realize morally catastrophic ends openly since even actors that were aware of the malevolent actors&rsquo; plans would be unable to thwart them. &nbsp;All this suggests that the combination of large-scale simulations and malevolent actors would pose a significant catastrophic risk.</span><sup class="c7"><a href="#ftnt82" id="ftnt_ref82">[82]</a></sup></li></ul><h1 class="c3 c25" id="h.48gijatv4jk0"><span class="c5">8. Background on the Simulation Hypothesis and the Simulation Argument</span></h1><p class="c3"><span class="c7">Previous sections examined connections between catastrophic risks and simulations that might be run in our universe. &nbsp;The next few sections will explore connections between catastrophic risks and the </span><span class="c7 c17">simulation hypothesis </span><span class="c7">that our universe is itself a simulation. &nbsp;While this may seem to be an outlandish or skeptical hypothesis, there is an interesting argument for it that is taken seriously by relevant experts.</span><sup class="c7"><a href="#ftnt83" id="ftnt_ref83">[83]</a></sup><span class="c7">&nbsp; In this section, I will rehearse the </span><span class="c7 c17">simulation argument </span><span class="c8 c7">for the simulation hypothesis. &nbsp;This will set the stage for discussing connections between the simulation hypothesis and catastrophic risk in later sections.</span></p><p class="c3"><span class="c7">There are two driving ideas behind the simulation argument. &nbsp;One is the broadly empirical claim that the expected motivations and computing potential of technologically advanced civilizations support </span><span class="c7 c17">simulation dominance</span><span class="c7">, the hypothesis that at least a small portion of beings like us will produce a very large number of beings like us that are simulated&mdash;a large enough number for most beings like us to turn out to be simulat</span><span>ions</span><span class="c7">.</span><sup class="c7"><a href="#ftnt84" id="ftnt_ref84">[84]</a></sup><span class="c7">&nbsp; &nbsp;The second is that if most beings like us are simulated, then we are probably simulated. &nbsp;This idea rests on </span><span class="c7 c17">the </span><span class="c7">(</span><span class="c7 c17">bland) indifference principle </span><span class="c7">that we should divide our credence evenly among hypotheses about our self-location in the class of observers like us.</span><sup class="c7"><a href="#ftnt85" id="ftnt_ref85">[85]</a></sup><span class="c8 c7">&nbsp; Simulation dominance and that application of the indifference principle jointly entail that we are probably in a simulation.</span></p><p class="c3"><span>The simulation argument has been spelled out in different ways in the literature. &nbsp;One choice point concerns how to precisify the relevant class of observers: while the indifference principle is intuitive, it is not clear exactly what it takes for observers to be like us. &nbsp;Here, I will finesse this issue by using &lsquo;beings like us&rsquo; to pick out whichever observers fall within the relevant class on the most plausible version of the principle that is applicable to you and me.</span><sup><a href="#ftnt86" id="ftnt_ref86">[86]</a></sup><span>&nbsp; For concreteness, you might think of this as the class of conscious beings with roughly human-level intelligence. &nbsp;In some presentations of the argument, the conclusion is simply that we are probably living in a simulation. &nbsp;In others, the conclusion is couched as a disjunction between our (probably) being in a simulation and possible ways of blocking that result.</span><sup><a href="#ftnt87" id="ftnt_ref87">[87]</a></sup><span class="c8 c7">&nbsp; The literature also contains various proposed amendments for patching the argument in response to objections, along with less consequential variations in formulation. &nbsp;To keep the discussion tractable, I will just work with the above formulation.</span></p><p class="c3"><span class="c7">The argument can be questioned in various ways.</span><sup class="c7"><a href="#ftnt88" id="ftnt_ref88">[88]</a></sup><span class="c8 c7">&nbsp;Some responses include:</span></p><ul class="c10 lst-kix_uo68tg6auowj-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Intelligent simulated beings will not dominate because:</span></li></ul><ul class="c10 lst-kix_uo68tg6auowj-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Civilizations generally go extinct before being able to create intelligent simulated beings.</span></li><li class="c2 li-bullet-0"><span class="c7">Civilizations that can create intelligent simulated beings generally opt not to do so.</span><sup class="c7"><a href="#ftnt89" id="ftnt_ref89">[89]</a></sup></li></ul><ul class="c10 lst-kix_uo68tg6auowj-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Granting that simulated beings will dominate, we should not use the indifference principle to infer that we are probably in a simulation because:</span></li></ul><ul class="c10 lst-kix_uo68tg6auowj-1 start"><li class="c2 li-bullet-0"><span class="c7">The indifference principle is false.</span><sup class="c7"><a href="#ftnt90" id="ftnt_ref90">[90]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">Simulated beings would not be conscious and therefore the indifference principle does not apply.</span><sup class="c7"><a href="#ftnt91" id="ftnt_ref91">[91]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">Some other feature of our evidence (such as our creativity, the fact that we seem to live in an immensely large universe, or the fact that we have not ourselves created simulated beings) indicates that we are not simulated beings, and hence prevents the indifference principle from showing that we are.</span><sup class="c7"><a href="#ftnt92" id="ftnt_ref92">[92]</a></sup></li></ul><ul class="c10 lst-kix_uo68tg6auowj-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">The argument is illicit in some other way:</span></li></ul><ul class="c10 lst-kix_uo68tg6auowj-1 start"><li class="c2 li-bullet-0"><span class="c7">We cannot have evidence that both indicates that there are many simulations in our universe and that we are simulations.</span><sup class="c7"><a href="#ftnt93" id="ftnt_ref93">[93]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">Evidence that we are in a simulation is unstable&mdash;in that it is trustworthy just in case we rationally take ourselves not to be in a simulation&mdash;and so should be ignored.</span><sup class="c7"><a href="#ftnt94" id="ftnt_ref94">[94]</a></sup></li><li class="c2 li-bullet-0"><span class="c8 c7">The simulation hypothesis is a skeptical one. Therefore we should reject the simulation argument even if we do not know where it goes wrong.</span></li></ul><p class="c3"><span class="c8 c7">This is not the place to attempt a comprehensive evaluation of the simulation argument and responses to it. &nbsp;So I will restrict myself to the following remarks, which aim to (1) address what I expect to be some of the more prevalent concerns about the argument and (2) highlight connections between some of the responses and catastrophic risks.</span></p><p class="c3"><span class="c7">First, consider the response that the argument fails because civilizations generally go extinct before being able to create intelligent simulated beings&mdash;for example, as a result of a technology that inevitably precedes simulation technology and leads to civilizational destruction shortly after its discovery.</span><sup class="c7"><a href="#ftnt95" id="ftnt_ref95">[95]</a></sup><span class="c8 c7">&nbsp; If this response is correct, then it gives us reason to think that our civilization will end before being able to create intelligent simulated beings. &nbsp;Thus, if the response is correct and there is reason to think we are on track to be able to create intelligent simulated beings, then there is also reason to think our civilization will succumb to catastrophe in the relatively near term&mdash;a catastrophe that at least knocks it off track, whether or not it terminates our civilization. &nbsp;This can be understood as an argument for taking near-term catastrophic risks more seriously.</span></p><p class="c3"><span class="c7">Second, if the simulation argument fails because advanced civilizations generally decide not to create intelligent simulated beings, this may be because civilizations that avoid catastrophe long enough to be able to produce such simulations are generally very risk averse.</span><sup class="c7"><a href="#ftnt96" id="ftnt_ref96">[96]</a></sup><span class="c7">&nbsp; There is also reason to think such civilizations would have centralized control and/or coordination schemes with near-universal compliance: absent such control or compliance, we would expect sub-civilizational actors to create many intelligent simulations, given that intelligent simulations would eventually become cheap to produce in advanced civilizations.</span><sup class="c7"><a href="#ftnt97" id="ftnt_ref97">[97]</a></sup><span class="c8 c7">&nbsp; Similarly, those who are attracted to this response may take it as an indication of the paths that are available to humanity which do not end in near-term extinction.</span></p><p class="c3"><span class="c8 c7">Third, in informal interactions I&#39;ve encountered a number of fellow philosophers who are tempted by the instability response. However, for several reasons, this response is unconvincing:</span></p><ul class="c10 lst-kix_diamaa70m7ux-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">Any instability induced by the simulation argument afflicts both the simulation hypothesis and its negation. Symmetry suggests that the correct moral of such instability cannot be that we should simply </span><span>reject </span><span class="c8 c7">the simulation hypothesis.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">The argument can be recast in terms of intelligent beings in </span><span class="c7 c17">stable simulations</span><span class="c7">, simulations of scenarios that are similar enough to the level of reality at which the simulations are run for the simulated beings to be able to reliably reason from their evidence, at least for the reasoning in the simulation argument.</span><sup class="c7"><a href="#ftnt98" id="ftnt_ref98">[98]</a></sup><span class="c7 c8">&nbsp; While the recast argument would need to be evaluated on its own merits, it is not clear that such recasting introduces any serious flaws.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">We should not in general disregard unstable evidence. &nbsp;This is vividly illustrated through real-life cases involving </span><span class="c7 c17">hypoxia</span><span class="c7">,</span><sup class="c7"><a href="#ftnt99" id="ftnt_ref99">[99]</a></sup><span class="c7">&nbsp;a condition involving oxygen deprivation and impaired reasoning abilities: any reasoning that leads to the conclusion that one is hypoxic is unstable, since it casts doubt on itself. &nbsp;Yet it would be foolish for mountain climbers and pilots to ignore the hypothesis that they are hypoxic on that basis&mdash;indeed, despite its instability, such reasoning plausibly sometimes gives people a reason to take that hypothesis seriously.</span><sup class="c7"><a href="#ftnt100" id="ftnt_ref100">[100]</a></sup><span class="c8 c7">&nbsp; Absent some reason for thinking that any instability associated with the simulation argument is relevantly different from the instability associated with arguments for one&rsquo;s being hypoxic, the simulation argument should not be dismissed on the ground that it is unstable. &nbsp;</span></li></ul><p class="c3"><span class="c8 c7">Another response that many people find tempting is to dismiss the simulation argument on the ground that it is a skeptical argument. &nbsp;For several reasons, this response is also unconvincing.</span></p><ul class="c10 lst-kix_dfzkz4v3pthk-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">Whereas typical skeptical arguments appeal to mere possibilities to cast doubt on what we take ourselves to know,</span><sup class="c7"><a href="#ftnt101" id="ftnt_ref101">[101]</a></sup><span class="c8 c7">&nbsp;the simulation argument uses broadly empirical considerations to support positive conclusions, namely simulation dominance and, in turn, the simulation hypothesis. &nbsp;(The susceptibility of the simulation hypothesis to empirical (dis)confirmation also distinguishes it from conspiracy theories that resist (dis)confirmation.)</span></li><li class="c3 c9 li-bullet-0"><span class="c7">The simulation hypothesis is arguably a metaphysical hypothesis about the origins and underlying nature of our universe.</span><sup class="c7"><a href="#ftnt102" id="ftnt_ref102">[102]</a></sup><span class="c8 c7">&nbsp; Traditional theistic hypotheses hold that our universe was created by an agent and are typically regarded as non-skeptical. But it is hard to see how the sort of creation of our universe suggested by the simulation argument could engender skepticism when traditional theistic hypotheses lack skeptical import. &nbsp;Similarly, non-skeptical theories in physics posit underlying computational processes to explain the observable universe. &nbsp;But it is hard to see how the sort of underlying nature of our universe suggested by the simulation argument could engender skepticism when these physical theories do not.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Finally, it should be borne in mind that we are not faced with a choice between embracing the simulation hypothesis and accepting that reality is largely as it appears: even on the assumption that we are not in a simulation, science and philosophy give us good reasons for thinking that the external world is not as our experience presents it, whether or not there is a watered-down sense in which our experiences can be said to be accurate.</span><sup class="c7"><a href="#ftnt103" id="ftnt_ref103">[103]</a></sup></li></ul><h1 class="c3 c25" id="h.sjkkvkv9bvps"><span class="c5">9. Shutdown Risk</span></h1><p class="c3"><span class="c7">One corollary of the simulation hypothesis is that our simulation may be shut down.</span><sup class="c7"><a href="#ftnt104" id="ftnt_ref104">[104]</a></sup><span class="c8 c7">&nbsp; Depending on the axiological trajectory of our universe at the time of shutdown, shutdown could be catastrophic. If a shutdown happened today, it would prematurely end the lives of billions of people and it might destroy immense quantities of expected value that lie in our potential to usher in a grand future. &nbsp;Shutdowns will continue to pose a catastrophic risk, at least as long as we manage to steer clear of other catastrophes.</span></p><p class="c3"><span class="c7">It may be tempting to think: we shouldn&rsquo;t worry about catastrophic shutdowns because there&rsquo;s no way for us to </span><span>influence whether they occur</span><span class="c7">. &nbsp;However, this thought is mistaken on two counts. &nbsp;First, even if we cannot </span><span>influence </span><span class="c7">the risk of catastrophic shutdowns, that risk has implications for the expected value of the long-term future: &nbsp;if we assign a tiny but constant probability to shutdown in any given year (conditional on survival up to that point), that will drive down the expected (dis)value of outcomes that would occur further in the future.</span><sup class="c7"><a href="#ftnt105" id="ftnt_ref105">[105]</a></sup><span class="c8 c7">&nbsp; Such discounting would drive down the expected (dis)value of nearer term future to a lesser extent. &nbsp;Thus, updates that elevate the risk of catastrophic shutdowns will tend to weaken the case for prioritizing the far future.</span></p><p class="c3"><span class="c8 c7">Second, there are shutdown triggers that we may be able to influence:</span></p><ul class="c10 lst-kix_d16v4uvczqej-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Excess computational consumption</span><span class="c7">:</span><sup class="c7"><a href="#ftnt106" id="ftnt_ref106">[106]</a></sup><span class="c8 c7">&nbsp;programs are often coded to halt under certain conditions rather than exhaust the computational resources of the systems on which they are run. &nbsp;In the event that we are living in a simulation and our simulators have finite computing resources, there is reason to think they would also program our simulation in an efficient manner&mdash;e.g. by not simulating microphysical details that lack observational import for simulation inhabitants&mdash;and incorporate such halting mechanisms. &nbsp;In that case, we might trigger shutdown by way of activities that require large quantities of compute and therefore risk engaging halting mechanisms.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Moral triggers</span><span class="c8 c7">: Simulators might shut down simulations for moral reasons. Since shutdown would itself be an existential catastrophe, moral criteria for triggering it would presumably concern some other moral bad such as suffering. &nbsp; This hypothesis suggests a way in which extinction risk might be higher and suffering risks lower than we&rsquo;d otherwise think.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Extinguishing superintelligent threats</span><span class="c8 c7">: There is an open question in AI safety research about whether superintelligent systems could be safely confined to a virtual environment&mdash;a crucial concern is that such systems might use their superintelligence to find clever escapes from their virtual confinement that we are not smart enough to anticipate and thwart. &nbsp;Simulators of our universe might share this concern about superintelligent systems and address it by shutting down simulations when superintelligent systems are created or become likely to emerge within the simulation. For this reason, creating superintelligent systems would raise the shutdown risk. &nbsp;The same goes for making progress toward creating such systems. &nbsp;Of course, creating superintelligent systems might mitigate other catastrophic risks, or even mitigate shutdown risk in a different way.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Interest triggers</span><span class="c7">: If we are in an entertainment or research simulation, we should expect the probability of shutdown to increase if our universe loses its research or entertainment value.</span><sup class="c7"><a href="#ftnt107" id="ftnt_ref107">[107]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Defeat in victory</span><span class="c8 c7">: If we are in a gaming simulation, meeting a victory condition for the game could result in shutdown.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Simulation awareness</span><span class="c7">: Finding out that we&rsquo;re in a simulation or in a certain sort of simulation might defeat the purpose of some sorts of simulations&mdash;e.g. Fermi research simulations&mdash;leading to shutdown.</span><sup class="c7"><a href="#ftnt108" id="ftnt_ref108">[108]</a></sup><span class="c8 c7">&nbsp;</span></li></ul><p class="c3"><span class="c7">Developing a more systematic understanding of potential shutdown triggers and the prospects for avoiding them is an underexplored topic.</span><sup class="c7"><a href="#ftnt109" id="ftnt_ref109">[109]</a></sup><span class="c8 c7">&nbsp;</span></p><h1 class="c3 c25" id="h.8ccunbsslpod"><span class="c27 c33 c21">10. Potential Upsides of Shutdow</span><span class="c27 c33 c21">n</span></h1><p class="c3"><span class="c8 c7">The previous subsection noted some obvious reasons for thinking that the shutdown of our simulation might be catastrophic. &nbsp;What may be less obvious is that there are also ways in which shutting down our simulation could mitigate catastrophic risks. &nbsp;These include:</span></p><ul class="c10 lst-kix_c8p3xgurtjdg-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Comparative advantage</span><span class="c8 c7">: In evaluating negative effects of shutdown, we should be mindful of the alternatives, as shutdown&rsquo;s negative effects may be unavoidable and it may turn out that alternatives would be morally worse.</span></li></ul><ul class="c10 lst-kix_c8p3xgurtjdg-1 start"><li class="c2 li-bullet-0"><span class="c7">It is tempting to think that shutdown is a catastrophe to be avoided because it would result in billions of deaths and the cessation of conscious beings in our universe. &nbsp;However, the available alternatives share these consequences&mdash;what is at stake with shutdown is how our universe will end, not whether it will cease realizing value after some time or other.</span><sup class="c7"><a href="#ftnt110" id="ftnt_ref110">[110]</a></sup></li><li class="c2 li-bullet-0"><span class="c8 c7">Similarly, one might think that shutdown is a catastrophe to be avoided because it would cut short many lives and projects whose continuation would be valuable. &nbsp;However, it is not clear that there is any realistic alternative available that would avoid this. &nbsp;Indeed, if shutdown is avoided, we would expect there to be a last generation of conscious beings whose lives and projects will be cut short&mdash;the same goes for conscious beings belonging to other future generations that will exist only if shutdown is avoided.</span></li><li class="c2 li-bullet-0"><span class="c7">A more plausible thought is that </span><span class="c7 c17">we</span><span class="c7">&nbsp;have reason to prevent shutdown because it would cut short </span><span class="c7 c17">our </span><span class="c7">lives and projects. &nbsp;While this point is well taken as one of prudence, it is doubtful that it has any purchase from a perspective of impartial benevolence: from that </span><span>moral vantage point</span><span class="c7">, there seems to be no reason for thinking it would be worse for us to be subjected to shutdown than it would be for other people (perhaps much larger in number, with longer and higher quality </span><span>lives</span><span class="c7">) to have their lives and projects prematurely terminated.</span><sup class="c7"><a href="#ftnt111" id="ftnt_ref111">[111]</a></sup></li><li class="c2 li-bullet-0"><span class="c7">Suppose that the axiological trajectory of our universe is negative: by default, the expected value of the future is negative&mdash;perhaps because we have created digital minds that endure kinds of suffering that cannot be compensated by any sort of good or perhaps because probes are launched </span><span>to spread </span><span class="c7">simple forms of life across the galaxy, setting the stage for the repetition of the horrors wrought by biological evolution on Earth.</span><sup class="c7"><a href="#ftnt112" id="ftnt_ref112">[112]</a></sup><span class="c7">&nbsp; In such a case, the default outcome might well be worse than shutdown, even if shutdown itself has negative expected value. If so, then triggering shutdown might serve as a kind of </span><span class="c7 c17">escape hatch </span><span class="c8 c7">from a world gone wrong. &nbsp;Indeed, even if one assigned only a small probability to the simulation hypothesis and a still smaller one to triggering attempts inducing shutdown, trying to bring about shutdown might be worth attempting.</span></li></ul><ul class="c10 lst-kix_c8p3xgurtjdg-0"><li class="c3 c9 li-bullet-0"><span class="c7">Suppose that in expectation our universe&rsquo;s future will contain more good than bad. &nbsp;Even in this case, there might be decisive moral reasons in favor of shut down as a result of </span><span class="c7 c17">goods discounting</span><span class="c7">.</span></li></ul><ul class="c10 lst-kix_c8p3xgurtjdg-1 start"><li class="c2 li-bullet-0"><span class="c7">For example, maybe a </span><span class="c7 c17">downside-focused </span><span class="c7">moral theory is</span><span>&nbsp;correct</span><span class="c7">.</span><sup class="c7"><a href="#ftnt113" id="ftnt_ref113">[113]</a></sup><span class="c8 c7">&nbsp; Such a theory would place greater weight on preventing negative outcomes rather than on bringing about positive ones. &nbsp;As a result, the negative outcomes averted by shutdown (astronomical quantities of suffering, say) could justify triggering it, even if doing so would prevent positive outcomes (even larger quantities of happiness, say) that are better than the negative outcomes are bad.</span></li><li class="c2 li-bullet-0"><span class="c7">Or maybe </span><span>the correct moral theory</span><span class="c7">&nbsp;assigns </span><span class="c7 c17">asymmetrically diminishing returns</span><span class="c8 c7">&nbsp;such that the amount of positive value realized per unit of goods decreases as the number of units of goods increases but the amount of negative value realized per unit of bads does not decrease (by as much) as the number of units of bad increases. &nbsp;In this case, we cannot simply read off the value of bringing about a given future in our universe from the goods and bads it contains: to calculate the value of a given future, we would need to discount the goods it contains in accordance with the relevant aggregation function. &nbsp;Thus, even if a given future of our universe would contain more good than bad, bringing it about might contribute net disvalue once the discounting of the goods is taken into account.</span></li></ul><ul class="c10 lst-kix_c8p3xgurtjdg-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">Views that embrace asymmetrically diminishing returns lend support to trying to trigger shutdown as follows: either we are in a simulation or we are not.</span><sup class="c7"><a href="#ftnt114" id="ftnt_ref114">[114]</a></sup><span class="c8 c7">&nbsp;If we are not, then we cannot trigger shutdown and there is little downside to trying to do so. &nbsp;If we are in a simulation, the world probably contains vastly many goods beyond those featured in our universe. &nbsp;In that case, conditional on asymmetrically diminishing returns, we should expect the value of any goods we can bring about to be severely discounted and the disvalue of any bads we can bring about not to be so discounted. &nbsp;Thus, to the extent that, in comparison with our other options, triggering shutdown trades off the realization of goods with the prevention of bads, asymmetrically diminishing returns should push us toward assigning higher expected value to triggering shutdown.</span></li></ul><p class="c3"><span class="c7">I hasten to add: even in circumstances where evidence indicates that the best option will involve trying to trigger shutdown, it could be a grave error to attempt to trigger shutdown too soon. &nbsp;That&rsquo;s because there&rsquo;s value in delaying&mdash;and preserving other options in the meantime&mdash;in order to put us in a better epistemic position to determine whether shutdown is the best option.</span><sup class="c7"><a href="#ftnt115" id="ftnt_ref115">[115]</a></sup><span class="c7">&nbsp; And that value could easily outweigh any time cost associated with delay. &nbsp;Compare: suppose you&rsquo;re on a slowly leaking ship that will probably sink. &nbsp;Even if your evidence indicates that jumping overboard with a life vest is your best option, that doesn&rsquo;t mean you should take it immediately. It might instead be more reasonable to wait to see how the situation evolves in case a better option emerges (e.g. staying on the ship in the event that the leak is fixed or exiting in a lifeboat)</span><span class="c7">.</span><sup class="c7"><a href="#ftnt116" id="ftnt_ref116">[116]</a></sup></p><h1 class="c3 c25" id="h.wrznkfv6s0su"><span class="c5">11. The Simulation Argument and Religious Catastrophic Risks</span></h1><p class="c3"><span>Traditional religions countenance distinctive catastrophic risks ranging from apocalypses inflicted by a creator to afterlives involving eternal torment. &nbsp;(Distinctively) religious catastrophic risks are rarely discussed in the scholarly literature on catastrophic risks. The same goes for public non-academic discussions of catastrophic risks that I am familiar with.</span><sup><a href="#ftnt117" id="ftnt_ref117">[117]</a></sup><span>&nbsp;Perhaps this is because such risks are typically approached with methods that do not treat these religions&mdash;or any religion for that matter&mdash;as a source of data, much less as authoritative. &nbsp;In any event, given the enormous stakes associated with religious catastrophic risks, they should be considered in analyzing how to reduce catastrophic risk unless we regard religious catastrophes as astronomically unlikely.</span><sup><a href="#ftnt118" id="ftnt_ref118">[118]</a></sup><span>&nbsp;For those who regard religious catastrophic risks as non-negligible, there is then a question as to how they interact with the simulation argument.</span><sup><a href="#ftnt119" id="ftnt_ref119">[119]</a></sup><span class="c8 c7">&nbsp; There are at least four connections of interest.</span></p><p class="c3"><span class="c7 c17">Flawed simulators and religious catastrophes</span><span class="c7">. &nbsp;Outside the context of the simulation argument, it is often assumed that our universe either was not created or else it was created by an agent that is all powerful, all knowing, and perfectly good. &nbsp;The simulation argument casts doubt on this assumption: it gives reasons for thinking that our universe is created by intelligent beings, but provides no reason to think such beings would exhibit a maximal degree of power, knowledge, or goodness. &nbsp;Further, taken with the many morally problematic features of our universe, the simulation argument lends support to the hypothesis that our universe was created by a morally flawed or morally indifferent agent. &nbsp;This in turn lends support to the hypothesis that our universe is subject to religious catastrophic risks such as catastrophic interventions by our creator(s) or disvaluable afterlives.</span><sup class="c7"><a href="#ftnt120" id="ftnt_ref120">[120]</a></sup></p><p class="c3"><span class="c7 c17">Simulation as a solution to the problem of natural evil. &nbsp;</span><span class="c7">The simulation argument arguably indirectly supports the hypothesis that God exists&mdash;this is so even on the assumption that our universe was, if created, created by a non-divine simulator. For the simulation hypothesis offers a candidate solution to the </span><span class="c7 c17">problem of natural evil</span><span class="c8 c7">, i.e. the problem of reconciling the apparent existence of natural evils (ones not caused by the free choices of agents) with the existence of God (understood as an agent that is all powerful, all knowing, and perfectly good). &nbsp;The candidate solution on offer is that the appearance of natural evils is an illusion: God created a world devoid of natural evils in which agents make free choices. &nbsp;One of those agents freely chose to create a simulation, a simulation that turns out to be our universe. &nbsp;Thus, what appear to be natural evils in our universe are really evils caused by our simulator, not by God.</span></p><p class="c3"><span class="c7 c17">Extending the solution to help with other problems for theism</span><span class="c7">. &nbsp;The simulation solution to the problem of natural evil can be developed to solve a range of other problems for theism. &nbsp;For suppose we add to the solution that our universe is just one simulation in a much larger reality of which it is, in a certain respect, not representative.</span><sup class="c7"><a href="#ftnt121" id="ftnt_ref121">[121]</a></sup></p><ul class="c10 lst-kix_44ecm3cp0gq8-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">By supposing that the ratio of good to evil in our universe is much worse than the ratio between good and evil in the larger world to which it belongs, we can extend the solution to help with </span><span class="c7 c17">the problem of evil</span><span class="c7">, i.e. the problem of reconciling the evil we find with the existence of God.</span><sup class="c7"><a href="#ftnt122" id="ftnt_ref122">[122]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7">By supposing that our universe is sub-optimal but part of an optimal world, we can extend the solution to help with </span><span class="c7 c17">the sub-optimality</span><span class="c7">&nbsp;</span><span class="c7 c17">problem</span><span class="c8 c7">, i.e. the problem of reconciling the sub-optimality of what we observe with the expectation that God would create an optimal world.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">By supposing that our universe is uncharacteristically inefficient in realizing value, we can extend the solution to help with </span><span class="c7 c17">the problem of scale</span><span class="c7">, i.e. the problem of reconciling the expectation that God&rsquo;s moral goodness would be reflected in the organization of his creation with the observation that such value seems to be realized at great inefficiency at a cosmically miniscule scale.</span><sup class="c7"><a href="#ftnt123" id="ftnt_ref123">[123]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7">By supposing that beings whose evidence is not manipulated through a simulator&rsquo;s exercise of free will have ample evidence of God&rsquo;s existence, we can extend the solution to help with </span><span class="c7 c17">the problem of divine hiddenness</span><span class="c7">, i.e. the problem of explaining why the available evidence for theism is weaker than we would have expected if theism were true.</span><sup class="c7"><a href="#ftnt124" id="ftnt_ref124">[124]</a></sup></li></ul><p class="c3"><span class="c7">Simulation solutions to problems for theism have implications for catastrophic afterlives. &nbsp;On the one hand, there is some plausibility to the thought that we are less likely to have the sort of free will required for responsibility if we are simulated than if we are not. &nbsp;This in turn suggests that, on the simulation solution, we are less likely to be subject to catastrophic afterlives imposed by God as just punishment. &nbsp;On the other hand, on simulation solutions, we need to distinguish between afterlives imposed by God and those imposed by the non-divine simulator(s) of our universe. &nbsp;Thus, there remains the possibility that we are in a simulation and will be subjected to a catastrophic afterlife by the non-divine simulator(s). &nbsp;The simulation solution somewhat constrains how bad such an afterlife could be: on pain of reinvigorating the problem of evil, the catastrophic afterlives cannot be so bad (or too inadequately compensated)</span><sup class="c7"><a href="#ftnt125" id="ftnt_ref125">[125]</a></sup><span class="c8 c7">&nbsp;as to render the existence of God implausible.</span></p><p class="c3"><span class="c7 c17">Simulations, fine-tuning, and God</span><span class="c7">. &nbsp;Cosmological fine-tuning arguments for the existence of God are among the most popular contemporary arguments for the existence of God. &nbsp;They appeal to the (supposed) fact that fundamental physical parameters take values within narrow ranges required for life.</span><sup class="c7"><a href="#ftnt126" id="ftnt_ref126">[126]</a></sup><span class="c7">&nbsp; This is claimed to be expected if there is a designer of our universe but not otherwise&mdash;the idea is that it wouldn&rsquo;t be surprising if a designer selected those parameters because they are required for life and the designer wanted to create a universe with life (or something for which life is a prerequisite such as biological intelligence or conscious organisms). &nbsp;It&rsquo;s usually assumed that if our universe has a designer, the designer would be God. &nbsp;Insofar as cosmological fine-tuning arguments support the hypothesis that God exists, they presumably also modulate the catastrophic risks associated with the existence of God. &nbsp;The simulation argument bears on cosmological fine-tuning in several respects.</span><sup class="c7"><a href="#ftnt127" id="ftnt_ref127">[127]</a></sup></p><ul class="c10 lst-kix_vrq4ps2bndej-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">The simulation argument casts doubt on the assumption that if our universe has a designer, it&rsquo;s God. &nbsp;It does this by pointing to an alternative: the fine-tuner of our universe might be the intelligent but non-divine being running our simulation. &nbsp;To the extent that fine-tuning evidence and the simulation argument together support this non-divine design hypothesis, the simulation argument constrains how much fine-tuning evidence can support the existence of God&mdash;the simulation argument in effect redirects support from fine-tuning for theism to a non-theistic design hypothesis.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">A more standard response to fine-tuning arguments is that they neglect the availability of a multiverse explanation of fine-tuning. &nbsp;Multiverse explanations posit a vast ensemble of universes with varying physical parameter values such that it is to be expected that some universe has life-supporting parameter values. &nbsp;One worry about the multiverse explanation is that it evidently requires many universes with different fundamental laws, and thus suffers the theoretical vice of having an immensely complicated set of basic laws.</span><sup class="c7"><a href="#ftnt128" id="ftnt_ref128">[128]</a></sup><span class="c8 c7">&nbsp;The simulation argument lends to the following response to this worry: an ensemble of simulation-universes explains fine-tuning; however, these simulations are non-fundamental entities within the universe in which our simulation is run; that universe may well have a simple set of fundamental laws; thus, the multiverse explanation does not require the proliferation of fundamental laws.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">It might be thought that cosmological fine-tuning arguments for the existence of God (or a multiverse with different fundamental laws in different universes) could be recast to concern the level of reality in which our simulation is run, if we are in a simulation.</span><sup class="c7"><a href="#ftnt129" id="ftnt_ref129">[129]</a></sup></li></ul><ul class="c10 lst-kix_vrq4ps2bndej-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">One problem with this thought is that since we lack access to the physics of any such universe, we lack the relevant empirical data needed to run the argument.</span></li><li class="c2 li-bullet-0"><span class="c7">It might be replied: the most likely scenario in which we are in a simulation is one in which (in accordance with the simulation argument) our simulation is the product of observers like us. But observers like us would tend to exist in situations with physics like our own (navigating a world with radically different physics isn&rsquo;t compatible with being an observer like us). And, being most interested in and capable of designing observers like themselves, such observers would tend to simulate observers in situations with physics like our own. All this suggests that if we are in a simulation, then our physics is similar to the physics of the level at which our simulation is run</span><span>&nbsp;and hence that </span><span class="c7">cosmological fine-tuning arguments </span><span>which </span><span class="c8 c7">initially seemed to apply to our universe will at least apply to whatever unsimulated universe we inhabit, even if we happen to inhabit a simulation.</span></li><li class="c2 li-bullet-0"><span class="c7">One limitation of this reply is that if the physics of the universe in which our simulation is run is like the physics of our universe, it is reasonable to expect the simulation argument to apply to our simulators. In that case, just as the simulation argument threatens cosmological fine-tuning arguments that invoke fine-tuning in our universe, so too would the simulation argument threaten cosmological fine-tuning arguments that invoke fine-tuning in our simulators&rsquo; universe. &nbsp;The reply could be repeated for our simulator&rsquo;s level to try to show that their simulator would also have a physics that is similar to our own. &nbsp;However, even if the reply is plausible at each level, the more times it is iterated, the more likely it is that it will fail at some level.</span><sup class="c7"><a href="#ftnt130" id="ftnt_ref130">[130]</a></sup><span class="c8 c7">&nbsp; One way to see this is to notice that small differences between simulator and simulatee physics at adjacent levels can add up to big differences in the descent from our level to the basement level. &nbsp;A big difference could well be that whereas our simulation&rsquo;s physics is fine-tuned, basement level physics is not.</span></li></ul><p class="c3"><span class="c8 c7">We&rsquo;ve seen that the simulation argument in different ways supports theism and undermines such support. Likewise, we&rsquo;ve seen that the simulation argument in different ways boosts and diminishes religious catastrophic risks. &nbsp;The net effects of the simulation argument on these issues remain open questions.</span></p><h1 class="c3 c25" id="h.bujvg7fqjzm"><span class="c5">12. The Simulation Argument, Self-Location, and Catastrophic Risk</span></h1><h2 class="c3 c25" id="h.l03611jelt5j"><span class="c5">12.1 Background on Self-Location</span></h2><p class="c3"><span class="c7">The simulation argument relies on self-locating information: it assumes you should divide your credence evenly among observers like yourself. There are other arguments that also rely on self-locating information and bear on catastrophic risks. &nbsp;The simulation argument interacts with these arguments in ways that bear on catastrophic risks. &nbsp;This section will describe some of these arguments and interactions. &nbsp;I should flag that there is much controversy within the literature on how to rationally respond to self-locating information and that much of the literature is of a technical nature that I cannot adequately summarize here.</span><sup class="c7"><a href="#ftnt131" id="ftnt_ref131">[131]</a></sup><span class="c8 c7">&nbsp; I expect that much of the discussion that follows will merit revisiting in a more technical setting and much of what I say in this subsection would need to be qualified or scrapped in light of reassessment. &nbsp;I therefore offer the discussion that follows in a provisional spirit in hopes that it will stimulate further work on the topic, even if what I say turns out to be misguided in important ways.</span></p><p class="c3"><span class="c8 c7">Before diving into the arguments, I&rsquo;ll use a simple example to put some of the key issues about self-location on the table: suppose an urn contains an unknown number of balls and that you and perhaps some other subjects are each going to draw one ball from. &nbsp;Next, suppose you find out that most subjects who take a ball from the urn draw a red ball. &nbsp;Intuitively, this gives you reason to think that you will probably draw a red ball. &nbsp;This is an instance of &lsquo;inward&rsquo; reasoning that moves from information about a distribution of subjects to a conclusion about yourself&mdash;this is the sort of reasoning encoded in the indifference principle invoked by the simulation argument.</span></p><p class="c3"><span class="c7">In contrast, &lsquo;outward reasoning&rsquo; moves from information about yourself to a conclusion that concerns the distribution of subjects like yourself.</span><sup class="c7"><a href="#ftnt132" id="ftnt_ref132">[132]</a></sup><span class="c7">&nbsp; To illustrate, suppose that you are initially ignorant about how many subjects will draw from the urn and what it contains. &nbsp;You then draw a red ball. &nbsp;Here, two inferences are prima facie plausible. First, you might take the fact that you drew a red ball to boost the expected </span><span class="c7 c17">number</span><span class="c7">&nbsp;of subjects who will draw red balls. &nbsp;After all, you presumably initially reserved credence for the hypothesis that no subject would draw a red ball. &nbsp;You&rsquo;ve now eliminated that hypothesis and presumably redistributed whatever you had in it to hypotheses on which more than one subject draws a red ball. &nbsp;Call this </span><span class="c7 c17">number boosting</span><span class="c7">. &nbsp;Second, you might take the fact that you drew a red ball to boost the expected </span><span class="c7 c17">proportion </span><span class="c7">of subjects who will draw red balls. &nbsp;After all, the higher proportion of subjects who draw red balls, the more likely it is that you will draw a red ball, in which case those hypotheses get a boost. &nbsp;Call this </span><span class="c7 c17">proportion boosting</span><span class="c8 c7">.</span></p><p class="c3"><span class="c7">A final variation: you are initially ignorant about what the urn contains, except that you know that it contains turquoise balls and that they&mdash;unlike balls of any other color the urn contains&mdash;are all too deep in the urn for any subject to reach. &nbsp;In this case, draws cannot be treated as representative of the urn&rsquo;s contents: they exhibit a </span><span class="c7 c17">sampling bias</span><span class="c7">. &nbsp;For example, drawing a red ball rather than a turquoise ball is not evidence against the urn containing turquoise balls. &nbsp;In this case, the sampling bias is an </span><span class="c7 c17">observation selection effect</span><span class="c8 c7">: observations do not qualify as random samples because observations are biased toward certain outcomes.</span></p><p class="c3"><span class="c7">In practice, factoring in number boosting, proportion boosting, and observation selection effects raises a host of difficult issues, e.g. concerning the individuation of reference classes.</span><sup class="c7"><a href="#ftnt133" id="ftnt_ref133">[133]</a></sup><span class="c7">&nbsp; A case in point is the bearing of number boosting and proportion boosting on the simulation argument. &nbsp;Insofar as number boosting favors hypotheses with more observers regardless of their observer-type,</span><sup class="c7"><a href="#ftnt134" id="ftnt_ref134">[134]</a></sup><span class="c8 c7">&nbsp;it will favor hypotheses on which the world contains large numbers of observers in simulations over non-simulation hypotheses on which the world is relatively sparsely populated. &nbsp;And insofar as number boosting favors hypotheses with more observers like us, it will favor large-scale simulation hypotheses on which many beings like us are simulated over sparsely populated non-simulation hypotheses. &nbsp;Thus, number boosting arguably bolsters the simulation argument&rsquo;s simulation dominance premise that at least a small portion of beings like us will create a large number of beings like us. &nbsp;In contrast, insofar as proportion boosting favors hypotheses on which a higher proportion of beings are observers of some type or other, it will favor panpsychist views over views on which the universe is relatively sparsely populated with macroscopic subjects (whether simulated or not). On the other hand, insofar as proportion boosting favors hypotheses on which a higher proportion of observers are observers like us, it will favor simulation hypotheses on which enough beings like us are simulated for there to be proportionally more beings like us than there are on rival simulation and non-simulation hypotheses. &nbsp;The latter result also fits with the simulation argument&rsquo;s simulation dominance premise, though that premise does not require that there be a higher proportion of simulated beings like us than there are other sorts of observers. &nbsp;</span></p><p class="c3"><span class="c7">It is not obvious that the individuation of reference class implicit in the foregoing applications of number boosting and proportion boosting are correct. &nbsp;That said, it should be borne in mind that number and proportion boosting may operate across multiple reference classes even within a single case. &nbsp;Indeed, the foregoing applications are all mutually compatible.</span><sup class="c7"><a href="#ftnt135" id="ftnt_ref135">[135]</a></sup><span class="c7">&nbsp; In what follows, rather than trying to settle the proper individuation of reference classes and what applications of number and proportion</span><span>&nbsp;boosting</span><span class="c8 c7">&nbsp;are legitimate, I will instead focus on drawing out implications of different choices on these scores.</span></p><h2 class="c3 c25" id="h.87slt66p576n"><span class="c5">12.2 Evolutionary Arguments for Easy Artificial Intelligence</span></h2><p class="c3"><span class="c7">Some authors have appealed to the observation that evolution by natural selection produced human intelligence to promote optimism about our ability to engineer artificial systems with human-level intelligence.</span><sup class="c7"><a href="#ftnt136" id="ftnt_ref136">[136]</a></sup><span class="c8 c7">&nbsp; Pre-empirically, it&rsquo;s not obvious that a world with our physics and chemistry could give rise to human-level intelligence at all, much less that it could feature beings with such intelligence that would be able to engineer systems with such intelligence. &nbsp;Thus, finding out that evolution produced human-level intelligence eliminates one obstacle&mdash;that of incompatibility with the laws of nature&mdash;to our engineering such systems and so provides at least a smidgen of support for our capacity to achieve this engineering feat.</span></p><p class="c3"><span class="c7">However, such optimism does not go beyond whatever optimism is licensed by the observation that physical and chemical processes in our world somehow gave rise to humans. The more interesting sort of evolutionary arguments contend that the fact that evolutionary processes </span><span class="c7 c17">not aiming for intelligence</span><span class="c7">&nbsp;gave rise to human intelligence </span><span class="c7 c17">during the relatively short time </span><span class="c7">(by cosmological standards) that Earth has existed suggests that engineering such intelligence is not that difficult of a problem. &nbsp;The idea can be spelled out in terms of design space: despite being a very inefficient search procedure relative to those that are available to human engineers, evolution by natural selection managed to hit upon human-level intelligence in design space; so, given our superior search capabilities, we should be able to hit upon it without great difficulty.</span><sup class="c7"><a href="#ftnt137" id="ftnt_ref137">[137]</a></sup></p><p class="c3"><span class="c8 c7">If this argument succeeds in reducing the expected difficulty of engineering human-level intelligence, it thereby bolsters the simulation argument by rendering it more plausible that civilizations like ours will be able to create simulated beings like us before going extinct. &nbsp;</span></p><p class="c3"><span class="c7">Rendering it more plausible that we will be able to engineer human-level intelligence also renders it more plausible that we will be able to&mdash;and, indeed, will&mdash;engineer superintelligent AI systems. &nbsp;That outcome would itself bear on catastrophic risks, as superintelligent AI both poses catastrophic risks and harbors the potential to mitigate them. &nbsp;Thus, the evolutionary argument for easy human-level AI bears on catastrophic risks not only by bolstering the simulation argument, but also by raising the probability of superintelligent AI.</span><sup class="c7"><a href="#ftnt138" id="ftnt_ref138">[138]</a></sup></p><p class="c3"><span class="c7">Just as the import of the simulation argument is sensitive to how number and proportion boosting are applied, so too is the import of the evolutionary argument for easy AI.</span><sup class="c7"><a href="#ftnt139" id="ftnt_ref139">[139]</a></sup><span class="c8 c7">&nbsp; To the extent that number boosting favors hypotheses with more observers, a crucial issue is how the difficulty in engineering intelligence co-varies with the number of observers. &nbsp;The relationship is not obvious: while less intelligent observers may require fewer resources per observer (compare, say, fish versus humans), increases in intelligence may yield access (e.g. via space colonization) to large but otherwise inaccessible resource reservoirs. &nbsp;For similar reasons, it is not obvious whether proportion boosting of hypotheses on which a higher proportion of entities are observers favors easy AI hypotheses. &nbsp;On the other hand, to the extent that number boosting favors hypotheses with more observers like us, it favors:</span></p><ul class="c10 lst-kix_mrqo42u1c3mo-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c17">easy evolution hypotheses </span><span class="c8 c7">on which evolution independently produces large numbers of intelligent beings like us across many planets over Rare Earth hypotheses on which Earth is the only planet on which evolution produces human-level intelligence, and</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">easy engineering hypotheses </span><span class="c8 c7">on which evolution produces intelligent beings like us who go on to create artificial beings with such intelligence over difficult engineering hypotheses on which evolution produces comparably many intelligent beings like us but such beings generally do not go on to create artificial beings with human-level intelligence.</span></li></ul><p class="c3"><span class="c8 c7">Similarly, if proportion boosting favors hypotheses on which a higher proportion of observers are like us, it presumably favors hypotheses to the extent that it is easier for evolution to create beings like us and easier for us to create AI systems with human-level intelligence. &nbsp;Thus, number and proportion boosting arguably boost easy AI hypotheses and in turn lend support to the simulation argument.</span></p><p class="c3"><span class="c7">The evolutionary argument for easy AI is also sensitive to observation selection effects and choice of reference class. To see this, suppose first that the operative reference class is that of observers with human-level intelligence. &nbsp;That is, suppose that our observations of a given sample space are to be treated not as random samples from that space but as random samples from the observations human-like intelligent observers make of that space. &nbsp;Under this supposition, even if creating human-like intelligence is in fact exceedingly difficult, it might </span><span class="c7 c17">seem</span><span class="c7">&nbsp;quite easy before taking into account the observation selection effect.</span><sup class="c7"><a href="#ftnt140" id="ftnt_ref140">[140]</a></sup><span class="c8 c7">&nbsp; For example, suppose that the expected duration for Earth-like conditions to yield human-like intelligence is many orders of magnitude greater than than have thus far elapsed on Earth. And suppose that the world contains many life-hospitable planets but that they all exist for durations on the same order of magnitude as Earth&rsquo;s history. &nbsp;Then only a tiny fraction of planets would harbor observers with human-level intelligence, but such observers would generally find themselves to be products of evolution that were produced in relatively short order. &nbsp;To resist the temptation to erroneously conclude that engineering human-level intelligence is relatively easy, they would need to take into account this selection effect.</span></p><p class="c3"><span class="c7">Alternatively, suppose that the operative reference class is a wider one&mdash;say, the one that includes observers with human-level intelligence and observers with lower levels of intelligence. &nbsp;In that case, while the observation selection effect would make the evolution of </span><span class="c7 c17">intelligence </span><span class="c7">appear easy even if it is hard, it would leave room for observing the absence of </span><span class="c7 c17">human-level intelligence</span><span class="c8 c7">. &nbsp;So even once we take this effect into account, the fact that we observe human-level intelligence would rule out some hypotheses on which it goes unobserved because of how difficult it is to produce. &nbsp;Thus, under the noted supposition, the observation selection effect leaves the evolutionary argument intact (albeit attenuated) and permits us to reason from evolution&rsquo;s rapid production of human-level intelligence on Earth to conclusions about how difficult human-level intelligence is to engineer.</span></p><p class="c3"><span class="c7">In this fashion, the evolutionary argument for easy AI&mdash;and in turn its ability to bolster the simulation argument&mdash;is sensitive to choice of reference class. I hasten to add that there are additional factors (notably timing of evolutionary transitions, convergent evolution of prerequisites for intelligence, and Fermi&rsquo;s paradox) that can radically alter the import of observation selection effects on a given choice of reference class.</span><sup class="c7"><a href="#ftnt141" id="ftnt_ref141">[141]</a></sup></p><h2 class="c3 c25" id="h.gsivttg1c6ul"><span class="c5">12.3 Simulation and the Doomsday Argument</span></h2><p class="c3"><span class="c7">The </span><span class="c7 c17">doomsday argument </span><span class="c7">holds that taking into account our birth rank should lead us to expect that we will go extinct sooner than we would have otherwise thought.</span><sup class="c7"><a href="#ftnt142" id="ftnt_ref142">[142]</a></sup><span class="c8 c7">&nbsp; &#262;irkovi&#263; (2008: 129-30) helpfully summarizes the argument with the following analogy:</span></p><p class="c3 c18"><span class="c7">[Suppose there are] two&hellip; urns in front of you, one of which you know &nbsp;contains </span><span class="c27 c33 c21">ten</span><span class="c7">&nbsp;balls, the other a </span><span class="c27 c21 c33">million</span><span class="c7">, but you do not know which is which. &nbsp;The balls in each urn are numbered 1, 2, 3, 4, ... Now take one ball at random from the left urn; it shows the number 7. This clearly is a strong indication that the left urn contains only ten balls&hellip;. Now consider the case where instead of two urns you have two possible models of humanity&#39;s future, and instead of balls you have human individuals, ranked according to birth order. One model suggests that the human race will soon become extinct (or at least that the number of individuals will be greatly reduced) , and as a consequence the total number of humans that &nbsp;ever will have existed is about </span><span class="c27 c33 c21">100 billion</span><span class="c7">&hellip;. The other model indicates that humans will colonize other planets, spread through the Galaxy, and continue to exist for many future millennia; we consequently can take the number of humans in this model to be of the order of, say, 10</span><span class="c7 c15">18</span><span class="c8 c7">. &nbsp;As a matter of fact, you happen to find that your rank is about 60 billion. (emphasis mine)</span></p><p class="c3"><span class="c8 c7">Proponents of the doomsday argument contend that we should reason in the same way about our birth rank as we do about drawing ball #7: our birth rank is more likely on hypotheses on which fewer observers like us will have existed than ones on which observers like us exist in numbers of the sort we would expect if we avoid near-term extinction; since the most plausible way for a smaller number of observers to exist is for extinction to happen soon; therefore, our birth rank provides evidence that we will succumb to extinction soon.</span></p><p class="c3"><span class="c8 c7">The doomsday argument is subject to a few qualifications.</span></p><ul class="c10 lst-kix_ck95np4oxwnl-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">It does not purport to show that we will go extinct soon, or even that we will probably go extinct soon. &nbsp;Instead, the doomsday argument merely purports to show that our birth rank raises the probability that we will go extinct soon. </span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Granting that the argument works, the probability of near-term extinction it pushes one to will depend on how probable one regards near-term extinction before taking the argument into account.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">How probable one regards near-term extinction before taking the argument into account is sensitive to whether one embraces number boosting and/or proportion boosting and, if so, what reference classes one chooses for them and for the doomsday argument.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Whether the argument works may depend on what other relevant information one has. &nbsp;Even if the argument ordinarily works, it might be screened off by firmer empirical evidence about extinction risk.</span><sup class="c7"><a href="#ftnt143" id="ftnt_ref143">[143]</a></sup></li></ul><p class="c3"><span>I&rsquo;ll now note </span><span class="c7">some interactions between the doomsday argument and the simulation argument</span><span>.</span><sup class="c7"><a href="#ftnt144" id="ftnt_ref144">[144]</a></sup></p><p class="c3"><span class="c7">Like the simulation argument, the doomsday argument relies on an application of the indifference principle: whereas the simulation argument tells you to divide your credence equally among simulated and unsimulated &lsquo;locations&rsquo; in your reference class, the doomsday argument in effect assumes that you should divide your credence equally among birth locations within your reference class.</span><span>&nbsp; </span><span class="c7">Thus, (dis)confirmation of the indifference principle would tend to (dis)confirm both arguments.</span><span>&nbsp; </span><span class="c7">However, a challenge to one argument&rsquo;s </span><span class="c7 c17">application</span><span class="c7">&nbsp;of the principle need not carry over to the other argument. &nbsp;One reason for this is that it is not obvious that the choices of reference classes presupposed in the arguments&rsquo; respective applications stand or fall together.</span></p><p class="c3"><span class="c7">I noted above that number boosting arguably bolsters the simulation argument (via its simulation dominance premise). &nbsp;In contrast, number boosting arguably cancels at least some of the import of the doomsday argument by boosting our priors in abundant-observer hypotheses.</span><sup class="c7"><a href="#ftnt145" id="ftnt_ref145">[145]</a></sup></p><p class="c3"><span class="c7">The doomsday argument can be taken to cast doubt on the simulation argument by raising the probability that we will go extinct before creating many intelligent simulated beings in our reference class. &nbsp;There are several ways this might go</span><span class="c8 c7">. &nbsp;</span></p><p class="c3"><span>First, o</span><span class="c7">bservers like us could generally go extinct before creating any intelligent simulated beings. &nbsp;In this case, given that observers like us would create intelligent simulated beings absent near-term extinction, we will go extinct soon. &nbsp;The plausibility of this outcome is inversely related to the number of civilizations at our level of technological development which contain observers like us: the more such civilizations there are, the less plausible it is that they generally go extinct before creating intelligent simulations.</span></p><p class="c3"><span>Second, o</span><span class="c7">bservers like us could sometimes create intelligent simulated beings, but never in quantities that underwrite the simulation argument. &nbsp;The plausibility of this outcome is also inversely related to the number of civilizations at our level of technological development which contain observers like us. However, this relation is weakened by the fact that the creation of simulated intelligent beings may be strongly technologically coupled with AI risks that tend to result in extinction around the same time as civilizations gain the ability to create intelligent simulated beings.</span></p><p class="c3"><span>Third, o</span><span class="c7">bservers like us could create many intelligent simulated beings </span><span>albeit ones not </span><span class="c7">in our reference class. In contrast to the previous outcomes, the plausibility of this outcome is not inversely related to the number of civilizations at our level of technological development which contain observers like us.</span><span>&nbsp; </span><span class="c7">Created simulated beings might fall outside our reference class either because they are unconscious or because they differ from us in some other respect. &nbsp;There is a potentially enormous difference in value between these two possibilities. If we create many simulated beings but not many conscious simulated beings, that would likely be because we mistakenly believe that the simulated beings are unconscious. &nbsp;This scenario would likely involve a catastrophic failure of resource allocation. &nbsp;In the limit, we might set in motion a starfaring civilization, ensure that it will be inhabited by vast populations of intelligent simulations, and inadvertently engineer the cessation of value through a design flaw that renders the simulations unconscious. A potential alternative would be to engineer our own extinction and replacement through conscious simulations that fall outside our reference class. &nbsp;For those who place substantial weight on the doomsday argument, there is reason to pursue this outcome, as it seems to be one of the only ones that is reasonably probable and positive by the lights of the doomsday argument. &nbsp;That said, whether the argument deems this outcome reasonably probable turns on the unresolved issue of delineating our reference class.</span></p><p class="c3"><span class="c8 c7">The simulation argument can also be taken to cast doubt on the doomsday argument by revealing a way in which we might have a typical birth rank in a civilization that is not on the brink of extinction: if the relevant reference class concerns our simulators as well as intelligent simulated beings in other simulations, we might have a typical birth rank in our reference class even if we have a very early birth rank among intelligent beings in our universe-simulation. On the other hand, it is not clear why it would be illicit to run two versions of the doomsday argument: one for a broader reference class that encompasses all beings with minds like ours and another for a narrower reference class that includes only the beings in our universe. The latter version of the doomsday argument would at least not be blocked by the simulation argument, since it would reveal a way in which we might have a typical birth rank within our universe-simulation.</span></p><h2 class="c3 c25" id="h.awueh4ndalbh"><span class="c5">12.4 The Fermi Paradox</span></h2><p class="c3"><span class="c8 c7">Recall that the Fermi paradox is the problem of explaining why we seem to be alone in the universe, given the apparently astronomical number of opportunities for life and advanced civilizations to emerge. &nbsp;In &sect;3.2 we saw that the Fermi paradox points to catastrophic risks. It does this by raising the possibility that we seem to be alone because civilizations generally go extinct before becoming observable by civilizations elsewhere in the universe. &nbsp;We also saw that simulation-based research on Fermi&rsquo;s paradox is one way simulations could be brought to bear on catastrophic risks. &nbsp;In this subsection, I explore other connections between Fermi&rsquo;s paradox, simulations, and catastrophic risks. Specifically, I will describe how some interactions between Fermi&rsquo;s paradox and the simulation argument bear on catastrophic risks.</span></p><ul class="c10 lst-kix_kxevegngypoe-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Partial simulation solution</span><span class="c7">. &nbsp;One candidate solution to Fermi&rsquo;s paradox suggested by the simulation argument is that we&rsquo;re in a simulation-universe that is smaller than it appears.</span><sup class="c7"><a href="#ftnt146" id="ftnt_ref146">[146]</a></sup><span class="c8 c7">&nbsp; One natural way of spelling out the solution contends that, due to resource constraints, our simulators run partial simulations that fill in the details of the universe as required to keep up appearances of a universe for the simulation inhabitants; as a related measure, they only simulate observers on one planet. &nbsp;On this development of the simulation solution, there were far fewer opportunities for intelligence to emerge in our universe than our astronomical observations suggest, and we are not alone (as our simulators exist as well, along with whatever other intelligent beings exist at their level of reality).</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Silent simulation solution</span><span class="c7">: Another candidate solution is that long-lasting civilizations, rather than engaging in observable activities such as space colonization, instead occupy themselves with unobservable activities that unfold within virtual environments of their own making.</span><sup class="c7"><a href="#ftnt147" id="ftnt_ref147">[147]</a></sup></li></ul><ul class="c10 lst-kix_kxevegngypoe-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">The plausibility of this solution depends on the vexed issue of how many advanced civilizations we should expect there to be within the observable universe: the higher the number, the less plausible it is that all potentially observable civilizations would be unobservable for this reason.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">The plausibility of this solution also depends on what reasons civilization would have to engage in silent simulation rather than detectable activities. &nbsp;Here are a couple of notable potential motivations:</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c17">Self-interested risk mitigation</span><span class="c7">. Remaining unobservable is an obvious strategy to avoid threats to civilizations posed by hostile actors that may lurk in our universe.</span><sup class="c7"><a href="#ftnt148" id="ftnt_ref148">[148]</a></sup><span class="c8 c7">&nbsp; Silent simulation may be an optimal means for realizing value while remaining unobservable. &nbsp;That this motivation would be widespread becomes more plausible on the assumption that civilizations that survive long enough to become technologically mature tend to be risk-averse.</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c17">Fairness / altruistic risk mitigation</span><span class="c8 c7">. For civilizations that face Fermi&rsquo;s paradox, it is an ominous warning sign of existential catastrophes. Any civilization that managed to avoid these catastrophes would be confronted with an issue of cosmic fairness: while becoming a space faring civilization might be of enormous benefit to it, doing so would dissolve (or attenuate) Fermi&rsquo;s paradox for other, less technologically advanced civilizations. &nbsp;Removing such a warning sign would therefore put other civilizations at a disadvantage and raise the catastrophic risks faced by such civilizations. That this motivation would be widespread becomes more plausible on the assumption that civilizations that survive long enough to become technologically mature tend to act in accordance with fairness or altruistic values.</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1"><li class="c2 li-bullet-0"><span class="c8 c7">To the extent that the silent simulation solution is motivated, it provides an additional reason to think that advanced civilizations would run simulations. &nbsp;Hence, the availability of this solution arguably bolsters the simulation argument.</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Shutdown risk</span><span class="c8 c7">. If we had awoken to a world evidently teaming with advanced civilizations, that would have been good news about the risk of simulation shutdown: if simulation shutdown were something that civilizations had a remote chance of triggering, we&rsquo;d expect some other civilization to have triggered it before we came into existence. So the fact that many other advanced civilizations existed would suggest that the risk of simulation shutdown is low. &nbsp;Conversely, instead awakening to the Fermi paradox is bad news, as doing so provides no assurances against the risk of simulation shutdown. &nbsp;For the same reasons, solutions to Fermi&rsquo;s paradox that posit many (unobserved) advanced civilizations will tend to lower the risk of simulation shutdown while solutions that instead posit a small number of advanced civilizations will tend to raise the risk of shutdown.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Filter placement and the difficulty of engineering intelligence.</span><span class="c8 c7">&nbsp; If we had awoken to a world replete with many independently evolved species of human-like intelligence, that would have been evidence that (per the argument in &sect;12.2) that engineering human-level intelligence is not an exceedingly hard problem. &nbsp;(The matter would not have been completely cut and dry: in light of observation selection effects, there&rsquo;d be room to wonder if evolution almost never produces such species but reliably produces many if it produces any.) &nbsp;Conversely, failing to observe the traces of such species when we gaze at the cosmos through telescopes is evidence that:</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">producing such species is difficult for evolution,</span></li><li class="c2 li-bullet-0"><span class="c8 c7">(hence) engineering human-level intelligence is difficult for human-level intelligent beings (ourselves included),</span></li><li class="c2 li-bullet-0"><span class="c8 c7">(hence) we will go extinct before being able to produce simulations with human-level intelligence, and </span></li><li class="c2 li-bullet-0"><span class="c7">(hence, </span><span class="c7 c17">pace</span><span class="c7">&nbsp;the simulation argument) beings like ourselves generally fail to produce many more simulations that are beings like ourselves.</span><sup class="c7"><a href="#ftnt149" id="ftnt_ref149">[149]</a></sup></li></ul><ul class="c10 lst-kix_kxevegngypoe-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Rare Earth solutions</span><span class="c7">. Rare Earth solutions hold that the universe appears devoid of other civilizations because an Early Filter renders life (or complex life or intelligent life) rare&mdash;rare enough for it to be unsurprising that there are no other civilizations for us to observe.</span><sup class="c7"><a href="#ftnt150" id="ftnt_ref150">[150]</a></sup><span class="c8 c7">&nbsp; </span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Proposed Rare Earth solutions tend to render it very unlikely that there would be even one planet that gives rise to (intelligent) life. &nbsp;</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">For illustration: some estimates for the odds of life arising on a given Earth-like planet include figures like 1 in 10</span><span class="c7 c15">40,000</span><span class="c7">&nbsp;and 10</span><span class="c7 c15">100,000,000,</span><sup class="c7"><a href="#ftnt151" id="ftnt_ref151">[151]</a></sup><span class="c7">&nbsp;while one estimate of the number of planets within the observable universe puts it on the order of 10</span><span class="c7 c15">20</span><span class="c7">, with most of those planets being life-inhospitable.</span><sup class="c7"><a href="#ftnt152" id="ftnt_ref152">[152]</a></sup><span class="c8 c7">&nbsp; </span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1"><li class="c2 li-bullet-0"><span class="c7">That Rare Earth solutions tend to render it </span><span class="c7 c17">very </span><span class="c7">unlikely that there would be even one planet that gives rise to (intelligent) life is not necessarily a bug: given that life is extremely unlikely to arise on any particular Earth-like planet, it would be a striking coincidence if the number of such planets was such as to render the expected number of planets that give rise to intelligent life/civilizations ~1.</span><sup class="c7"><a href="#ftnt153" id="ftnt_ref153">[153]</a></sup></li><li class="c2 li-bullet-0"><span class="c8 c7">On the other hand, it is hard to believe that intelligent life&mdash;being the striking phenomenon that it is&mdash;just happened to arise if the chances or its doing so were miniscule. &nbsp;That would be a form of unexplained fine-tuning akin to physical parameter values all just happening to fall within the narrow ranges &nbsp;required for life.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Just as such cosmological fine-tuning would arguably be more likely given a designer or multiverse, so too would a Very Rare Earth. &nbsp;Thus, rare earth solutions to Fermi&rsquo;s paradox arguably support multiverse and design hypotheses. &nbsp;For reasons encountered in &sect;11, multiverse and design hypotheses fit with the simulation hypothesis. &nbsp;Moreover, on the face of it, nothing about the simulation versions of the design and multiverse hypotheses renders them worse off at explaining the existence of a Very Rare Earth than their non-simulation counterparts.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Thus, Rare Earth solutions to Fermi&rsquo;s paradox arguably support the simulation hypothesis.</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Number boosting</span><span class="c8 c7">. &nbsp;Recall that number boosting tells us to favor hypotheses on which there are more observers (like us).</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Given the vastness of our universe, one application of number boosting tells us to favor hypotheses on which there are more observers, and hence to favor solutions to Fermi&rsquo;s paradox on which the apparent dearth of observers is misleading. </span></li></ul><ul class="c10 lst-kix_kxevegngypoe-2 start"><li class="c3 c16 li-bullet-0"><span class="c8 c7">These include simulation solutions on which many observers exist but we are not in a position to observe them because they inhabit simulations. &nbsp;</span></li></ul><ul class="c10 lst-kix_kxevegngypoe-1"><li class="c2 li-bullet-0"><span class="c8 c7">If we take the apparent dearth of observers at face value, this provides grounds for questioning number boosting. &nbsp;This arguably weakens the simulation argument, since, as seen in &sect;12.1, number boosting arguably bolsters the simulation argument.</span></li></ul><h2 class="c3 c25" id="h.p5r6p1ugz6r7"><span class="c5">12.5 Boltzmannian Cosmologies</span></h2><p class="c3"><span class="c7">Physics offers a number of otherwise appealing cosmological models that have a peculiar consequence: they predict that the vast majority of systems in brain states like ours are &ldquo;Boltzmann brains&rdquo;, short-lived brains that result from thermal or quantum fluctuations.</span><sup class="c7"><a href="#ftnt154" id="ftnt_ref154">[154]</a></sup><span class="c7">&nbsp; When taken with some innocuous-seeming assumptions, these theories yield the skeptical conclusion that we ourselves are probably Boltzmann Brains. &nbsp;In particular, if we assume that what experience a system has is fixed by its brain state, then these theories predict that almost all observers with our experiences are Boltzmann brains. &nbsp;An application of the indifference principle then dictates that we should divide our credences evenly among observers with our experiences and conclude that, on these theories, we are almost certainly Boltzmann brains. Avoiding this skeptical conclusion is </span><span class="c7 c17">the</span><span class="c7">&nbsp;</span><span class="c7 c17">Boltzmann brain problem</span><span class="c8 c7">.</span></p><p class="c3"><span class="c7">There is much controversy about what to make of the implications of these theories. &nbsp;Here, I will describe some interactions between the Boltzmann brain problem and the simulation argument.</span><sup class="c7"><a href="#ftnt155" id="ftnt_ref155">[155]</a></sup><span class="c7">&nbsp; As far as I can tell, the main bearing that the Boltzmann brain problem has on catastrophic risks is by way of these interactions.</span><sup class="c7"><a href="#ftnt156" id="ftnt_ref156">[156]</a></sup></p><ul class="c10 lst-kix_4to7a6wumrti-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">The Boltzmann brain problem shows how the simulation argument could fail even granting the indifference principle and that ordinary observers like us are vastly outnumbered by observers like us in simulations: given that simulated observers would themselves be vastly outnumbered by Boltzmannian observers, the indifference principle would require us to think we&rsquo;re probably Boltzmannian observers rather than observers in a simulation.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Gaining evidence for Boltzmannian cosmologies gives those who wish to resist skepticism reason to revisit the assumption that brain states fix experiences, as well as the indifference principle.</span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Since the simulation argument relies on closely related assumptions, such evidence would also give us reason to revisit the simulation argument.</span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c17">Indifference-rejecting solutions to the Boltzmann brain problem constrain indifference-rejecting responses to the simulation argument. &nbsp;</span><span class="c8 c7">Assuming that brain states fix experiences, Boltzmannian cosmologies predict that almost all observers with experiences like ours are Boltzmann brains on virtually any precisification of &lsquo;like ours&rsquo;. &nbsp;In contrast, as we&rsquo;ve seen, the simulation argument&rsquo;s application of the principle of indifference is sensitive to choice of reference class. &nbsp;This yields the following asymmetry: whereas rejecting the simulation argument based on illicit choice of reference class will tend to leave the Boltzmann brain problem unscathed, solving the Boltzmann brain problem based on illicit choice of reference class will tend to generate an objection to the simulation argument.</span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-3 start"><li class="c3 c39 li-bullet-0"><span class="c7">This asymmetry is subject to an important qualification: rejecting the application of the indifference principle in the Boltzmann brain problem on the ground that it leads to skepticism does not jeopardize the simulation argument&rsquo;s application of the indifference principle&mdash;at least given that the simulation </span><span>hypothesis </span><span class="c8 c7">is not a skeptical one. </span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-2"><li class="c3 c16 li-bullet-0"><span class="c7 c17">Zombification solutions to the Boltzmann brain problem undermine the simulation argument</span><span class="c7">. &nbsp;Zombification solutions to the Boltzmann brain problem hold that Boltzmann brains would be unconscious (&lsquo;zombies&rsquo;) and therefore not observers like us, meaning that the fact that we&rsquo;re conscious entails that we&rsquo;re not Boltzmann brains. &nbsp;While the Boltzmann brain problem is typically couched in terms of brain states, it is robust under choices of much larger states, including states that involve brain states, bodies, and local environments.</span><sup class="c7"><a href="#ftnt157" id="ftnt_ref157">[157]</a></sup><span class="c7">&nbsp; Thus, correspondingly robust zombification solutions require experience to be fixed by very large (perhaps global) physical states, rather than any sort of intrinsic or local physical state. &nbsp;If experience has such a large physical basis, it becomes doubtful that duplicating causal structure in the brain in a simulation would suffice to duplicate experiences. &nbsp;Thus, zombification solutions tell against the simulation argument by giving us a reason to think that simulations would be unconscious.</span><sup class="c7"><a href="#ftnt158" id="ftnt_ref158">[158]</a></sup></li></ul><ul class="c10 lst-kix_4to7a6wumrti-1"><li class="c2 li-bullet-0"><span class="c7 c17">Instability again. &nbsp;</span><span class="c7">In &sect;11 we saw that some have objected to the simulation argument on the ground that it is cognitively unstable. &nbsp;The same objection has been leveled against Boltzmannian cosmologies.</span><sup class="c7"><a href="#ftnt159" id="ftnt_ref159">[159]</a></sup><span class="c8 c7">&nbsp;In the context of Boltzmannian cosmologies, the objection is that evidence cannot be stably taken to support such cosmologies since supporting such cosmologies would indicate that we are Boltzmann brains and therefore not subjects who have such evidence. &nbsp;At least some of the reasons given for doubting that the objection works against the simulation argument also suggest that they fail against Boltzmannian cosmologies.</span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-2 start"><li class="c3 c16 li-bullet-0"><span class="c8 c7">For example, take the hypoxia cases in which it is irrational to disregard the cognitively unstable hypothesis that one is hypoxic. &nbsp;These cases show that cognitive instability is generally insufficient as a ground for rejecting a &nbsp;hypothesis.</span></li><li class="c3 c16 li-bullet-0"><span class="c7">And just as the simulation argument can be recast in terms of stable simulations, so too can the Boltzmann brain problem be recast in terms of </span><span class="c7 c17">stable Boltzmannian bubbles</span><span class="c8 c7">, localities that feature observers that are like us in local physical respects and which are large enough and long-lasting enough to avoid rendering the reasoning in the Boltzmann brain problem unstable.</span></li></ul><ul class="c10 lst-kix_4to7a6wumrti-0"><li class="c3 c9 li-bullet-0"><span class="c7 c17">Simulation solutions to the Boltzmann brain problem</span><span class="c7">. &nbsp;We saw in &sect;12.4 that there is reason to think we&rsquo;re in a partial simulation, on which details of our universe-simulation are filled in as we observe, making our universe appear much larger and more detailed than it is. This suggests the following </span><span class="c7 c17">partial simulation solutions</span><span class="c8 c7">&nbsp;to the Boltzmann brain problem: it may be that Boltzmann observers do not numerically dominate ordinary observers because we are living in a partial simulation and Boltzmann brains would either (i) exist in the (apparent) parts of our universe that are not simulated, (ii) exist in parts of our universe that are simulated in too little detail to realize consciousness, or (iii) be unconscious because of differences that hold between them and us outside the simulation.</span></li><li class="c3 c9 li-bullet-0"><span class="c7 c17">Number boosting</span><span class="c8 c7">. &nbsp;Number boosting tends to favor Boltzmannian cosmologies over non-Boltzmannian cosmologies and therefore engenders skepticism, absent a solution to the Boltzmann brain problem. &nbsp;This could be taken as a reason to reject number boosting. &nbsp; Since number boosting arguably strengthens the simulation argument, the fact that number boosting tends to favor Boltzmannian cosmologies can also be taken as a reason for reducing confidence in the simulation argument.</span></li></ul><h1 class="c3 c25" id="h.689t9d5stkrq"><span class="c5">13 Simulation as Non-Causal Intervention</span></h1><p class="c3"><span class="c7">This section will introduce </span><span class="c7 c17">simulations as non-causal interventions</span><span class="c7">, which I regard as an important and neglected factor with the potential to </span><span>significantly </span><span class="c7">influence </span><span>risk levels for a wide range of catastrophes</span><span class="c7">. &nbsp;To a first approximation, the idea is that by raising the probability that certain types of simulations will be run, we can change the probability of our currently inhabiting those types of simulations and (perhaps counterintuitively) thereby meaningfully but non-causally affect the probability of various types of catastrophic risks. &nbsp;The concept will require some unpacking, as it combines insights from the simulation argument and some recent work in decision theory.</span><sup class="c7"><a href="#ftnt160" id="ftnt_ref160">[160]</a></sup></p><p class="c3 c22"><span class="c7">To start, recall the simulation argument&rsquo;s premise of simulation dominance: at least a small portion of beings like us will produce a very large number of beings like us that are simulated&mdash;a large enough number for most beings like us to turn out to be simulations. &nbsp;The main ways for this premise to be false would be if beings like us went extinct before we created beings like us that are simulations or if we decided never to create such beings. &nbsp;Even if plausible, this premise is at present a speculative empirical claim. So we should not be extremely confident in it. &nbsp;However, this could change: if we began mass producing simulations that realized beings like us, that would give us a powerful reason to accept simulation dominance. Those who have a high credence in the indifference principle (the other premise in the simulation argument) would then be under strong pressure to conclude from the argument that we are probably living in a simulation.</span><sup class="c7"><a href="#ftnt161" id="ftnt_ref161">[161]</a></sup><span class="c8 c7">&nbsp; Likewise, acquiring evidence that we will mass produce such simulations should boost our confidence in simulation dominance and in turn the simulation hypothesis via the simulation argument.</span></p><p class="c3 c22"><span class="c7">Next, consider that there is a family of variations of the simulation argument that concern different types of simulations. For example, there are </span><span class="c7 c17">salvation simulations </span><span class="c7">in which minds like ours are immune to catastrophic risks but have much misleading evidence to the contrary. &nbsp;There are also </span><span class="c7 c17">doom simulations</span><span class="c7">&nbsp;in which it is inevitable that minds like ours will collectively succumb to certain catastrophic risks, regardless of their evidence concerning risk levels and regardless of their risk mitigation efforts. &nbsp;And there are myriad other sorts of simulations in which minds like ours </span><span>face risks that are bizarrely related to their </span><span class="c8 c7">evidence and actions. &nbsp;Just as evidence that we will one day mass produce simulations with minds like ours should boost our confidence that we are in a simulation, so too should evidence that we will one day mass produce simulations of a given type that contain minds like ours boost our confidence that we are in a simulation of that type. &nbsp;Thus, evidence that we will run such salvation simulations would be good news while evidence that we will run such doom simulations would be bad news. &nbsp;And we are in a position to choose what type of news we receive&mdash;for example, setting up a fund to sponsor salvation simulations when they become technologically feasible would be a way of bringing about good news about the catastrophic risks we face.</span></p><p class="c3 c22"><span class="c7">Of course, we are either in a given type of simulation or we are not. &nbsp;And we control neither whether we are in a simulation nor which type of simulation we are in if we are in one. Thus, there is no way to causally exploit the described connection between our advancing the creation of certain sorts of simulations and our inhabiting such simulations. &nbsp;So it may seem that this connection is not decision-relevant in the context of catastrophic risks. &nbsp;Endorsing this appearance would be to take a stand on an ongoing debate in decision theory. The debate concerns whether </span><span class="c7 c17">causal decision theory</span><span class="c7">&nbsp;is </span><span>correct</span><span class="c7">, rather than one of its rivals such as </span><span class="c7 c17">evidential decision theory</span><span class="c8 c7">. &nbsp;Roughly, whereas &ldquo;[evidential decision theory] tells you to perform the action that would be the best indication of a good outcome, &hellip; [causal decision theory] tells you to perform the action that will tend to causally bring about good outcomes&rdquo; (Levinstein &amp; Soares, 2020). </span></p><p class="c3 c22"><span class="c7">To briefly illustrate, suppose you are deciding whether to vote. Holding fixed how everyone else votes, you know (let&rsquo;s suppose) that your vote won&rsquo;t make a difference as to who wins and hence that you could cause more good by doing something else instead of voting. You also know that the people like you will vote for the better candidate just in case you vote for that candidate, and that that candidate will probably win just in case the people like you vote for her. &nbsp;In this case evidential decision theory recommends voting, while causal decision theory recommends not voting.</span><sup class="c7"><a href="#ftnt162" id="ftnt_ref162">[162]</a></sup><span class="c8 c7">&nbsp; Similarly, evidential decision theory will tend to recommend running salvation simulations, preventing doom simulations, and bringing about evidence that the former will be run and the latter prevented. &nbsp;On evidential decision theory, such actions affect catastrophic risk levels in a decision relevant sense without causally affecting them&mdash;in this sense, evidential decision theory licenses non-causal interventions. &nbsp;In contrast, causal decision theory will tend to judge these actions unworthy of promotion and will recommend allocating our resources elsewhere should these actions come with even the slightest opportunity cost. &nbsp;</span></p><p class="c3 c22"><span class="c7">Causal decision theory seems more widely favored than evidential decision theory.</span><sup class="c7"><a href="#ftnt163" id="ftnt_ref163">[163]</a></sup><span class="c8 c7">&nbsp; And it might seem that non-causal interventions can influence the expected value of outcomes in decision relevant ways only to those who accept evidential decision theory. &nbsp;Putting these two points together, one might be tempted to conclude that simulations as non-causal interventions bear on catastrophic risks in decision-relevant ways only given a certain minority view (evidential decision theory). &nbsp;Contrary to this response, simulations as non-causal interventions have broader significance. &nbsp;This is for several reasons:</span></p><ul class="c10 lst-kix_iovfrf24wp-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">While evidential decision theory is perhaps the most often discussed rival to causal decision theory, it is not the only rival. &nbsp;There are various other non-causal decision theories that agree with evidential decision theory in recommending non-causal interventions via simulations.</span><sup class="c7"><a href="#ftnt164" id="ftnt_ref164">[164]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c7">Perhaps pluralism is true of rationality and different decision theories accurately describe different forms of rationality corresponding to different foci of evaluation.</span><sup class="c7"><a href="#ftnt165" id="ftnt_ref165">[165]</a></sup></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">We should be uncertain about whether causal decision theory is correct. &nbsp;Arguably, factoring this uncertainty into our decision making should make us act in accordance with evidential decision theory&rsquo;s recommendations concerning non-causal interventions even if we think causal decision theory is probably true instead.</span></li></ul><ul class="c10 lst-kix_iovfrf24wp-1 start"><li class="c2 li-bullet-0"><span class="c7">We should be uncertain about which decision theory is correct because decision theory is a difficult and highly technical subject and there is disagreement among relevant experts about whether causal decision theory is</span><span>&nbsp;correct</span><span class="c8 c7">.</span></li><li class="c2 li-bullet-0"><span class="c7">Given uncertainty about whether causal decision theory is </span><span>correct</span><span class="c8 c7">, there is a question of how to factor in our decision-theoretic uncertainty when deciding what to do.</span></li><li class="c2 li-bullet-0"><span class="c7">One natural option is to use </span><span class="c29 c17 c37">meta decision theo</span><span class="c7 c17">ry</span><span class="c7">. &nbsp;It recommends taking the option with the highest meta expected value, where an action&rsquo;s meta expected value = the sum of expected values assigned to it by rival first-order decision theories weighted by their probability of being correct.</span><sup class="c7"><a href="#ftnt166" id="ftnt_ref166">[166]</a></sup><span class="c8 c7">&nbsp; By the lights of meta decision theory, there can be strong reasons to act in accordance with the recommendations of evidential decision theory (or other non-causal decision theories) rather than causal decision theory even if we are much more confident in causal decision theory.</span></li></ul><ul class="c10 lst-kix_iovfrf24wp-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">Meta decision theory leads to </span><span class="c7 c17">the evidentialist&rsquo;s wager</span><span class="c7">, which contends that we should in this fashion act against the odds in a wide range of cases in which causal decision theory and evidential decision theory issue conflicting recommendations.</span><sup class="c7"><a href="#ftnt167" id="ftnt_ref167">[167]</a></sup><span class="c8 c7">&nbsp; The basic reason for this is that evidential decision theory channels expected value to acts via agents in similar decision contexts whose acts are non-causally correlated with that of the actor (for instance, evidential decision theory amplifies the expected value of your voting, given that your voting gives you strong evidence of how similarly minded agents whose decisions are non-causally correlated with your own will act); since causal decision theory does no such thing, the meta expected value of evidential decision theory&rsquo;s recommendations tend to swamp causal decision theory&rsquo;s in such cases. &nbsp;Running salvation simulations is a case in point: taking actions to raise the probability of running salvation simulations that contain many minds like ours raises the probability that the same sort of action will be taken by the many other agents in advanced civilizations whose decisions are non-causally correlated with our own; by the above argument, that action thereby diminishes the probability of catastrophic risks and so accrues much expected value by the lights of evidential but not causal decision theory. &nbsp;Thus, the evidentialist&rsquo;s wager applies in this context: even if we are confident but not certain that causal decision theory is correct, we still have strong reason to act on evidential decision theory&rsquo;s recommendation to raise the probability that we will run salvation simulations, since this would in turn raise the probability that we are in a salvation simulation and so reduce catastrophic risks. &nbsp;Similar reasoning applies to other non-causal interventions through simulations of observers like us.</span></li></ul><ul class="c10 lst-kix_iovfrf24wp-1"><li class="c2 li-bullet-0"><span class="c8 c7">Meta decision theory is not the only option for dealing with decision theoretic uncertainty.</span></li></ul><ul class="c10 lst-kix_iovfrf24wp-2 start"><li class="c3 c16 li-bullet-0"><span class="c7">Some alternatives recommend in effect ignoring decision theoretic uncertainty and acting in accordance with the decision theory you believe, have the highest credence in, is most supported by your evidence, or which is </span><span>correct</span><span class="c7">. &nbsp;Given corresponding auxiliary assumptions about causal decision theory, these proposals will deliver the verdict that you should act in accordance with it rather than any theory that promotes non-causal interventions. &nbsp;However, these proposals are subject to severe limitations and powerful objections. &nbsp;For instance, the proposal that you should act in accordance with the </span><span>correct </span><span class="c7">theory provides no guidance if you are uncertain which theory is </span><span>correct</span><span class="c7">. &nbsp;Acting in accordance with the theory you have the highest credence allows small differences in credence to outweigh arbitrarily large differences in stakes. Acting in accordance with the theory you believe precludes dominance reasoning in cases where that theory is indifferent between two options and the other theories you have credence in agree about which option you should take.</span><sup class="c7"><a href="#ftnt168" id="ftnt_ref168">[168]</a></sup></li><li class="c3 c16 li-bullet-0"><span class="c7">Other alternatives to meta decision theory agree with meta decision theory that we should factor in decision theoretic uncertainty but propose a different way of taking it into account. &nbsp;These include variations of meta decision theory that incorporate an extra parameter for risk aversion or to correct for counterintuitive effects of certain theories swamping other theories by assigning much higher stakes.</span><sup class="c7"><a href="#ftnt169" id="ftnt_ref169">[169]</a></sup><span class="c7">&nbsp;They also include bargaining and parliamentary </span><span>proposals </span><span class="c7">that treat different theories as if they were engaged in moral trade or voting members in a parliament.</span><sup class="c7"><a href="#ftnt170" id="ftnt_ref170">[170]</a></sup><span class="c8 c7">&nbsp; On the face of it, we&rsquo;d expect these and other alternatives to meta decision theory that take decision theoretic uncertainty into account and which are at least somewhat stake sensitive to promote non-causal interventions via (evidence for) simulations. &nbsp;The space of such theories is underexplored, as is the application of such theories to non-causal interventions via simulations.</span></li></ul><p class="c3"><span class="c7">Some might take the promotion and prevention of certain simulations as non-causal interventions as an implausible result that indicates that the argument has gone wrong somewhere. &nbsp;My own view is that this reaction is understandable but unwarranted. &nbsp;Admittedly, that simulations as non-causal interventions should be promoted is a weird and wacky idea. &nbsp;However, this comes with the territory: it&rsquo;s no surprise that interactions between decision theory, decision theoretic uncertainty, self-locating belief, and simulation hypotheses have bizarre consequences. &nbsp;If there is something especially objectionable about this one, it remains to be specified. I expect others to disagree here. So there is a project of exploring the prospects </span><span>for </span><span class="c8 c7">logically weakening the argument while preserving its upshot along with the prospects for a robust escape from the argument.</span></p><p class="c3"><span class="c8 c7">If simulation as non-causal interventions is admitted as relevant to analyzing and seeking to reduce catastrophic risks, how should we devise a risk-reduction portfolio that is appropriately sensitive to them? This is a large question that I cannot hope to settle here. This is partly because of the residual issues concerning how to deal with decision theoretic uncertainty that we encountered above, and partly because addressing it would require a systematic investigation of what sorts of non-causal interventions via simulation are possible, their expected (causal and non-causal, descriptive and axiological) consequences. &nbsp;While I will not attempt such an investigation here, I will lay out some considerations that would need to be addressed in such an investigation.</span></p><p class="c3"><span class="c8 c7">First, we need to consider various types of non-causal interventions that could affect catastrophic risk levels via simulation hypotheses about different types of simulations. &nbsp;Types to consider include not only salvation and doom simulations, but also:</span></p><ul class="c10 lst-kix_8ax6ke4754b9-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Research simulations</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Entertainment simulations</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Catastrophic simulations</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Different types of salvation simulations</span></li></ul><ul class="c10 lst-kix_8ax6ke4754b9-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Compensatory simulations in which the good and badness of the simulation is set in advance and minds are compensated in a simulation afterlife to achieve the preset value.</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Miraculous simulations in which certain sorts of would-be catastrophes are miraculously prevented at the last moment.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Merely apparent suffering simulations in which the vast majority of apparently suffering minds are either not minds or not suffering.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Simulations that terminate once the overall expected value of their continuation becomes negative</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-0 start"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Debunking simulations</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">For a given belief, we could undermine it by running simulations of minds like ours in which that belief has a debunking causal origin, thereby raising the probability that our belief has a debunking causal origin. </span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Simulations that vindicate/falsify different philosophical views.</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-1 start"><li class="c2 li-bullet-0"><span class="c7">Personal identity simulations: by running simulations in which a certain view of personal identity is true, we could raise the probability that it is true. Some views of personal identity may have the potential to influence individual&rsquo;s levels of altruism&mdash;for example, the distinction between self and others is diminished on some eliminativist and reductionist views of personal identity.</span><sup class="c7"><a href="#ftnt171" id="ftnt_ref171">[171]</a></sup><span class="c8 c7">&nbsp;By bringing it about that we are probably in a simulation in which one of those views is true of us and broadcasting that result thus constitutes a strategy for persuading people to be less self-concerned.</span></li><li class="c2 li-bullet-0"><span class="c7">Free will and anti-free will simulations: by running simulations in which minds like ours have (lack) free will, we would raise (lower) the probability that we have free will. &nbsp;E.g. if free will is </span><span>less likely given</span><span class="c7">&nbsp;determinism, we </span><span>might lower the probability that we have free will by </span><span class="c8 c7">running deterministic simulations containing minds like ours.</span></li><li class="c2 li-bullet-0"><span class="c7">By running simulations that conform to a certain model of time, we would raise (lower) the probability that we are in a simulation that conforms to that model. Arguably, different models of time have different axiological consequences.</span><sup class="c7"><a href="#ftnt172" id="ftnt_ref172">[172]</a></sup><span class="c8 c7">&nbsp;So modifying the probability of different models can be expected to affect the distribution of value in our universe.</span></li><li class="c2 li-bullet-0"><span class="c7">One of the leading interpretations of quantum mechanics (the many worlds interpretation) has the unsettling consequence (given auxiliary assumptions about personal identity) that each of us should expect to survive with certainty arbitrarily long into the future, probably via means that involve great suffering.</span><sup class="c7"><a href="#ftnt173" id="ftnt_ref173">[173]</a></sup><span class="c8 c7">&nbsp; By running simulations of universes inhabited by minds like ours and underpinned by an interpretation of quantum mechanics that does not have the unsettling consequence, we could lower the probability that we are subject to it.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Running simulations in which minds like ours are skeptically situated would arguably raise the probability that we are skeptically situated in such a simulation (subject to complications involving cognitive instability discussed in &sect;8)</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-0"><li class="c3 c9 li-bullet-0"><span class="c8 c7">Anti-simulation hypothesis simulations are simulations such that running them (and evidence that we will run them) lower the probability that we are in a simulation. This would tend to lower simulation termination risks for us.</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-1 start"><li class="c2 li-bullet-0"><span class="c8 c7">Totem simulations: in such simulations, minds like ours would generally have a &ldquo;totem&rdquo;, a test that they could easily run to tell that they are in a simulation.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Unconscious simulations: if we ensured that any simulations of minds like ours are unconscious, that would cast doubt on the simulation dominance premise and so give us reason to reduce our confidence in the simulation hypothesis.</span></li><li class="c2 li-bullet-0"><span class="c8 c7">Anti-nesting simulations: if we run simulations in which minds like ours cannot create minds like ours via simulation, that would cast doubt on the simulation dominance premise and so give us reason to reduce our confidence in the simulation hypothesis.</span></li><li class="c2 li-bullet-0"><span class="c7">Skeptical simulations: running simulations in which minds like ours are skeptically situated would arguably raise the probability that we are in a skeptical situation if we are in a simulation. If we have independent warrant for believing we are not in a skeptical situation,</span><sup class="c7"><a href="#ftnt174" id="ftnt_ref174">[174]</a></sup><span class="c8 c7">&nbsp;this would then tell against our being in a simulation.</span></li></ul><ul class="c10 lst-kix_wyi70rgo9eij-0"><li class="c3 c9 li-bullet-0"><span class="c7">Level coordination simulations: by running simulations that are coordinated with our simulation with respect to certain features (e.g. physics), we could raise the probability that we are in a simulation that shares those features with the level of its simulators.</span><sup class="c7"><a href="#ftnt175" id="ftnt_ref175">[175]</a></sup><span class="c8 c7">&nbsp; By requiring these features to be strictly preserved in nested simulations, we could raise the probability that they are preserved in any simulations in which ours is nested.</span></li></ul><p class="c3"><span class="c8 c7">If simulation as non-causal interventions is admitted as relevant to analyzing and seeking to reduce catastrophic risks, there are some potential mistakes that we should take care to avoid or else ensure that they are not in fact mistakes. These include:</span></p><ul class="c10 lst-kix_tsc2do13t185-0 start"><li class="c3 c9 li-bullet-0"><span class="c7">Researching catastrophic risks through simulations of minds like ours facing such risks. &nbsp;This would raise the probability that we&rsquo;re in such a simulation and hence that we will succumb to catastrophe. &nbsp;(To avoid this mistake, we could run only unconscious simulations or ensure that catastrophic sim</span><span>ulations</span><span class="c7">&nbsp;are offset by sufficiently many non-catastrophic simulations</span><span>, though the latter option would be morally objectionable.</span><span class="c8 c7">)</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Running simulations in which minds like ours are skeptically situated could constitute an epistemic catastrophe for us. &nbsp;The extent to which such epistemic catastrophe is a real risk is unclear: maybe we would be warranted in concluding that sim</span><span>ulations</span><span class="c7">&nbsp;are unconscious if we found out that most minds like ours would be skeptically situated if </span><span>simulations</span><span class="c8 c7">&nbsp;were conscious. &nbsp;Or maybe epistemic catastrophes would be psychologically ignored and as a result not be catastrophes from a moral or all-things-considered perspective.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Running simulations that induce cognitive instability (such that we cannot form a coherent view about our evidence and whether we are in a certain type of simulation for which we apparently have evidence) could undermine our ability to evaluate and reduce catastrophic risks, perhaps while leaving our epistemic standing in other domains intact.</span></li><li class="c3 c9 li-bullet-0"><span class="c7">Running simulations in which minds like ours are short-lived would </span><span>reduce our life expectancy</span><span class="c7">. &nbsp;The same goes for </span><span>our civilization&rsquo;s life expectancy if we run </span><span class="c7">simulations </span><span class="c8 c7">with civilizations that are short-lived and which contain minds like ours.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Some catastrophes would&mdash;if they occurred at all&mdash;occur before the creation of salvation simulations. &nbsp;The risk of such catastrophes is subject to screening off effects that severely limit the potential for reducing them by raising the probability of salvation simulations. &nbsp;For such catastrophes, interventions that raise the probability of salvation simulations in which they are avoided will do so only conditional on the catastrophe not happening&mdash;but, conditional on the catastrophe not happening, its probability of happening is insensitive to the probability of salvation simulations. &nbsp;Failing to take this into account would encourage misallocation of resources to salvation simulations.</span></li><li class="c3 c9 li-bullet-0"><span class="c8 c7">Neglecting world-scale when analyzing simulations as non-causal interventions.</span></li></ul><ul class="c10 lst-kix_tsc2do13t185-1 start"><li class="c2 li-bullet-0"><span class="c7">The most obvious way this could happen would be if we initially analyzed catastrophic risks in our universe and then considered the impact of simulation hypotheses and simulations as non-causal interventions on </span><span class="c7 c17">those </span><span class="c7">risks without considering their impact on the world that contains our simulation along with our simulator&rsquo;s level of reality and whatever else exists. &nbsp;Taking the latter into account could be crucial on some views of value, notably those on which a world&rsquo;s value doesn&rsquo;t scale with good- and bad- making features. For example, suppose there is an upper bound on how much value good-making features in a world can collectively generate but no bound on how much disvalue bad-making features in a world can generate. On this view, finding out that our universe is a simulation in a world that likely contains many other simulations would suggest that there is almost certainly no way for us to make the world better by increasing the number of good making features but much we can do to improve the world by reducing the number of bad-making features that our universe will contain. &nbsp;This view would then give us reason to, for example, prioritize preventing </span><span>a future with catastrophic quantities of suffering</span><span class="c8 c7">&nbsp;over bringing about a grand future. &nbsp;Similarly, views on which scaling up good- and bad-making features equally tends to diminish the value of a world would give us reason to run anti-simulation hypotheses so as to reduce the expected size of our world and lessen this unwanted scaling effect.</span></li></ul><h1 class="c3 c25" id="h.livxbled7n17"><span class="c26 c27 c23 c33 c21">14. Open Questions and Avenues for Future Research</span></h1><p class="c3"><span class="c7 c23">By way of conclusion, I&rsquo;ll highlight what I regard as some key open questions we&rsquo;ve encountered along with some promising avenues for future research. &nbsp;I&rsquo;ll classify these in a rough and ready way by research area. &nbsp;A disproportionate number of questions and research avenues will belong to areas of philosophy, which is my own field of expertise. This merely reflects my being in a better position to identify promising topics in philosophy. &nbsp;This collection is by no means comprehensive, and I would be delighted </span><span class="c23">if others improved upon it.</span></p><ul class="c10 lst-kix_nb7x6iyvcsx2-0 start"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Epistemology</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">How should unstable evidence for the simulation hypothesis be handled? Under what conditions if any does the fact that a hypothesis renders evidence unstable mean that the hypothesis should be ignored?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">The reference problem for observation selection effects has largely been approached through a priori arguments. There is a project of teasing out empirical consequences of different reference class hypotheses, comparing them with our evidence, and tracing the impact on associated risks via Fermi&rsquo;s paradox, the simulation argument, the doomsday argument, and evolutionary arguments for easy AI.</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">The literature on self-locating belief in the last two decades is sprawling, technical, and lacking in uniform terminology. This suggests a few projects that could facilitate analysis of catastrophic risks that interact with self-locating beliefs:</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">It would be helpful to have an accessible and up-to-date synthesis of the self-locating belief literature.</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">A critical mass of researchers could agree to terminological conventions concerning self-locating belief and announce this to relevant research communities.</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">A sensitivity analysis of how different interactions between self-locating belief and catastrophic risks are fragile/robust under different views about self-locating belief could provide strategic guidance even without resolving debates surrounding self-locating belief.</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c26 c23">A sensitivity analysis of how different views of general accounts about justification and evidence (ones not specifically concerned with self-locating information) bear on the simulation argument.</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Formulating and evaluating the simulation argument in terms of evidentially stable simulations</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Philosophy of mind / cognitive science / AI</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">What tests should we use to assess whether simulations are conscious?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">What criteria should we use when evaluating whether simulations are conscious and, if so, what sorts of experience they have?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">What can we do to reduce the probability that simulated systems we create are conscious?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">What can we do to reduce the risk that simulated systems we create suffer?</span></li><li class="c2 li-bullet-0"><span class="c7 c23">What can we do to increase the probability that simulated systems we create have positively valenced experiences?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">How does psychophysical fine-tuning evidence bear on the simulation hypothesis and associated catastrophic risks?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Philosophy of physics</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Interactions between the simulation argument and the Boltzmann brain problem</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Interactions between cosmological and planetary fine-tuning arguments and the simulation hypothesis</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Metaethics</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Which evolutionary debunking arguments are amenable to simulation testing? How can we test them? What is the role of such tests in solving the alignment problem?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Ethics</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Aside from hedonic constraints, what other ethical constraints need to be respected in creating simulations containing minds? What would be best practices for satisfying these constraints?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Sensitivity analysis of how bad shutdown would be on different ethical theories</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Taxonomy of theories in population ethics that lead to downside focus or asymmetrically diminishing returns in large-worlds, along with those theories&rsquo; motivations and problems</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Philosophy of Religion</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">How do different simulation solutions to problems for design hypotheses affect the probability of religious catastrophes?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">How does the simulation hypothesis interact with arguments for/against different design hypotheses and associated catastrophic risks? For instance, which arguments (if successful) support the existence of God but not the existence of a non-divine simulator?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Decision theory</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Research on the following could put us in a better position to evaluate the expected value of non-causal simulation interventions.</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">Adjudicating between causal vs. non-causal decision theories</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">Systematic exploration of the space of decision theories (in contrast to narrow focus on causal vs. evidential vs. functional decision theory)</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What is the best version of the evidentialist wager argument for non-causal simulation interventions?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">How robust is that argument to different decision-theoretic assumptions concerning both first-order and higher-order theories?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What should we make of arguments for causal and non-causal theories sharing predictions in a wider range of cases than is standardly supposed? &nbsp;If any of these arguments work, under what conditions do these theories in fact yield the same predictions?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Forecasting</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Taxonomy of technologies that are relevant to both simulation and catastrophic risk</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Forecasting development timelines for these technologies</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Forecasting use of these technologies conditional on arrival</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Forecasting catastrophe conditional on the use of these technologies</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Risk modeling</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">How can simulations be used to yield better estimates of catastrophic risks and the efficacy of candidate interventions?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">What are the main risk factors for misestimation? How can these be mitigated?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">How can existing risk models be adapted to take into account information about changes in information about whether we&rsquo;re in a simulation</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Strategic analysis</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Shutdown risk. Research on the following could put us in a better position to evaluate and mitigate shutdown risks:</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">How can we fruitfully taxonomize shutdown risks?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">How credible are different shutdown risks?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">How can different shutdown risks be mitigated?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">How do shutdown risks interact with other risks?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c26 c23">How do different simulation scenarios impact religious catastrophic risks?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">Systematic exploration religious catastrophic risks</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">Systematic exploration of interactions between simulation and religious catastrophic risks</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Developing non-causal simulation intervention as a strategy for risk reduction</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What type of interventions to use?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">Are there any viable near-term non-causal interventions?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What&rsquo;s the relative importance of different non-causal interventions vs. other such interventions and vs. causal interventions?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c26 c23">What are the risk-reducing prospects for simulation refuges and fallbacks? How can these be improved?</span></li><li class="c2 li-bullet-0"><span class="c7 c23 c26">What sort of differential technological progress would reduce the catastrophic risks associated with simulation? &nbsp;How can we induce such progress?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">To what extent does the potential for simulation refuges and fallbacks to reduce risks tell in favor of speeding up simulation technology?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c26 c23">What sorts of policies would reduce simulation-related catastrophic risks? &nbsp;How would these policies bear on other catastrophic risks?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">What roles for simulations are promising in grand future scenarios?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">Social science</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">There are various projects of simulating different catastrophic risks and factors that are directly or indirectly relevant to such risks in order to reduce our uncertainty about them and to test risk reduction strategies.</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">There are also various projects of simulating catastrophic risks in order to illustrate realistic catastrophic scenarios in a way that engages public and policymaker attention more than abstract analyses and models.</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Do games, interactive simulations, and/or immersive simulations that simulate risks to promote risk responsiveness? If so, how can they be designed to better promote risk responsiveness?</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Simulations that test hypotheses about value dynamics.</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Simulations of risk scenarios that test hypotheses about the impact of different cognitive capacities and/or biases on navigating those scenarios.</span></li><li class="c2 li-bullet-0"><span class="c7 c23">Research on using si</span><span class="c23">mulations to enhance </span><span class="c7 c26 c23">cognitive capacities and diminish biases that affect catastrophic risk.</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">AI Governance</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c23">What policies should be put in place to reduce the catastrophic risks associated with simulations?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-2 start"><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What sorts of policies would prevent catastrophic simulations (ones realizing immense quantities of moral disvalue) in the context of games, entertainment, research, digital economies, wars, and power-seeking, morally indifferent, or malevolent actors?</span></li><li class="c3 c16 li-bullet-0"><span class="c7 c26 c23">What policies should be enacted to reduce catastrophic risks associated with dual uses of simulations? &nbsp;</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1"><li class="c2 li-bullet-0"><span class="c7 c23">When is the best time to</span><span class="c23">&nbsp;promote such policies</span><span class="c7 c26 c23">? </span></li><li class="c2 li-bullet-0"><span class="c7 c23">What policies does it make sense to push for now</span><span class="c23">? H</span><span class="c7 c26 c23">ow should this be done?</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-0"><li class="c3 c9 li-bullet-0"><span class="c7 c26 c23">AI Safety</span></li></ul><ul class="c10 lst-kix_nb7x6iyvcsx2-1 start"><li class="c2 li-bullet-0"><span class="c7 c26 c23">Developing nested simulations for safety testing</span></li><li class="c2 li-bullet-0"><span class="c7 c26 c23">Developing nested simulations for boxing purposes or for the purpose of incentivizing aligned behavior</span></li><li class="c2 li-bullet-0"><span class="c7 c23">Using simulations </span><span class="c7 c26 c23">to find overlooked threat models and interventions</span></li><li class="c2 li-bullet-0"><span class="c23">Using simulations to vividly demonstrate dangers posed by AI to relevant parties that underestimate these dangers</span></li></ul><h1 class="c3 c25" id="h.vfff1n7h1k1d"><span>R</span><span class="c29">eferences</span></h1><p class="c3"><span class="c29">Adams, M. (2000). </span><span class="c17 c29">Horrendous evils and the goodness of God</span><span class="c8 c7">. Cornell University Press.</span></p><p class="c3"><span class="c29">Alexander, S. (2019) </span><span class="c24"><a class="c13" href="https://slatestarcodex.com/2019/08/21/dont-fear-the-simulators/">Don&rsquo;t Fear The Simulators</a></span><span class="c29 c34">. Slate Star Codex.</span></p><p class="c3"><span class="c1">Althaus, D., &amp; Baumann, T. (2020). Reducing long-term risks from malevolent actors. Publications of Center of Long Term Risk.</span></p><p class="c3"><span class="c11">Anthis, J.R. (2018) &ldquo;Why I Prioritize Moral Circle Expansion Over Artificial Intelligence Alignment.&rdquo; Effective Altruism Forum. </span><span class="c24"><a class="c13" href="https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial">https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial</a></span><span class="c1">.</span></p><p class="c3"><span class="c11">Anthis, J.R. (2022) The Future Might Not Be So Great. URL: </span><span class="c24"><a class="c13" href="https://www.sentienceinstitute.org/blog/the-future-might-not-be-so-great">https://www.sentienceinstitute.org/blog/the-future-might-not-be-so-great</a></span></p><p class="c3"><span class="c11">Armstrong, S.; Sandberg, A. &amp; Bostrom, N. (2012). Thinking Inside the Box: Controlling and Using an Oracle AI. </span><span class="c0">Minds and Machines </span><span class="c1">22 (4):299-324.</span></p><p class="c3"><span class="c11">Babcock, J., Kram&aacute;r, J., &amp; Yampolskiy, R. V. (2019). Guidelines for artificial intelligence containment. </span><span class="c0">Next-Generation Ethics: Engineering a Better Society</span><span class="c1">, 90-112.</span></p><p class="c3"><span class="c11">Banks, I.M. (2010) </span><span class="c0">Surface Detail</span><span class="c1">, London: Orbit.</span></p><p class="c3"><span class="c11">Barak B. &amp; Edelman B. (2022) AI will change the world, but won&rsquo;t take it over by playing &ldquo;3-dimensional chess&rdquo;. URL: </span><span class="c24"><a class="c13" href="https://www.alignmentforum.org/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3#Our_argument__an_executive_summary_">https://www.alignmentforum.org/posts/zB3ukZJqt3pQDw9jz/ai-will-change-the-world-but-won-t-take-it-over-by-playing-3#Our_argument__an_executive_summary_</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Barnett, Z. &amp; Li, H. (2016). Conciliationism and merely possible disagreement. </span><span class="c0">Synthese</span><span class="c1">&nbsp;193 (9):1-13.</span></p><p class="c3"><span class="c11">Baum, S. D., Denkenberger, D. C., &amp; Haqq-Misra, J. (2015). Isolated refuges for surviving global catastrophes. </span><span class="c0">Futures</span><span class="c1">, 72, 45-56.</span></p><p class="c3"><span class="c20">Baumann, T. (2022) </span><span class="c20 c17">Avoiding the Worst: How to Prevent a Moral Catastrophe</span><span class="c20">&nbsp;URL: </span><span class="c28 c34"><a class="c13" href="https://centerforreducingsuffering.org/wp-content/uploads/2022/10/Avoiding_The_Worst_final.pdf">https://centerforreducingsuffering.org/wp-content/uploads/2022/10/Avoiding_The_Worst_final.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1">BBC News. (2010) &quot;Stephen Hawking warns over making contact with aliens&quot;. URL: </span></p><p class="c3"><span class="c24"><a class="c13" href="http://news.bbc.co.uk/2/hi/uk_news/8642558.stm">http://news.bbc.co.uk/2/hi/uk_news/8642558.stm</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Beckstead, N. (2015). How much could refuges help us recover from a global catastrophe?. </span><span class="c0">Futures</span><span class="c1">, 72, 36-44.</span></p><p class="c3"><span class="c11">Bedke, (2009). Intuitive non-naturalism meets cosmic coincidence. </span><span class="c0">Pacific Philosophical Quarterly </span><span class="c1">90 (2):188-209.</span></p><p class="c3"><span class="c11">Benatar, D. (2008). </span><span class="c0">Better never to have been: The harm of coming into existence</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Bennett, C.H., Hanson, R., &amp; Riedel, C.J. (2019). Comment on &lsquo;The Aestivation Hypothesis for Resolving Fermi&rsquo;s Paradox&rsquo;. </span><span class="c0">Foundations of Physics</span><span class="c1">, 49(8), 820-829.</span></p><p class="c3"><span class="c1">Bentham, J. (1791). Panopticon: or, The inspection-house. Containing the idea of a new principle of construction applicable to any sort of establishment, in which persons of any description are to be kept under inspection, etc. Thomas Byrne.</span></p><p class="c3"><span class="c11">Benton, M. A., Hawthorne, J., &amp; Isaacs, Y. (2016). Evil and evidence. </span><span class="c0">Oxford Studies in Philosophy of Religion</span><span class="c1">, Vol. 7, 1-31.</span></p><p class="c3"><span class="c11">Birch, J. (2013). On the &lsquo;simulation argument&rsquo; and selective scepticism. </span><span class="c0">Erkenntnis</span><span class="c1">, 78(1), 95-107.</span></p><p class="c3"><span class="c11">Bhogal, H. (forthcoming). What&#39;s the Coincidence in Debunking? </span><span class="c0">Philosophy and Phenomenological Research</span><span class="c1">.</span></p><p class="c3"><span class="c11">Bogardus, T. (2016). Only All Naturalists Should Worry About Only One Evolutionary Debunking Argument. </span><span class="c0">Ethics </span><span class="c1">126 (3):636-661.</span></p><p class="c3"><span class="c11">Bostrom, N. (2002</span><span class="c0">a</span><span class="c11">). Existential risks: Analyzing human extinction scenarios and related hazards. </span><span class="c0">Journal of Evolution and technology</span><span class="c1">, 9.</span></p><p class="c3"><span class="c11">Bostrom, N. (2002</span><span class="c0">b</span><span class="c11">) </span><span class="c0">Anthropic bias: observation selection effects in science and philosophy</span><span class="c1">. New York, NY: Routledge.</span></p><p class="c3"><span class="c11">Bostrom, N. (2003</span><span class="c0">a</span><span class="c11">). Are we living in a computer simulation?. </span><span class="c0">The philosophical quarterly</span><span class="c1">, 53(211), 243-255.</span></p><p class="c3"><span class="c11">Bostrom, N. (2003</span><span class="c0">b</span><span class="c11">). Ethical issues in advanced artificial intelligence. In </span><span class="c0">Science fiction and philosophy: from time travel to superintelligence</span><span class="c1">. </span></p><p class="c3"><span class="c11">Bostrom, N. (2005). The simulation argument: Reply to Weatherson. </span><span class="c0">The Philosophical Quarterly</span><span class="c1">, 55(218), 90-97.</span></p><p class="c3"><span class="c11">Bostrom N. (2008) Where are they? Why I hope the search for extraterrestrial life finds nothing. </span><span class="c0">MIT Technology Review </span><span class="c1">May/June: 72-77.</span></p><p class="c3"><span class="c11">Bostrom, N. (2011) The Simulation Argument FAQ. URL: </span><span class="c24"><a class="c13" href="http://www.simulation-argument.com/faq.html">http://www.simulation-argument.com/faq.html</a></span></p><p class="c3"><span class="c11">Bostrom, N. (2014). </span><span class="c0">Superintelligence: paths, dangers, strategies</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Bostrom, N. (2019). The vulnerable world hypothesis. </span><span class="c0">Global Policy</span><span class="c11">, </span><span class="c0">10</span><span class="c1">(4), 455-476.</span></p><p class="c3"><span class="c11">Bostrom, N., &amp; &#262;irkovi&#263;, M. M. (Eds.). (2008). </span><span class="c0">Global catastrophic risks</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Bourget, D. &amp; Chalmers, D.., (2021) Philosophers on Philosophy: The 2020 PhilPapers Survey. URL: </span><span class="c24"><a class="c13" href="https://philpapers.org/go.pl?id%3DBOUPOP-3%26u%3Dhttps%253A%252F%252Fphilpapers.org%252Farchive%252FBOUPOP-3.pdf">https://philpapers.org/go.pl?id=BOUPOP-3&amp;u=https%3A%2F%2Fphilpapers.org%2Farchive%2FBOUPOP-3.pdf</a></span></p><p class="c3"><span class="c11">Braddon-Mitchell, D., &amp; Latham, A. J. (2022). Ancestor simulations and the Dangers of Simulation Probes. </span><span class="c0">Erkenntnis</span><span class="c1">, 1-11.</span></p><p class="c3"><span class="c11">Buchak, L. M. (2013). </span><span class="c0">Risk and rationality</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Bykvist, K. (2017). Moral uncertainty. </span><span class="c0">Philosophy Compass</span><span class="c1">, 12(3), e12408.</span></p><p class="c3"><span class="c11">Carroll, S.M. (2020). Why Boltzmann brains are bad. In </span><span class="c0">Current Controversies in Philosophy of Science</span><span class="c1">&nbsp;(pp. 7-20). Routledge.</span></p><p class="c3"><span class="c11">Carter B. (1983) The anthropic principle and its implications for biological evolution. </span><span class="c0">Philos Trans R Soc Lond A Math Phys Sci </span><span class="c1">310:347&ndash;363.</span></p><p class="c3"><span class="c11">Chalmers, D. (2003). The Matrix as metaphysics. </span><span class="c0">Science Fiction and Philosophy: From Time Travel to Superintelligence</span><span class="c1">.</span></p><p class="c3"><span class="c11">Chalmers, D. (2006). Perception and the Fall from Eden. </span><span class="c0">Perceptual experience</span><span class="c1">, 49-125.</span></p><p class="c3"><span class="c11">Chalmers, D. (2010). The Singularity. </span><span class="c0">Journal of Consciousness Studies</span><span class="c11">, 17(9-10), 7-65.</span></p><p class="c3"><span class="c11">Chalmers, D. (2018</span><span class="c0">a</span><span class="c11">). The meta-problem of consciousness. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 25(9-10).</span></p><p class="c3"><span class="c11">Chalmers, D. (2018</span><span class="c0">b</span><span class="c11">). Structuralism as a response to skepticism. </span><span class="c0">The Journal of Philosophy</span><span class="c1">, 115, 625&ndash;660.</span></p><p class="c3"><span class="c11">Chalmers, D. (2020). Debunking Arguments for Illusionism about Consciousness. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 27(5-6), 258-281.</span></p><p class="c3"><span class="c11">Chalmers, D. J. (2022). </span><span class="c0">Reality+: Virtual worlds and the problems of philosophy</span><span class="c1">. Penguin UK.</span></p><p class="c3"><span class="c11">Chen, E.K. (forthcoming). Time&#39;s Arrow and Self-Locating Probability. </span><span class="c0">Philosophy and Phenomenological Research</span><span class="c1">.</span></p><p class="c3"><span class="c11">Christensen, D. (2016). Conciliation, Uniqueness and Rational Toxicity. </span><span class="c0">No&ucirc;s</span><span class="c1">, 50(3), 584-603.</span></p><p class="c3"><span class="c11">&#262;irkovi&#263;, M.M. (2008). Observation selection effects and global catastrophic risks. </span><span class="c0">Global Catastrophic Risks</span><span class="c1">. OUP. pp. 120-145.</span></p><p class="c3"><span class="c11">&#262;irkovi&#263;, M.M. (2015). Linking simulation argument to the AI risk. </span><span class="c0">Futures</span><span class="c1">, 72, 27-31.</span></p><p class="c3"><span class="c11">&#262;irkovi&#263;, M.M. (2018). </span><span class="c0">The great silence: Science and philosophy of Fermi&#39;s paradox</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Clarke-Doane, J. (2020). </span><span class="c0">Morality and Mathematics</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c20">Christiano, P. (2019) &ldquo;What failure looks like&rdquo; </span><span class="c20 c17">Alignment forum</span><span class="c1">.</span></p><p class="c3"><span class="c1">Cotra, A. (2020) Forecasting Transformative AI with Biological Anchors. Open Philanthropy.</span></p><p class="c3"><span class="c20">Cotra, A. (2022) Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover. </span><span class="c20 c17">Alignment forum</span><span class="c1">.</span></p><p class="c3"><span class="c11">Crawford, L. (2013). Freak observers and the simulation argument. </span><span class="c0">Ratio</span><span class="c1">, 26(3), 250-264.</span></p><p class="c3"><span class="c20">Critch, A. &nbsp;(2021)What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes. </span><span class="c0 c30 c27">Alignment forum.</span></p><p class="c3"><span class="c11">Crummett, D. (2017). Sufferer-centered requirements on theodicy and all-things-considered harms. </span><span class="c0">Oxford Studies in Philosophy of Religion</span><span class="c1">, 8.</span></p><p class="c3"><span class="c11">Crummett, D. (2020). The real advantages of the simulation solution to the problem of natural evil. </span><span class="c0">Religious Studies</span><span class="c1">, 1-16.</span></p><p class="c3"><span class="c11">Cuneo, Terence (2007). </span><span class="c0">The Normative Web: An Argument for Moral Realism</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Cutter, B., &amp; Crummett, D. (forthcoming) Psychophysical Harmony: A New Argument for Theism. </span><span class="c0">Oxford Studies in Philosophy of Religion</span><span class="c1">.</span></p><p class="c3"><span class="c11">Cutter, B. (2021). Perceptual illusionism. </span><span class="c0">Analytic Philosophy</span><span class="c1">.</span></p><p class="c3"><span class="c11">Cutter &amp; Saad (</span><span class="c20">forthcoming</span><span class="c11">) The Problem of Nomological Harmony. Manuscript. </span><span class="c0">Nous</span><span class="c1">.</span></p><p class="c3"><span class="c11">Dainton, B. (2012). On singularities and simulations. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 19(1-2), 42-85.</span></p><p class="c3"><span class="c11">Dainton, B. (2020). Natural evil: the simulation solution. </span><span class="c0">Religious Studies</span><span class="c1">, 56(2), 209-230.</span></p><p class="c3"><span class="c11">Danaher, J. (2015). Why AI doomsayers are like sceptical theists and why it matters. </span><span class="c0">Minds and Machines</span><span class="c1">, 25(3), 231-246.</span></p><p class="c3"><span class="c1">Daniel, M. (2017). S-risks: Why they are the worst existential risks, and how to prevent them (EAG Boston 2017). Foundational research institute.</span></p><p class="c3"><span class="c20">Davidson, T. (2023) Continuous doesn&rsquo;t mean slow. URL: </span><span class="c28 c34"><a class="c13" href="https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/">https://www.planned-obsolescence.org/continuous-doesnt-mean-slow/</a></span><span class="c1">&nbsp; </span></p><p class="c3"><span class="c11">Dello-Iacovo, M.A. (2017) On terraforming, wild-animal suffering and the far future. </span><span class="c0">Sentience Politics</span><span class="c1">.</span></p><p class="c3"><span class="c11">Dreier, J. (2012). Quasi&#8208;realism and the problem of unexplained coincidence. </span><span class="c0">Analytic Philosophy</span><span class="c1">, 53(3), 269-287.</span></p><p class="c3"><span class="c1">Dogramaci, S. (2017). Explaining our moral reliability. Pacific Philosophical Quarterly, 98, 71-86.</span></p><p class="c3"><span class="c11">Dogramaci, S. (2020). Does my total evidence support that I&rsquo;m a Boltzmann Brain? </span><span class="c0">Philosophical Studies </span><span class="c1">177 (12):3717-3723.</span></p><p class="c3"><span class="c20">Doody, R. (2022). Don&#39;t Go Chasing Waterfalls: Against Hayward&#39;s &ldquo;Utility Cascades&rdquo;. </span><span class="c20 c17">Utilitas </span><span class="c1">34 (2):225-232.</span></p><p class="c3"><span class="c11">Dorr, C., &amp; Arntzenius, F. (2017). Self-locating priors and cosmological measures. </span><span class="c0">The philosophy of cosmology</span><span class="c1">, 396-428.</span></p><p class="c3"><span class="c11">Easwaran, K. (2021). A classification of Newcomb problems and decision theories. </span><span class="c0">Synthese</span><span class="c1">, 198(27), 6415-6434.</span></p><p class="c3"><span class="c11">Elamrani, A., &amp; Yampolskiy, R. V. (2019). Reviewing tests for machine consciousness. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 26(5-6), 35-64.</span></p><p class="c3"><span class="c11">Elga, A. (2000). Self-locating belief and the sleeping beauty problem. </span><span class="c0">Analysis </span><span class="c1">60 (2):143&ndash;147.</span></p><p class="c3"><span class="c11">Elga, A. (2004). Defeating dr. evil with self-locating belief. </span><span class="c0">Philosophy and Phenomenological Research </span><span class="c1">69 (2):383&ndash;396.</span></p><p class="c3"><span class="c11">Elga, A. (2008). &ldquo;Lucky to be rational.&rdquo; Unpublished manuscript. URL: &nbsp;</span><span class="c24"><a class="c13" href="https://www.princeton.edu/~adame/papers/bellingham-lucky.pdf">https://www.princeton.edu/~adame/papers/bellingham-lucky.pdf</a></span><span class="c1">. </span></p><p class="c3"><span class="c11">Enoch, D. (2011). </span><span class="c0">Taking Morality Seriously: A Defense of Robust Realism</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c20">Erez, F. (2023). Ought we align the values of artificial moral agents?. </span><span class="c20 c17">AI and Ethics</span><span class="c1">, 1-10.</span></p><p class="c3"><span class="c11">Everitt, N. (2004). </span><span class="c0">The non-existence of God</span><span class="c1">. Routledge.</span></p><p class="c3"><span class="c11">Fitzpatrick, L. (2009). &quot;A Brief History of China&#39;s One-Child Policy&quot;. </span><span class="c0">Time</span><span class="c1">. ISSN 0040-781X</span></p><p class="c3"><span class="c11">Friederich, S., &quot;Fine-Tuning&quot;, </span><span class="c0">The Stanford Encyclopedia of Philosophy </span><span class="c11">(Winter 2018 Edition), E.N. Zalta (ed.), URL = &lt;</span><span class="c24"><a class="c13" href="https://plato.stanford.edu/archives/win2018/entries/fine-tuning/">https://plato.stanford.edu/archives/win2018/entries/fine-tuning/</a></span><span class="c1">&gt;.</span></p><p class="c3"><span class="c20">Gabriel, I. (2020). Artificial intelligence, values, and alignment. </span><span class="c20 c17">Minds and machines</span><span class="c1">, 30(3)411-437.</span></p><p class="c3"><span class="c11">Gloor, L. (2016) &ldquo;The Case for Suffering-Focused Ethics&rdquo; URL: </span><span class="c24"><a class="c13" href="https://longtermrisk.org/the-case-for-suffering-focused-ethics/">https://longtermrisk.org/the-case-for-suffering-focused-ethics/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Gloor, L. (2018) &ldquo;Cause prioritization for downside-focused value systems&rdquo; URL: </span><span class="c24"><a class="c13" href="https://longtermrisk.org/cause-prioritization-downside-focused-value-systems/">https://longtermrisk.org/cause-prioritization-downside-focused-value-systems/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Goff, P. (2018). Conscious thought and the cognitive fine-tuning problem. </span><span class="c0">Philosophical Quarterly</span><span class="c1">,</span></p><p class="c3"><span class="c1">68(270), 98&ndash;122.</span></p><p class="c3"><span class="c1">Grace, C. (2010). Anthropic reasoning in the great filter. Unpublished manuscript.</span></p><p class="c3"><span class="c20">Greaves, H. (2016, October). Cluelessness. In </span><span class="c20 c17">Proceedings of the Aristotelian Society</span><span class="c1">&nbsp;(Vol. 116, No. 3, pp. 311-339). OUP.</span></p><p class="c3"><span class="c11">Greaves, H., Cotton-Barratt (2019) &ldquo;A bargaining-theoretic approach to moral uncertainty&rdquo; URL: </span><span class="c24"><a class="c13" href="https://globalprioritiesinstitute.org/wp-content/uploads/2020/Cotton-Barratt_%2520Greaves_bargaining_theoretic.pdf">https://globalprioritiesinstitute.org/wp-content/uploads/2020/Cotton-Barratt_%20Greaves_bargaining_theoretic.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Green, A., &amp; Stump, E. (Eds.). (2015). </span><span class="c0">Hidden Divinity and Religious Belief</span><span class="c1">. Cambridge.</span></p><p class="c3"><span class="c1">Greene, J. (2007). The secret joke of Kant&rsquo;s soul. In W. Sinnott-Armstrong (ed.), Moral Psychology, Vol. 3. MIT Press.</span></p><p class="c3"><span class="c11">Greene, P. (2020). The Termination Risks of Simulation Science. </span><span class="c0">Erkenntnis </span><span class="c1">85(2):489-509.</span></p><p class="c3"><span class="c20">Gustafsson, J.E., &amp; Peterson, M. (2012). A computer simulation of the argument from disagreement. </span><span class="c20 c17">Synthese</span><span class="c1">, 184(3), 387-405.</span></p><p class="c3"><span class="c11">H&auml;ggstr&ouml;m, O. (2021) AI, orthogonality and the M&uuml;ller-Cannon instrumental vs general intelligence distinction. URL: </span><span class="c24"><a class="c13" href="https://arxiv.org/ftp/arxiv/papers/2109/2109.07911.pdf">https://arxiv.org/ftp/arxiv/papers/2109/2109.07911.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Hanson, R., (1998). The Great Filter - Are We Almost Past It? URL: </span><span class="c24"><a class="c13" href="http://hanson.gmu.edu/greatfilter.html">http://hanson.gmu.edu/greatfilter.html</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Hanson, R. (2001). How to live in a simulation. </span><span class="c0">Journal of Evolution and Technology</span><span class="c1">, 7(1), 3-13.</span></p><p class="c3"><span class="c11">Hanson, R. (2016). </span><span class="c0">The age of Em: Work, love, and life when robots rule the Earth</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c1">Hanson, R., Martin, D., McCarter, C., &amp; Paulson, J. (2021). If Loud Aliens Explain Human Earliness, Quiet Aliens Are Also Rare. The Astrophysical Journal, 922(2),182.</span></p><p class="c3"><span class="c20">Hayward, M.K. (2020). Utility cascades. </span><span class="c20 c17">Analysis</span><span class="c1">&nbsp;80 (3):433-442.</span></p><p class="c3"><span class="c11">Hill, R. R., &amp; Tolk, A. (2017). A history of military computer simulation. In </span><span class="c0">Advances in Modeling and Simulation </span><span class="c1">(pp. 277-299). Springer.</span></p><p class="c3"><span class="c11">Hoffman, D. (2019). </span><span class="c0">The case against reality: Why evolution hid the truth from our eyes.</span><span class="c1">&nbsp;WW Norton &amp; Company.</span></p><p class="c3"><span class="c1">Huang, S., ... &amp; Wei, F. (2023). Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045.</span></p><p class="c3"><span class="c11">Isaacs, Y.; Hawthorne, J. &amp; Russell, J.S. (forthcoming). Multiple Universes and Self-Locating Evidence. </span><span class="c0">Philosophical Review</span><span class="c1">.</span></p><p class="c3"><span class="c11">Ichikawa, J. (2011). Quantifiers, Knowledge, and Counterfactuals. </span><span class="c0">Philosophy and Phenomenological Research</span><span class="c1">&nbsp;82 (2):287 - 313.</span></p><p class="c3"><span class="c11">James, W. (1890). </span><span class="c0">The principles of psychology </span><span class="c1">(Vol. 2). New York: Henry Holt and Company</span></p><p class="c3"><span class="c11">Jaquet, F. (2022). Speciesism and tribalism: embarrassing origins. </span><span class="c0">Philosophical Studies</span><span class="c1">, 179(3), 933-954.</span></p><p class="c3"><span class="c11">Johnson, D.K. (2011). Natural evil and the simulation hypothesis. </span><span class="c0">Philo</span><span class="c1">, 14(2), 161-175.</span></p><p class="c3"><span class="c11">Joyce, R. (2007). </span><span class="c0">The evolution of morality</span><span class="c1">. MIT press., </span></p><p class="c3"><span class="c20">Karnofsky, H. (2021). &ldquo;All Possible Views About Humanity&#39;s Future Are Wild&rdquo; URL: </span><span class="c28 c34"><a class="c13" href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/">https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1">Kagan, S. (2000). Evaluative focal points. Morality, rules, and consequences: A critical reader, 134-55.</span></p><p class="c3"><span class="c11">Knab, B. (2019) &nbsp;</span><span class="c0">Three Problems in Formal Epistemology</span><span class="c1">. Dissertation.</span></p><p class="c3"><span class="c1">Arden Koehler, Benjamin Todd, Robert Wiblin and Keiran Harris &nbsp;(2020) &ldquo;BenjaminTodd on varieties of longtermism and things 80,000 Hours might be getting wrong&rdquo; Podcast. URL:</span></p><p class="c3"><span class="c24"><a class="c13" href="https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/#articles-books-and-other-media-discussed-in-the-show">https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/#articles-books-and-other-media-discussed-in-the-show</a></span></p><p class="c3"><span class="c11">Korman, D.Z. (2019). Debunking arguments. </span><span class="c0">Philosophy Compass </span><span class="c1">14 (12).</span></p><p class="c3"><span class="c11">Kotzen, M. (2020). What Follows from the Possibility of Boltzmann Brains?. In </span><span class="c0">Current Controversies in Philosophy of Science </span><span class="c1">(pp. 21-34). Routledge.</span></p><p class="c3"><span class="c11">Kraay, K. (2010) &quot;Theism, Possible Worlds, and the Multiverse.&quot; </span><span class="c0">Philosophical Studies </span><span class="c1">147:355-368.</span></p><p class="c3"><span class="c11">Kraay, </span><span class="c20">K. </span><span class="c11">(ed.) (2014). </span><span class="c0">God and the Multiverse: Scientific, Philosophical, and Theological Perspectives</span><span class="c1">. Routledge.</span></p><p class="c3"><span class="c11">Ladak, </span><span class="c20">A.</span><span class="c11">(2022) Is Artificial Consciousness Possible? A Summary of Selected Books. URL: </span><span class="c24"><a class="c13" href="https://www.sentienceinstitute.org/blog/is-artificial-consciousness-possible">https://www.sentienceinstitute.org/blog/is-artificial-consciousness-possible</a></span><span class="c1">.</span></p><p class="c3"><span class="c11">de Lazari-Radek, K., &amp; Singer, P. (2012). The objectivity of ethics and the unity of practical reason. </span><span class="c0">Ethics</span><span class="c1">. 123(1)9-31.</span></p><p class="c3"><span class="c20">Lawsen, A. (2023) AI x-risk, approximately ordered by embarrassment. </span><span class="c20 c17">Alignment forum</span><span class="c1">.</span></p><p class="c3"><span class="c11">Lee, G. (2019). 13 Alien Subjectivity and the Importance of Consciousness. </span><span class="c0">Blockheads!: Essays on Ned Block&#39;s Philosophy of Mind and Consciousness</span><span class="c1">, 215.</span></p><p class="c3"><span class="c11">Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt, T., Lefrancq, A., ... &amp; Legg, S. (2017). AI safety gridworlds. URL: </span><span class="c24"><a class="c13" href="https://arxiv.org/pdf/1711.09883.pdf">https://arxiv.org/pdf/1711.09883.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Lenman, J. (2002). On becoming extinct. </span><span class="c0">Pacific Philosophical Quarterly </span><span class="c1">83 (3):253&ndash;269.</span></p><p class="c3"><span class="c11">Leslie, J. (1989) </span><span class="c0">Universes</span><span class="c1">. Routledge. [reprinted in 1996]</span></p><p class="c3"><span class="c11">Leslie, J. (1991). Ensuring two Bird deaths with one throw. </span><span class="c0">Mind </span><span class="c1">100 (1):73-86.</span></p><p class="c3"><span class="c11">Leslie, J. (1996), </span><span class="c0">The End of the World: The Science and Ethics of Human Extinction</span><span class="c1">. Routledge.</span></p><p class="c3"><span class="c11">Lewis, D. (2001). Sleeping beauty: reply to Elga. </span><span class="c0">Analysis</span><span class="c1">, 61(3), 171-176.</span></p><p class="c3"><span class="c11">Lewis, D. (2004). How many lives has Schr&ouml;dinger&#39;s cat?. </span><span class="c0">Australasian Journal of Philosophy</span><span class="c1">, 82(1), 3-22.</span></p><p class="c3"><span class="c11">Lewis, P.J. (2013). The doomsday argument and the simulation argument. </span><span class="c0">Synthese</span><span class="c1">, 190(18), 4009-4022.</span></p><p class="c3"><span class="c1">Li, K., Hopkins, A. K., Bau, D., Vi&eacute;gas, F., Pfister, H., &amp; Wattenberg, M. (2022). Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382.</span></p><p class="c3"><span class="c1">Lockhart, T. (2000). Moral uncertainty and its consequences. OUP.</span></p><p class="c3"><span class="c11">Long, R. Key questions about artificial sentience: an opinionated guide. Effective Altruism Forum. URL: </span><span class="c24"><a class="c13" href="https://forum.effectivealtruism.org/posts/gFoWdiGYtXrhmBusH/key-questions-about-artificial-sentience-an-opinionated">https://forum.effectivealtruism.org/posts/gFoWdiGYtXrhmBusH/key-questions-about-artificial-sentience-an-opinionated</a></span></p><p class="c3"><span class="c11">Joyce, R. (2001). </span><span class="c0">The Myth of Morality.</span><span class="c1">&nbsp;Cambridge University Press.</span></p><p class="c3"><span class="c11">MacAskill, W. (2016). Smokers, psychos, and decision-theoretic uncertainty. </span><span class="c0">The Journal of Philosophy</span><span class="c1">, 113(9), 425-445.</span></p><p class="c3"><span class="c11">MacAskill, W. (2022) </span><span class="c0">What We Owe the Future</span><span class="c1">. Basic Books.</span></p><p class="c3"><span class="c11">MacAskill, W., Bykvist, K., &amp; Ord, T. (2020). </span><span class="c0">Moral Uncertainty. </span><span class="c1">OUP.</span></p><p class="c3"><span class="c11">MacAskill, W.; Vallinder, A.; Oesterheld, C.; Shulman, C. &amp; Treutlein, J. (2021). The Evidentialist&#39;s Wager. </span><span class="c0">Journal of Philosophy </span><span class="c1">118 (6):320-342.</span></p><p class="c3"><span class="c11">Mackie, J.L. (1977). </span><span class="c0">Ethics: Inventing Right and Wrong</span><span class="c1">. Penguin Books.</span></p><p class="c3"><span class="c11">Madden JC, Enoch SJ, Paini A, Cronin MTD. A Review of In Silico Tools as Alternatives to Animal Testing: Principles, Resources and Applications. </span><span class="c0">Alternatives to Laboratory Animals</span><span class="c1">. 2020;48(4):146-172.</span></p><p class="c3"><span class="c1">Manley, D. Manuscript. &ldquo;On Being a Random Sample&rdquo;</span></p><p class="c3"><span class="c11">Mason, K. (2010). Debunking arguments and the genealogy of religion and morality. </span><span class="c0">Philosophy Compass</span><span class="c1">, 5(9), 770-778.</span></p><p class="c3"><span class="c11">Megill, J. (2011). Evil and the many universes response. </span><span class="c0">International Journal for Philosophy of Religion</span><span class="c1">&nbsp;70:127&ndash;138.</span></p><p class="c3"><span class="c11">Mogensen A. (2019). Doomsday rings twice. Available at </span><span class="c24"><a class="c13" href="https://globalprioritiesinstitute.org/wp-content/uploads/2019/Mogensen_doomsday_rings_twice.pdf">https://globalprioritiesinstitute.org/wp-content/uploads/2019/Mogensen_doomsday_rings_twice.pdf</a></span><span class="c1">&nbsp; &nbsp;</span></p><p class="c3"><span class="c1">Mogensen, A. (2019). &lsquo;The only ethical argument for positive &#x1d6ff;&rsquo;?. Working paper.</span></p><p class="c3"><span class="c11">Miller, J.D. (2019). When two existential risks are better than one. Foresight, 21(1), 130&ndash;137. </span><span class="c24"><a class="c13" href="https://doi.org/10.1108/FS04-2018-0038">https://doi.org/10.1108/FS04-2018-0038</a></span></p><p class="c3"><span class="c11">Miller, J.D., &amp; Felton, D. (2017). The Fermi paradox, Bayes&rsquo; rule, and existential risk management. </span><span class="c0">Futures</span><span class="c1">, 86, 44-57.</span></p><p class="c3"><span class="c29">Monton, B. (2009). </span><span class="c29 c17">Seeking God in science: an atheist defends intelligent design</span><span class="c8 c7">. Broadview Press.</span></p><p class="c3"><span class="c29">Moravec, H. (1976) &ldquo;The Role of Raw Rower in Intelligence.&rdquo; Unpublished manuscript. URL: </span><span class="c28 c29"><a class="c13" href="http://www.frc.ri.cmu.edu/users/hpm/project.archive/general.articles/1975/Raw.Power.html">http://www.frc.ri.cmu.edu/users/hpm/project.archive/general.articles/1975/Raw.Power.html</a></span><span class="c8 c7">&nbsp;</span></p><p class="c3"><span class="c29">Moravec, H. (1999) </span><span class="c29 c17">Robot: Mere Machine to Transcendent Mind</span><span class="c8 c7">. OUP.</span></p><p class="c3"><span class="c8 c7">Mowshowitz, Z. (2023) A Hypothetical Takeover Scenario Twitter Poll. </span></p><p class="c3"><span class="c29">M&oslash;rch, H. (2018). The evolutionary argument for phenomenal powers. </span><span class="c29 c17">Philosophical Perspectives</span><span class="c8 c7">, 31,</span></p><p class="c3"><span class="c8 c7">293&ndash;316.</span></p><p class="c3"><span class="c29">Muehlhauser, L. (2021) &ldquo;Treacherous turns in the wild&rdquo; </span><span class="c28 c29"><a class="c13" href="http://lukemuehlhauser.com/treacherous-turns-in-the-wild/">http://lukemuehlhauser.com/treacherous-turns-in-the-wild/</a></span><span class="c29">&nbsp;</span></p><p class="c3"><span class="c1">Muehlhauser, L. (2017) A software agent illustrating some features of an </span></p><p class="c3"><span class="c11">illusionist account of consciousness, OpenPhilanthropy, [Online], </span><span class="c24"><a class="c13" href="https://www.openphilanthropy.org/software-agent-illustrating-some-features-illusionistaccount-consciousness">https://www.openphilanthropy.org/software-agent-illustrating-some-features-illusionistaccount-consciousness</a></span></p><p class="c3"><span class="c11">M&uuml;ller, V.C. &amp; Cannon, M. (2021). Existential risk from AI and orthogonality: Can we have it both ways? </span><span class="c0">Ratio </span><span class="c1">35(1):25-36.</span></p><p class="c3"><span class="c11">Murphy, P., &amp; Black, T. (2012). Sensitivity meets explanation: an improved counterfactual condition on knowledge. In K. Becker &amp; T. Black (eds.), </span><span class="c0">The Sensitivity Principle in Epistemology</span><span class="c1">. Cambridge.</span></p><p class="c3"><span class="c11">Newberry, T., &amp; Ord, T. (2021). The Parliamentary Approach to Moral Uncertainty. Technical Report 2021-2, Future of Humanity Institute, University of Oxford. URL: </span><span class="c24"><a class="c13" href="https://www.">https://www. fhi. ox. ac. uk/wpcontent/uploads/2021/06/Parliamentary-Approach-to-Moral-Uncertainty. pdf</a></span></p><p class="c3"><span class="c20">Nowak, A., Gelfand, M. J., Borkowski, W., Cohen, D., &amp; Hernandez, I. (2016). The evolutionary basis of honor cultures. </span><span class="c20 c17">Psychological science</span><span class="c1">, 27(1), 12-24.</span></p><p class="c3"><span class="c11">Nozick, R. (1981). </span><span class="c0">Philosophical Explanations</span><span class="c1">. Harvard University Press.</span></p><p class="c3"><span class="c1">Nozick, R. (1993). The nature of rationality. Princeton University Press.</span></p><p class="c3"><span class="c1">Olds, J., &amp; Milner, P. (1954). Positive reinforcement produced by electrical stimulation of septal area and</span></p><p class="c3"><span class="c11">other regions of rat brain. </span><span class="c0">Journal of Comparative &amp; Physiological Psychology</span><span class="c1">, 47, 419&ndash;427.</span></p><p class="c3"><span class="c11">Owen Cotton-Barratt (2021) Web of Virtue thesis. URL: </span><span class="c24"><a class="c13" href="https://forum.effectivealtruism.org/posts/yejoA6bmQSjRqEGK3/web-of-virtue-thesis-research-note">https://forum.effectivealtruism.org/posts/yejoA6bmQSjRqEGK3/web-of-virtue-thesis-research-note</a></span></p><p class="c3"><span class="c11">Parfit, D. (1984). </span><span class="c0">Reasons and persons</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Parfit, D. (2011) </span><span class="c0">On what matters: Vol. 2</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c1">Park, J. S., O&#39;Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., &amp; Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</span></p><p class="c3"><span class="c11">Passini, E., Britton, O. J., Lu, H. R., Rohrbacher, J., Hermans, A. N., Gallacher, D. J., ... &amp; Rodriguez, B. (2017). Human in silico drug trials demonstrate higher accuracy than animal models in predicting clinical pro-arrhythmic cardiotoxicity. </span><span class="c0">Frontiers in physiology</span><span class="c1">, 668.</span></p><p class="c3"><span class="c1">Pautz, A. (2014). The real trouble with phenomenal externalism: New empirical evidence</span></p><p class="c3"><span class="c11">for a brain-based theory of consciousness. In </span><span class="c0">Consciousness inside and out: Phenomenology, neuroscience, and the nature of experience </span><span class="c1">(pp. 237-298). Springer, Dordrecht.</span></p><p class="c3"><span class="c11">Pautz, A. (2020). Consciousness and coincidence: Comments on Chalmers. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, (5-6).</span></p><p class="c3"><span class="c11">Pautz, A. (2021). </span><span class="c0">Perception</span><span class="c1">. Routledge.</span></p><p class="c3"><span class="c20">Peterson, M. (2019). The value alignment problem: a geometric approach. </span><span class="c20 c17">Ethics and Information Technology</span><span class="c1">, 21, 19-28.</span></p><p class="c3"><span class="c11">Perez, E. (2022) A Test for Language Model Consciousness. URL: </span><span class="c24"><a class="c13" href="https://www.greaterwrong.com/posts/9hxH2pxffxeeXk8YT/a-test-for-language-model-consciousness">https://www.greaterwrong.com/posts/9hxH2pxffxeeXk8YT/a-test-for-language-model-consciousness</a></span></p><p class="c3"><span class="c20">Pettigrew, R. (2022) Effective altruism, risk, and human extinction. URL: </span><span class="c28 c34"><a class="c13" href="https://globalprioritiesinstitute.org/wp-content/uploads/Pettigrew-Effective-altruism-risk-and-human-extinction-2.pdf">https://globalprioritiesinstitute.org/wp-content/uploads/Pettigrew-Effective-altruism-risk-and-human-extinction-2.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Prinz, J. (2012). Singularity and inevitable doom. </span><span class="c0">Journal of Consciousness Studies</span><span class="c11">, </span><span class="c0">19</span><span class="c1">(7-8), 77-86.</span></p><p class="c3"><span class="c11">Ord, T. (2020). </span><span class="c0">The precipice: existential risk and the future of humanity</span><span class="c1">. Hachette Books.</span></p><p class="c3"><span class="c11">Olson, J. (2014). </span><span class="c0">Moral Error Theory: History, Critique, Defence</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c1">Rowland, R. (2019). Local evolutionary debunking arguments. Philosophical Perspectives, 33(1), 170-199.</span></p><p class="c3"><span class="c11">Richmond, A.M. (2017). Why doomsday arguments are better than simulation arguments. </span><span class="c0">Ratio</span><span class="c1">, 30(3), 221-238.</span></p><p class="c3"><span class="c11">Riedener, S. (2021) Existential risks from a Thomist Christian perspective. URL: </span><span class="c24"><a class="c13" href="https://globalprioritiesinstitute.org/wp-content/uploads/Stefan-Riedener_Existential-risks-from-a-Thomist-Christian-perspective.pdf">https://globalprioritiesinstitute.org/wp-content/uploads/Stefan-Riedener_Existential-risks-from-a-Thomist-Christian-perspective.pdf</a></span></p><p class="c3"><span class="c11">Saad, B. (2019). A teleological strategy for solving the meta-problem of consciousness. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 26(9-10), 205-216.</span></p><p class="c3"><span class="c11">Saad, B. (2020). Two solutions to the neural discernment problem. </span><span class="c0">Philosophical Studies</span><span class="c1">, 177(10), 2837-2850.</span></p><p class="c3"><span class="c11">Saad, B. (forthcoming</span><span class="c0">a</span><span class="c11">) Harmony in a Panpsychist World. </span><span class="c0">Synthese</span><span class="c1">.</span></p><p class="c3"><span class="c11">Saad, B. (forthcoming</span><span class="c0">b</span><span class="c11">) &ldquo;The sooner the better: an argument for bias toward the earlier&rdquo;. </span><span class="c0">Journal of the American Philosophical Association</span></p><p class="c3"><span class="c20">Saad, B. (forthcoming</span><span class="c20 c17">c</span><span class="c20">) &ldquo;Lessons from the void: what Boltzmann brains teach&rdquo; </span><span class="c20 c17">Analytic Philosophy.</span></p><p class="c3"><span class="c11">Saad, B. &amp; Bradley, A. (</span><span class="c20">2022</span><span class="c11">). The Problem of Digital Suffering. </span><span class="c0">Inquiry</span><span class="c1">.</span></p><p class="c3"><span class="c1">Sampson, E. (2022) Disaster Prevention and the Possibility of Hell: A Dilemma for Longtermist Effective Altruists. Manuscript.</span></p><p class="c3"><span class="c1">Sandbrink, J., Hobbs, H., Swett, J., Dafoe, A., &amp; Sandberg, A. (2022). Differential technology development: A responsible innovation principle for navigating technology risks. Available at SSRN.</span></p><p class="c3"><span class="c1">Sandberg, A., Armstrong, S., &amp; Cirkovic, M.M. (2017). That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi&#39;s paradox. arXiv preprint arXiv:1705.03394.</span></p><p class="c3"><span class="c1">Sandberg, A., Drexler, E., &amp; Ord, T. (2018). Dissolving the Fermi paradox. arXiv preprint arXiv:1806.02404.</span></p><p class="c3"><span class="c11">Schellenberg, J. L. (1996). Divine Hiddenness and Human Reason. </span><span class="c0 c30 c27">International Journal for</span></p><p class="c3"><span class="c0">Philosophy of Religion </span><span class="c1">40(2):121-124.</span></p><p class="c3"><span class="c11">Schellenberg, J. L. (2010). The Hiddenness Problem and the Problem of Evil. </span><span class="c0 c30 c27">Faith and Philosophy</span></p><p class="c3"><span class="c1">27(1):45-60.</span></p><p class="c3"><span class="c11">Schelling, T.C., (1971). &ldquo;Dynamic Models of Segregation,&rdquo; </span><span class="c0">Journal of Mathematical Sociology</span><span class="c1">, 1: 143&ndash;186.</span></p><p class="c3"><span class="c20">Schwartz, S. H., &amp; Boehnke, K. (2004). Evaluating the structure of human values with confirmatory factor analysis. </span><span class="c20 c17">Journal of research in personality</span><span class="c1">, 38(3), 230-255.</span></p><p class="c3"><span class="c11">Schwitzgebel, E. (2017). 1% Skepticism. </span><span class="c0">No&ucirc;s</span><span class="c1">, 51(2), 271-290.</span></p><p class="c3"><span class="c1">Schwartz, S. H., &amp; Boehnke, K. (2004). Evaluating the structure of human values with confirmatory factor analysis. Journal of research in personality, 38(3), 230-255.</span></p><p class="c3"><span class="c11">Seth, A. K., &amp; Bayne, T. (2022). Theories of consciousness. </span><span class="c0">Nature Reviews Neuroscience</span><span class="c1">, 1-14.</span></p><p class="c3"><span class="c11">Shafer-Landau, R. (2012). Evolutionary debunking, moral realism and moral knowledge. </span><span class="c0">J. Ethics &amp; Soc. Phil</span><span class="c1">., 7, i.</span></p><p class="c3"><span class="c11">Shiller, D. (2017). In Defense of Artificial Replacement. </span><span class="c0">Bioethics</span><span class="c1">, 31(5)393-399.</span></p><p class="c3"><span class="c1">Shulman, C. (2010). Whole brain emulation and the evolution of superorganisms. Mach. Intell. Res. Inst. Work. Pap. Httpintelligence OrgfilesWBESuperorgs Pdf.</span></p><p class="c3"><span class="c11">Shulman, C., &amp; Bostrom, N. (2012). How hard is artificial intelligence? Evolutionary arguments and selection effects. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">, 19(7-8), 103-130.</span></p><p class="c3"><span class="c11">Shulman, C., &amp; Bostrom, N. (2021). Sharing the World with Digital Minds. In </span><span class="c0">Rethinking Moral Status</span><span class="c1">.</span></p><p class="c3"><span class="c11">Sider, T. (2020). </span><span class="c0">The tools of metaphysics and the metaphysics of science. </span><span class="c1">OUP.</span></p><p class="c3"><span class="c11">Silva Jr, P. (forthcoming). Debunking Objective Consequentialism: The Challenge of Knowledge-Centric Anti-Luck Epistemology. In M. Klenk (ed.), </span><span class="c0">Higher Order Evidence and Moral Epistemology</span><span class="c1">. Routledge.</span></p><p class="c3"><span class="c11">Singer, P. (2005). &lsquo;Ethics and Intuitions&rsquo;. </span><span class="c0">The Journal of Ethics </span><span class="c1">9: 331&ndash;52.</span></p><p class="c3"><span class="c11">Snyder-Beattie, A. E., Sandberg, A., Drexler, K. E., &amp; Bonsall, M. B. (2021). The timing of evolutionary transitions suggests intelligent life is rare. </span><span class="c0">Astrobiology</span><span class="c1">, 21(3), 265-278.</span></p><p class="c3"><span class="c11">Sotala, K., &amp; Gloor, L. (2017). Superintelligence as a cause or cure for risks of astronomical suffering. </span><span class="c0">Informatica</span><span class="c1">, 41(4).</span></p><p class="c3"><span class="c11">Steinhart, E. (2010). Theological implications of the simulation argument. </span><span class="c0">Ars Disputandi</span><span class="c1">, 10(1), 23-37.</span></p><p class="c3"><span class="c11">Street, S. (2006). A Darwinian dilemma for realist theories of value. </span><span class="c0">Philosophical studies</span><span class="c1">, 127(1), 109-166.</span></p><p class="c3"><span class="c11">Street, S. (2010). What is constructivism in ethics and metaethics? </span><span class="c0">Philosophy Compass </span><span class="c1">5 (5):363-384.</span></p><p class="c3"><span class="c11">Street, S. (2011). Mind-Independence Without the Mystery: Why Quasi-Realists Can&rsquo;t Have it Both Ways. In Russ Shafer-Landau (ed.), </span><span class="c0">Oxford Studies in Metaethics</span><span class="c1">, Volume 6. OUP.</span></p><p class="c3"><span class="c11">Streumer, B. (2017). </span><span class="c0">Unbelievable Errors: An Error Theory About All Normative Judgments</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Stump, E. (1985). The problem of evil. </span><span class="c0">Faith and philosophy</span><span class="c1">, 2(4), 392-423.</span></p><p class="c3"><span class="c11">Summers, M., &amp; Arvan, M. (2021). Two New Doubts about Simulation Arguments. </span><span class="c0">Australasian Journal of Philosophy</span><span class="c1">, 1-13.</span></p><p class="c3"><span class="c11">Tersman, Folke (2017). Debunking and Disagreement. </span><span class="c0">No&ucirc;s </span><span class="c1">51 (4):754-774.</span></p><p class="c3"><span class="c11">Thomas, T. (2022) Simulation expectation. URL: </span><span class="c24"><a class="c13" href="https://globalprioritiesinstitute.org/wp-content/uploads/Teruji-Thomas-Simulation-Expectation-2.pdf">https://globalprioritiesinstitute.org/wp-content/uploads/Teruji-Thomas-Simulation-Expectation-2.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Titelbaum, M.G. (2013). Ten reasons to care about the Sleeping Beauty problem. </span><span class="c0">Philosophy Compass</span><span class="c1">, 8(11), 1003-1017.</span></p><p class="c3"><span class="c1">Todd, B. (2020). The emerging school of patient longtermism. 80,000 Hours. </span></p><p class="c3"><span class="c24"><a class="c13" href="https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/">https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/</a></span></p><p class="c3"><span class="c11">Tomasik, B. (2016). How the Simulation Argument Dampens Future Fanaticism. URL: </span><span class="c24"><a class="c13" href="https://longtermrisk.org/files/how-the-simulation-argument-dampens-future-fanaticism.pdf">https://longtermrisk.org/files/how-the-simulation-argument-dampens-future-fanaticism.pdf</a></span></p><p class="c3"><span class="c11">Tomasik, B. (2017). What Are Suffering Subroutines. URL: </span><span class="c24"><a class="c13" href="https://reducing-suffering.org/what-are-suffering-subroutines/">https://reducing-suffering.org/what-are-suffering-subroutines/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Tufekci, Z. (2022) Open Thread. URL: </span><span class="c24"><a class="c13" href="https://www.theinsight.org/p/open-thread-heres-hoping-we-dont">https://www.theinsight.org/p/open-thread-heres-hoping-we-dont</a></span></p><p class="c3"><span class="c11">Turchin, A. (2016) &ldquo;The Map of Shelters and Refuges from Global Risks&rdquo;. URL: </span><span class="c24"><a class="c13" href="https://www.academia.edu/50829599/The_Map_of_Shelters_and_Refuges_from_Global_Risks_Plan_B_of_X_risks_Prevention_">https://www.academia.edu/50829599/The_Map_of_Shelters_and_Refuges_from_Global_Risks_Plan_B_of_X_risks_Prevention_</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c11">Turchin, A. (2018). A Meta-Doomsday Argument: Uncertainty About the Validity of the Probabilistic Prediction of the End of the World. URL: </span><span class="c24"><a class="c13" href="https://philarchive.org/rec/TURAMA-4">https://philarchive.org/rec/TURAMA-4</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1">Turchin, A., Batin, M., Denkenberger, D., &amp; Yampolskiy, R. (2019). Simulation Typology and Termination Risks. arXiv preprint arXiv:1905.05792.</span></p><p class="c3"><span class="c11">Udell, D. B. &amp; Schwitzgebel, E. (2021). Susan Schneider&#39;s Proposed Tests for AI Consciousness: Promising but Flawed. </span><span class="c0">Journal of Consciousness Studies</span><span class="c1">&nbsp;28 (5-6):121-144.</span></p><p class="c3"><span class="c20">Urbina, F., Lentzos, F., Invernizzi, C., &amp; Ekins, S. (2022). Dual use of artificial-intelligence-powered drug discovery. </span><span class="c20 c17">Nature Machine Intelligence</span><span class="c1">, 4(3), 189-191.</span></p><p class="c3"><span class="c11">Van Fraassen, Bas C. (1989). </span><span class="c0">Laws and Symmetry</span><span class="c1">. OUP.</span></p><p class="c3"><span class="c11">Vavova, K. (2014). Debunking evolutionary debunking. </span><span class="c0">Oxford studies in metaethics</span><span class="c1">, 9(7), 6-101.</span></p><p class="c3"><span class="c11">Vavova, K. (2015). Evolutionary debunking of moral realism. </span><span class="c0">Philosophy Compass</span><span class="c11">, </span><span class="c0">10</span><span class="c1">(2), 104-116.</span></p><p class="c3"><span class="c1">Vemprala, S., Bonatti, R., Bucker, A., &amp; Kapoor, A. (2023). Chatgpt for robotics: Design principles and model abilities. Microsoft.</span></p><p class="c3"><span class="c11">Viru&eacute;s-Ortega, J., Buela-Casal, G., Garrido, E., &amp; Alc&aacute;zar, B. (2004). Neuropsychological functioning associated with high-altitude exposure. </span><span class="c0">Neuropsychology review</span><span class="c1">, 14(4), 197-224.</span></p><p class="c3"><span class="c11">Ward P, &amp; Brownlee D. (1999) </span><span class="c0">Rare earth</span><span class="c1">. In Copernicus, Springer-Verlag, New York, NY.</span></p><p class="c3"><span class="c11">Webb, S. (2015). </span><span class="c0">If the universe is teeming with aliens... where is everybody?: fifty solutions to the Fermi paradox and the problem of extraterrestrial life</span><span class="c1">. 2nd ed. &nbsp;New York, NY: Copernicus Books.</span></p><p class="c3"><span class="c11">White, R. (2006). Problems for dogmatism. </span><span class="c0">Philosophical Studies</span><span class="c1">, 131, 525&ndash;557.</span></p><p class="c3"><span class="c11">White, R. &nbsp;(2010). You just believe that because&hellip;. </span><span class="c0">Philosophical Perspectives</span><span class="c1">&nbsp;24 (1):573-615.</span></p><p class="c3"><span class="c11">White, R. (2018). Reasoning with Plenitude. In </span><span class="c0">Knowledge, Belief, and God: New Insights in Religious Epistemology</span></p><p class="c3"><span class="c11">Wright, C. (2004). Warrant for nothing. </span><span class="c0">Proceedings of the Aristotelian Society</span><span class="c1">, Supplementary Volume,</span></p><p class="c3"><span class="c1">78, 167&ndash;212.</span></p><p class="c3"><span class="c1">Xia, L., Robock, A., Scherrer, K., Harrison, C. S., Bodirsky, B. L., Weindl, I., ... &amp; Heneghan, R. (2022). Global food insecurity and famine from reduced crop, marine fishery and livestock production due to climate disruption from nuclear war soot injection. Nature Food, 3(8), 586-596.</span></p><p class="c3"><span class="c11">Yampolskiy, R. V. (2014). Utility function security in artificially intelligent agents. </span><span class="c0">Journal of Experimental &amp; Theoretical Artificial Intelligence</span><span class="c1">, 26(3), 373-389.</span></p><p class="c3"><span class="c11">Yampolskiy, R.V. (2018). Why we do not evolve software? Analysis of evolutionary algorithms. </span><span class="c0">Evolutionary Bioinformatics</span><span class="c1">, 14, 1176934318815906.</span></p><p class="c3"><span class="c1">Yudkowsky, E. (2004). Coherent extrapolated volition. Singularity Institute for Artificial Intelligence.</span></p><p class="c3"><span class="c29">Zackrisson, E., Calissendorff, P., Gonz&aacute;lez, J., Benson, A., Johansen, A., &amp; Janson, M. (2016). Terrestrial planets across space and time. </span><span class="c29 c17">The Astrophysical Journal</span><span class="c8 c7">, 833(2), 214.</span></p><p class="c3"><span class="c8 c7">Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., ... &amp; Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</span></p><p class="c3"><span>Zhang, L. What Rethink Priorities General Longtermism Team Did in 2022, and Updates in Light of the Current Situation. URL: </span><span class="c28"><a class="c13" href="https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022#Shelters_and_Other_Civilizational_Resilience_work">https://forum.effectivealtruism.org/posts/C26RHHYXzT6P6A4ht/what-rethink-priorities-general-longtermism-team-did-in-2022#Shelters_and_Other_Civilizational_Resilience_work</a></span><span class="c8 c7">&nbsp;</span></p><p class="c3"><span class="c29">Zuber, S., Venkatesh, N., T&auml;nnsj&ouml;, T., Tarsney, C., Stef&aacute;nsson, H. O., Steele, K., ... &amp; Asheim, G. B. (2021). What should we agree on about the repugnant conclusion?. </span><span class="c29 c17">Utilitas</span><span class="c29">, 33(4), 379-383.</span></p><h1 class="c31 c25" id="h.vb4ye49nvjzg"><span class="c23">F</span><span class="c8 c29 c40">unding</span></h1><p class="c3"><span>This project has been supported by the Long-Term Future Fund, the Sentience Institute, and Utrecht University. The views expressed in this report do not necessarily reflect those of funders.</span></p><div><p class="c3 c32"><span class="c8 c7"></span></p></div><hr class="c38"><div><p class="c3"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c4">&nbsp;For discussion of various sorts of global catastrophic risks, see the essays in Bostrom &amp; &#262;irkovi&#263; (2008).</span></p></div><div><p class="c3"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c4">&nbsp;See Baumann (2022), Gloor (2016; 2018), and Daniel (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c6">&nbsp;</span><span class="c12">S</span><span class="c6">ee Ord (2020</span><span class="c12">).</span></p></div><div><p class="c3"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c4">&nbsp;Research is already underway on small-scale (25 agents) social simulations that embed instances of OpenAI&rsquo;s ChatGPT in virtual social contexts (Park, et al., 2023).</span></p></div><div><p class="c3"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c4">&nbsp;See Davidson (2023).</span></p></div><div><p class="c3"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c12">&nbsp; See Huang (2023), &nbsp;</span><span class="c28 c12"><a class="c13" href="https://en.wikipedia.org/wiki/Auto-GPT">wikipedia.org/wiki/Auto-GPT</a></span><span class="c4">, Li et al. (2022), and Vemprala et al. (2023)</span></p></div><div><p class="c3"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c6">&nbsp;See Chalmers (1996: Chs. 7, 9)</span><span class="c4">&nbsp;and &nbsp;Sandberg &amp; Bostrom (2008).</span></p></div><div><p class="c3"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c4">&nbsp;These sum to more than 100% because the first two possibilities are not exclusive: we could on large scales both unwittingly run some simulations containing conscious minds and intentionally run others. Or multiple agents running the same simulations could hold different views about whether those simulations contain conscious minds.</span></p></div><div><p class="c3"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c4">&nbsp;The situation here parallels the situation with AI-involving catastrophic risks more generally. &nbsp;While the sources of risk do not depend on the specifics of concrete scenarios we can conjure, it is still advisable to describe concrete scenarios since the uninitiated may find abstract risks difficult to take seriously without first seeing how they could be concretely realized. &nbsp;And it is advisable to emphasize that the general catastrophic risks do not depend on the details of those scenarios, lest the target audience take challenges to details of those scenarios to be levers for driving down estimates for the general risks they illustrate. &nbsp;See Christiano (2019), Cotra (2022), Critch (2021), Hendrycks (2023), Lawsen (2023), and Mowshowitz (2023).</span></p></div><div><p class="c3"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c12">&nbsp;For other presentations and discussion of scenarios, see Bostrom (2003</span><span class="c12 c17">a</span><span class="c12">), Chalmers (2010</span><span class="c12 c17">b</span><span class="c4">; forthcoming), Dainton (2012), and Hanson (2016). &nbsp;</span></p></div><div><p class="c3"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c4">&nbsp;See &nbsp;Karnofsky (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c4">&nbsp;For a book-length discussion of a future with whole brain emulations, see Hanson (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c4">&nbsp;Simulations of nuclear war have already been used to research catastrophic risks. For example, Xia et al. (2022) used simulations to evaluate the impact of nuclear war on global food supply and to arrive at the estimate that five billion people would die in a large-scale nuclear war between the United States and Russia. For a news cycle, the article was widely discussed in popular media. This points to another way in which research simulation could reduce catastrophic risks: by providing easily understood statistics about concrete scenarios, simulation research may elicit stronger responses from the public and policymakers than, say, compelling arguments that deal with a given type of risk in the abstract.</span></p></div><div><p class="c3"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c4">&nbsp;See Bostrom (2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c12">&nbsp;For theoretical discussions, see Chalmers (2010</span><span class="c12 c17">b</span><span class="c4">: &sect;7), Bostrom (2014: 116-119), and Babcock et al. (2019). AI safety testing in virtual environments is already being explored in practice. For example, OpenAI has developed AI Safety Gridworlds, a suite of virtual environments for establishing safety in AI systems (Leike, 2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c4">&nbsp;See, e.g., Bostrom (2014) and Muehlhauser (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c4">&nbsp;Cf. Armstrong et al. (2012).</span></p></div><div><p class="c3"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c4">&nbsp;See Bentham (1791) and Bostrom (2014: 134-5).</span></p></div><div><p class="c3"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c4">&nbsp;See Olds &amp; Milner, P. (1954) and Yampolskiy (2014).</span></p></div><div><p class="c3"><a href="#ftnt_ref20" id="ftnt20">[20]</a><span class="c4">&nbsp;Cf. Bostrom (2014: 134-5).</span></p></div><div><p class="c3"><a href="#ftnt_ref21" id="ftnt21">[21]</a><span class="c4">&nbsp;Because official public messaging during disasters is low-bandwidth, it often involves a tradeoff between direct and indirect effects. This was evident during the Covid-19 pandemic when public health institutions issued unwarranted statements that could be charitably interpreted as aiming at indirect effects&mdash;cf. Tufekci (2022) &nbsp;Substantiating a heuristic concerning direct vs. indirect effects could help protect against basing public messaging during catastrophes on overestimations of indirect effects of the messaging.</span></p></div><div><p class="c3"><a href="#ftnt_ref22" id="ftnt22">[22]</a><span class="c4">&nbsp;See Bostrom (2014: Ch. 14) and Sandbrink et al (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref23" id="ftnt23">[23]</a><span class="c4">&nbsp;Cf. Anthis (2022), Bostrom (2014: Chs. 12-3), Dello-Iacovo (2017), Doody (2022), Hayward (2020), and MacAskill (2022: Chs. 3-4). A well-known toy simulation of this sort can be found in Schelling (1971). In it, agents manifested different preferences for segregation into groups with members of the same kind through movement rules. Surprisingly, he found that slight preferences for segregation induced segregation from a non-segregated state.</span></p></div><div><p class="c3"><a href="#ftnt_ref24" id="ftnt24">[24]</a><span class="c4">&nbsp;For a simulation study of the dynamics of moral disagreement, see Gustafsson &amp; Peterson (2012). For a simulation study of honor culture, see Nowak et al. (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref25" id="ftnt25">[25]</a><span class="c4">&nbsp;See Schwartz &amp; Boehnke (2004). Relatedly, simulation testing of different civilizational balances between exploration and exploitation (e.g. in institutional design) seems promising.</span></p></div><div><p class="c3"><a href="#ftnt_ref26" id="ftnt26">[26]</a><span class="c4">&nbsp;See Todd (2020) for discussion and references.</span></p></div><div><p class="c3"><a href="#ftnt_ref27" id="ftnt27">[27]</a><span class="c4">&nbsp;See Koehler et al. (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref28" id="ftnt28">[28]</a><span class="c4">&nbsp;See Anthis (2018) and Owen Cotton-Barratt (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref29" id="ftnt29">[29]</a><span class="c4">&nbsp;For a meta-analysis of empirical work on the effectiveness of simulation-based learning, see Chernikova et al. (2020). &nbsp;</span></p></div><div><p class="c3"><a href="#ftnt_ref30" id="ftnt30">[30]</a><span class="c6">&nbsp;For discussion of artificial evolution as a method for developing AI, see Bostrom (2014: 24-8, 37-44, &nbsp;Chalmers (2010</span><span class="c6 c17">b</span><span class="c4">: 16-17), Shulman &amp; Bostrom (2012), and Shulman (2010). See Yampolskiy (2018) for pessimism about artificially evolving software. For cautionary observations about using artificial selection, see Bostrom (2014: 153-5).</span></p></div><div><p class="c3"><a href="#ftnt_ref31" id="ftnt31">[31]</a><span class="c4">&nbsp;For presentations of Fermi&rsquo;s paradox and book-length discussions of candidate solutions, see &#262;irkovi&#263; (2018) and Webb (2015). &nbsp;For reasons to think that the paradox arises because of mishandling of uncertainties in calculations that are used to pose it, see Sandberg et al. (2018). &nbsp;For simulations of a &ldquo;grabby alien&rsquo;s&rdquo; solution to Fermi&rsquo;s Paradox, see Hanson et al. (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref32" id="ftnt32">[32]</a><span class="c6">&nbsp; See Bostrom (2002</span><span class="c6 c17">a</span><span class="c4">), Grace (2010), and Hanson (1998).</span></p></div><div><p class="c3"><a href="#ftnt_ref33" id="ftnt33">[33]</a><span class="c4">&nbsp;See, e.g., Miller &amp; Felton (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref34" id="ftnt34">[34]</a><span class="c6">&nbsp;For a collection of resources on longtermism, see </span><span class="c28 c6"><a class="c13" href="https://longtermism.com/resources">longtermism.com/resources</a></span><span class="c4">.</span></p></div><div><p class="c3"><a href="#ftnt_ref35" id="ftnt35">[35]</a><span class="c4">&nbsp;See Bostrom (2008).</span></p></div><div><p class="c3"><a href="#ftnt_ref36" id="ftnt36">[36]</a><span class="c4">&nbsp;See Jumper et al. (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref37" id="ftnt37">[37]</a><span class="c4">&nbsp;For an overview of different sorts of debunking arguments, see Korman (2019). For discussion of moral debunking arguments, see, e.g., Joyce (2007), Shafer-Landau (2012), Street (2006), and Vavova (2015). &nbsp;For discussion of religious debunking arguments, see Mason (2010), references therein, and White (2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref38" id="ftnt38">[38]</a><span class="c12">For example, perhaps evolutionary considerations could debunk some intuitions about the non-physicality of consciousness&mdash;cf.</span><span class="c6">&nbsp;Chalmers (2018</span><span class="c6 c17">a</span><span class="c6">; 2020).</span><span class="c12">&nbsp; If so, this could be relevant to whether simulation inhabitants have moral status, since it is more plausible that consciousness has special moral significance if it is a basic non-physical property than if it is a physical property.</span></p></div><div><p class="c3"><a href="#ftnt_ref39" id="ftnt39">[39]</a><span class="c6">&nbsp;In the case of normative beliefs, there is an analogy between evolutionary forces being orthogonal to true moral beliefs (even if </span><span class="c12">those forces promote </span><span class="c4">instrumental rationality) and the orthogonality thesis in AI that levels of intelligence and final goals can generally be arbitrarily combined (even if intelligence engenders instrumental rationality)&mdash;for discussion of the latter thesis, see Bostrom (2014: Ch. 7), H&auml;ggstr&ouml;m (2021), and M&uuml;ller &amp; Cannon (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref40" id="ftnt40">[40]</a><span class="c4">&nbsp;See Parfit (2011: 494-6); cf. M&uuml;ller &amp; Cannon (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref41" id="ftnt41">[41]</a><span class="c6">&nbsp;A field of study that is relevant here is artificial life, which is partly concerned with analyzing life-like agents through simulations of evolutionary processes. See </span><span class="c28 c6"><a class="c13" href="https://en.wikipedia.org/wiki/Artificial_life">https://en.wikipedia.org/wiki/Artificial_life</a></span><span class="c4">&nbsp;for an overview.</span></p></div><div><p class="c3"><a href="#ftnt_ref42" id="ftnt42">[42]</a><span class="c4">&nbsp;For discussion of evolutionary debunking arguments against normative rather than metaethical theories, see Greene (2007), de Lazari-Radek &amp; Singer (2012), Rowlands (2019), and Silva (forthcoming), and Singer (2005).</span></p></div><div><p class="c3"><a href="#ftnt_ref43" id="ftnt43">[43]</a><span class="c4">&nbsp;Cf Cuneo (2007).</span></p></div><div><p class="c3"><a href="#ftnt_ref44" id="ftnt44">[44]</a><span class="c4">&nbsp;See Joyce (2001), Mackie (1977), Olson (2014), and Streumer (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref45" id="ftnt45">[45]</a><span class="c4">&nbsp;See, e.g., Street (2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref46" id="ftnt46">[46]</a><span class="c4">&nbsp;Cf. Dreier (2012) and Street (2011).</span></p></div><div><p class="c3"><a href="#ftnt_ref47" id="ftnt47">[47]</a><span class="c4">&nbsp;See Cuneo (2007), Dogramaci (2017), Vavova (2014; 2015), Shafer-Landau (2012), and White (2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref48" id="ftnt48">[48]</a><span class="c6">&nbsp;There is a burgeoning technical subfield of AI safety devoted to the alignment problem. Much of the research on this topic can be found at </span><span class="c28 c6"><a class="c13" href="https://www.alignmentforum.org/">https://www.alignmentforum.org/</a></span><span class="c4">.</span></p></div><div><p class="c3"><a href="#ftnt_ref49" id="ftnt49">[49]</a><span class="c6">&nbsp;See Bostrom (2003</span><span class="c6 c17">b</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref50" id="ftnt50">[50]</a><span class="c4">&nbsp;Cf. Bostrom (2014: 2019-20).</span></p></div><div><p class="c3"><a href="#ftnt_ref51" id="ftnt51">[51]</a><span class="c12">&nbsp;</span><span class="c6">Unless, of course, the human preferences in question are idealized via alignment with the objective moral facts&mdash;in that case, there would not be room for moral catastrophe to result from aligning powerful AI with those human preferences but not with the moral facts. This contrasts with idealizations that modify preferences by imposing non-moral constraints such as </span><span class="c12">c</span><span class="c6">oherence among preferences, the elimination of lower-order preferences that conflict with higher-order preferences, reflective, empirically informed endorsement of preferences&mdash;cf. Yudkowsky, E. (2004). &nbsp;On moral realism, there is no guarantee that aligning AI with human preferences that result from the latter sorts of idealizations would align the AI with the moral facts&mdash;cf. Bostrom (2014: 2018),</span><span class="c12">&nbsp;Erez (2023), Gabriel (2020), Peterson (2019), and Shafer-Landau (2003: 42)</span><span class="c4">. &nbsp;The risk level for a catastrophe from this kind of alignment failure depends partly on the plausibility of moral realism. &nbsp;Some relevant data: in a recent survey of professional philosophers, 62.1% favored moral realism while only &nbsp;26.1% favored moral antirealism (Bourget &amp; Chalmers, 2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref52" id="ftnt52">[52]</a><span class="c4">&nbsp;See Dretske (1971), Murphy &amp; Black (2012), Nozick (1981), and Ichikawa (2011).</span></p></div><div><p class="c3"><a href="#ftnt_ref53" id="ftnt53">[53]</a><span class="c6">&nbsp;Reflection on skeptical scenarios suggests that insensitivity on its own is not enough to make a belief epistemically defective: I would believe I&rsquo;m not a brain in a vat with exactly </span><span class="c6 c17">this </span><span class="c6">experience even if I were; yet my belief that I am not a brain in a vat is not epistemically defective. On the other hand, reflection on non-skeptical cases suggests that insensitivity can result in epistemic defect: for example, if evolution explains widespread robust belief in the moral superiority of </span><span class="c6 c17">homo sapiens</span><span class="c4">&nbsp;over other species and that belief would have been widespread even if it were false, that would raise a serious challenge to the belief&mdash;see Jaquet (2022). This is so even if the belief is held in all nearby scenarios in which evolution went differently.</span></p></div><div><p class="c3"><a href="#ftnt_ref54" id="ftnt54">[54]</a><span class="c4">&nbsp;See Barnett &amp; Li (2016) Bedke (2009), Bhogal (forthcoming), Bogardus (2016), Clarke-Doane (2020), Enoch (2011), and Tersman (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref55" id="ftnt55">[55]</a><span class="c6">&nbsp;Theories of consciousness tend to fall into one of three areas: the metaphysics of mind, the philosophy of perception, or the science of consciousness. For an overview of theories in the metaphysics of mind, see Chalmers (2010</span><span class="c6 c17">a</span><span class="c4">: Ch. 5). &nbsp;For an introduction to theories in the philosophy of perception, see Pautz (2021). &nbsp;For an overview of theories in the science of consciousness, see Seth &amp; Bayne (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref56" id="ftnt56">[56]</a><span class="c4">&nbsp;For an overview of a range of books over the last few decades that address consciousness in artificial systems, see Ladak (2022). For an overview of key issues and open questions concerning artificial consciousness, see Long (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref57" id="ftnt57">[57]</a><span class="c6">&nbsp;For discussion of tests for consciousness, see Saad &amp; Bradley (</span><span class="c12">2022</span><span class="c6">), Chalmers (2018</span><span class="c6 c17">a</span><span class="c4">: 34-5), Elamrani &amp; Yampolskiy (2019), Muehlhauser (2017), Perez (2022) Schneider (2019: Ch. 4), and Udell &amp; Schwitzgebel (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref58" id="ftnt58">[58]</a><span class="c4">&nbsp;The same goes for simulation technologies more generally. &nbsp;However, I&rsquo;ll just focus on dual-use risks posed by research simulation technologies. &nbsp;I do this for tractability and because, among simulation technologies, research simulation seems like an especially large source of dual use risk.</span></p></div><div><p class="c3"><a href="#ftnt_ref59" id="ftnt59">[59]</a><span class="c4">&nbsp;See Urbina et al. (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref60" id="ftnt60">[60]</a><span class="c4">&nbsp;See Ord (2020: Ch. 5) and MacAskill (2022: Ch. 5).</span></p></div><div><p class="c3"><a href="#ftnt_ref61" id="ftnt61">[61]</a><span class="c4">&nbsp;See Barak and Edelman (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref62" id="ftnt62">[62]</a><span class="c6">&nbsp;See </span><span class="c28 c6"><a class="c13" href="https://en.wikipedia.org/wiki/Unethical_human_experimentation">https://en.wikipedia.org/wiki/Unethical_human_experimentation</a></span><span class="c4">. </span></p></div><div><p class="c3"><a href="#ftnt_ref63" id="ftnt63">[63]</a><span class="c6">&nbsp;</span><span class="c12">T</span><span class="c6">here are already cases in which computer simulations </span><span class="c12">outperform </span><span class="c6">animal experiments. For instance, see Passini (2017) for a case in which (Virtual Assay) human-based computer simulations outperformed animal experimentation on predicting drug-induced cardiotoxicity in humans. &nbsp;For a review of existing </span><span class="c6 c17">in silico</span><span class="c4">&nbsp;alternatives to animal testing, see Madden et al. (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref64" id="ftnt64">[64]</a><span class="c4">&nbsp;Some such requirements were relaxed in 2022 by the FDA Modernization Act 2.0, which lifts the United State&rsquo;s federal mandate on testing experimental drugs in animals before testing them in humans.</span></p></div><div><p class="c3"><a href="#ftnt_ref65" id="ftnt65">[65]</a><span class="c6">&nbsp;See </span><span class="c28 c6"><a class="c13" href="https://www.1daysooner.org/past">https://www.1daysooner.org/past</a></span><span class="c4">. </span></p></div><div><p class="c3"><a href="#ftnt_ref66" id="ftnt66">[66]</a><span class="c12">&nbsp;For an interactive, simulation-based model of AGI arrival timelines, see </span><span class="c28 c12"><a class="c13" href="https://takeoffspeeds.com/playground.html">https://takeoffspeeds.com/playground.html</a></span><span class="c12">.</span></p></div><div><p class="c3"><a href="#ftnt_ref67" id="ftnt67">[67]</a><span class="c4">&nbsp;A notable potential risk of gamified simulations (and virtual reality simulations more generally) is that they could become so encompassing as to disempower humanity: if all of humanity, or even just key actors, became more interested in pursuing goals within gamified simulations than in pursuing goals, our civilization would lose much of its ability to respond to risks. </span></p></div><div><p class="c3"><a href="#ftnt_ref68" id="ftnt68">[68]</a><span class="c6">&nbsp;See Baum et al. (2015). &nbsp;For a chart of different types of refuge, including digital shelters, see Turchin (2016). Rethink Priorities is one organization that has recently explored </span><span class="c12">refuges as a risk-reduction strategy (Zhang, 2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref69" id="ftnt69">[69]</a><span class="c4">&nbsp;See Bostrom &amp; Shulman (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref70" id="ftnt70">[70]</a><span class="c4">&nbsp;For potential advantages of aestivation, see Sandberg et al. (2017). For criticism of their analysis, see Bennett et al. (2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref71" id="ftnt71">[71]</a><span class="c4">&nbsp;See, e.g., &nbsp;Chalmers (2003; 2022) and Dainton (2012).</span></p></div><div><p class="c3"><a href="#ftnt_ref72" id="ftnt72">[72]</a><span class="c4">&nbsp;Cf. Shulman &amp; Bostrom (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref73" id="ftnt73">[73]</a><span class="c4">&nbsp;See, e.g., Chalmers (1996) and Saad &amp; Bradley (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref74" id="ftnt74">[74]</a><span class="c4">&nbsp; For discussion in the context of superintelligence and its bearing on the risk of astronomical quantities of suffering, see Sotala &amp; Gloor (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref75" id="ftnt75">[75]</a><span class="c4">&nbsp;See Hill &amp; Tolk (2017) for a history of military simulations.</span></p></div><div><p class="c3"><a href="#ftnt_ref76" id="ftnt76">[76]</a><span class="c4">&nbsp;See Perez (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref77" id="ftnt77">[77]</a><span class="c4">&nbsp;See Bostrom (2014: Ch. 11) and Hanson (2016).</span></p></div><div><p class="c41"><a href="#ftnt_ref78" id="ftnt78">[78]</a><span class="c4">&nbsp;See Dainton (2012) and his discussion of Banks (2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref79" id="ftnt79">[79]</a><span class="c4">&nbsp;Compare: OpenAI, a leading company among those trying to develop AGI, inadvertently trained a large language model (GPT-2) to optimize for expressing negative sentiment as a result of a flipped sign and AI developers (literally) being asleep during the training process (Ziegler et al., 2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref80" id="ftnt80">[80]</a><span class="c6">&nbsp;See, e.g., Tomasik (2017). </span></p></div><div><p class="c3"><a href="#ftnt_ref81" id="ftnt81">[81]</a><span class="c6">&nbsp;For instance, Mao Zedong&rsquo;s policies led to population growth followed (</span><span class="c12">perhaps in ways that were foreseeable but not intended) </span><span class="c4">by famines in which tens of millions of Chinese citizens died (Fitzpatrick, 2009).</span></p></div><div><p class="c3"><a href="#ftnt_ref82" id="ftnt82">[82]</a><span class="c4">On the other hand, creating suitable ensembles of simulated agents with malevolent impulses that are punished when they act on those impulses could incentivize even powerful would-be malevolent actors to behave morally by giving them reason to think they are in such a simulation&mdash;see Elga (2004) and the discussion of the simulation argument below. For discussion of existential and suffering risks posed by malevolent actors, see Althaus &amp; Baumann (2020). &nbsp;</span></p></div><div><p class="c3"><a href="#ftnt_ref83" id="ftnt83">[83]</a><span class="c6">&nbsp;The argument was introduced and defended by Bostrom (2003</span><span class="c6 c17">a</span><span class="c4">). &nbsp;Others who take it seriously include Braddon-Mitchell &amp; Latham (2022), Chalmers (2022), &#262;irkovi&#263; (2015), Crummet (2020), Dainton (2012; 2020), Greene (2020), Hanson (2001), Johnson (2011), Lewis (2013), Monton (2009: Ch. 3). Schwitzgebel (2017), Steinhart (2010), Thomas (2022), and Turchin et al. (2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref84" id="ftnt84">[84]</a><span class="c4">&nbsp;There is debate in the literature about whether this hypothesis should, given its role in the argument, be made conditional on the reasoner&rsquo;s not being a simulation&mdash;see Thomas (2022); cf. Bostrom (2011) and &nbsp;Crawford (2013). For tractability and simplicity, I set this issue aside and work with the unconditional formulation.</span></p></div><div><p class="c3"><a href="#ftnt_ref85" id="ftnt85">[85]</a><span class="c6">&nbsp;More precisely, the bland indifference principle says that for a given hypothesis about how the world is qualitatively, we should divide our credence concerning our self-location on that hypothesis evenly among the observers like us that exist on that hypothesis. This should not be confused with indifference principles that require one to divide one&rsquo;s credence evenly between self-locations posited on different qualitative hypotheses or between different qualitative hypotheses when </span><span class="c12">one&rsquo;s evidence does not adjudicate between them</span><span class="c6">. &nbsp;These stronger principles are open to serious objections that do not apply to the bland indifference principle (hence the label &lsquo;bland&rsquo;)&mdash;see Elga (2004: 387-8) and Van Fraassen (1989). &nbsp;For discussion and defense of restricting rather than rejecting </span><span class="c12">indifference principles, see Greaves (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref86" id="ftnt86">[86]</a><span class="c4">&nbsp;It may turn out that being an observer like us is a matter of degree, either because being an observer is a matter of degree (an outcome that is hard to avoid on a reductive physicalist view of observerhood&mdash;see Lee (2019)) or because counting as like us is a matter of degree. In either case, the most plausible rendering of the indifference principle may then require one to divide one&rsquo;s self-locating credence among observers that are to some degree like oneself in proportion to the degree to which they are like oneself&mdash;see Dorr &amp; Arntzeneius (2017). &nbsp;While I think such a proportional rather than egalitarian indifference principle may well turn out to be correct, I do not think the difference between them is important for the discussion that follows. In any event, for simplicity, I will work with the just sketched egalitarian principle.</span></p></div><div><p class="c3"><a href="#ftnt_ref87" id="ftnt87">[87]</a><span class="c6">&nbsp;See Bostrom (2003</span><span class="c6 c17">a</span><span class="c4">) and Chalmers (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref88" id="ftnt88">[88]</a><span class="c4">&nbsp;See Chalmers (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref89" id="ftnt89">[89]</a><span class="c6">&nbsp;See Bostrom (2003</span><span class="c6 c17">a</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref90" id="ftnt90">[90]</a><span class="c4">&nbsp;See Dogramaci (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref91" id="ftnt91">[91]</a><span class="c4">&nbsp;See Summers &amp; Arvan &nbsp;(2021), Dainton (2012), and Chalmers (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref92" id="ftnt92">[92]</a><span class="c4">&nbsp;See Chalmers (2022), Richmond (2017), and Hanson (2001).</span></p></div><div><p class="c3"><a href="#ftnt_ref93" id="ftnt93">[93]</a><span class="c4">&nbsp;See Birch (2013), Crawford (2013), and Thomas (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref94" id="ftnt94">[94]</a><span class="c6">&nbsp;See Crawford (2013). &nbsp;For discussion, see Chalmers (2022: online appendix) and Dainton (2012). &nbsp;For a response to this sort of objection, see Bostrom (2009). &nbsp;For related discussions of cognitive instability in the context of Boltzmann </span><span class="c12">b</span><span class="c6">rains, see Carroll (2020), Chalmers (2018</span><span class="c6 c17">b</span><span class="c6">), Dogramaci (2020), Kotzen (2020), Saad (forthcoming</span><span class="c12 c17">c</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref95" id="ftnt95">[95]</a><span class="c6">&nbsp;See Bostrom (2003</span><span class="c6 c17">a</span><span class="c4">) and Chalmers (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref96" id="ftnt96">[96]</a><span class="c4">&nbsp;See Chalmers (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref97" id="ftnt97">[97]</a><span class="c4">&nbsp;See, e.g., &#262;irkovi&#263; (2008: 138).</span></p></div><div><p class="c3"><a href="#ftnt_ref98" id="ftnt98">[98]</a><span class="c4">&nbsp;See Dainton (2012); cf. Chalmers (2022: online appendix).</span></p></div><div><p class="c3"><a href="#ftnt_ref99" id="ftnt99">[99]</a><span class="c4">&nbsp;For a summary of literature on neuropsychological responses to hypoxia, see Viru&eacute;s-Ortega (2004).</span></p></div><div><p class="c3"><a href="#ftnt_ref100" id="ftnt100">[100]</a><span class="c4">&nbsp;For relevant discussion, see Elga (2008), Kotzen (2020), and Christensen (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref101" id="ftnt101">[101]</a><span class="c4">&nbsp;See Bostrom (2005). &nbsp;However, there are exceptions, notably skeptical arguments from dreams, the evolutionary origins of our beliefs, and physical theories that proliferate Boltzmann Brains.</span></p></div><div><p class="c3"><a href="#ftnt_ref102" id="ftnt102">[102]</a><span class="c6">&nbsp;See Chalmers (2003; </span><span class="c12">2022</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref103" id="ftnt103">[103]</a><span class="c4">&nbsp;See, e.g., Chalmers (2006), Cutter (2021), Hoffman (2019), and Pautz (2014).</span></p></div><div><p class="c3"><a href="#ftnt_ref104" id="ftnt104">[104]</a><span class="c6">&nbsp;See Bostrom (2002</span><span class="c6 c17">a</span><span class="c4">), &#262;irkovi&#263; (2008), Greene (2020), and Turchin et al. (2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref105" id="ftnt105">[105]</a><span class="c4">&nbsp;See Tomasik (2016); cf. Ord (2020: Appendix B).</span></p></div><div><p class="c3"><a href="#ftnt_ref106" id="ftnt106">[106]</a><span class="c4">&nbsp;See, e.g., &#262;irkovi&#263; (2008).</span></p></div><div><p class="c3"><a href="#ftnt_ref107" id="ftnt107">[107]</a><span class="c4">&nbsp;See Hanson (2001) and Greene (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref108" id="ftnt108">[108]</a><span class="c4">&nbsp;For reasons to think simulation awareness is a risk, see Greene (2020). For reasons to think it is not, see Alexander (2019) and Braddon-Mitchell &amp; Latham (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref109" id="ftnt109">[109]</a><span class="c4">&nbsp;The most thorough discussions of this that I am aware of are Braddon-Mitchell &amp; Latham (2022), Greene (2020), and Turchin et al. (2019).</span></p></div><div><p class="c3"><a href="#ftnt_ref110" id="ftnt110">[110]</a><span class="c4">&nbsp;See Lenman (2002).</span></p></div><div><p class="c3"><a href="#ftnt_ref111" id="ftnt111">[111]</a><span class="c6">&nbsp;See Lenman (2002) and Prinz (2012). &nbsp;</span><span class="c12">Those who countenance agent-relative moral reasons may see middle ground here&mdash;see, e.g., Mogensen (2019</span><span class="c12 c17">b</span><span class="c12">) and references therein.</span></p></div><div><p class="c3"><a href="#ftnt_ref112" id="ftnt112">[112]</a><span class="c4">&nbsp;For moral concerns about directed panspermia, see Dello-Iacovo (2017) and O&rsquo;Brien (forthcoming).</span></p></div><div><p class="c3"><a href="#ftnt_ref113" id="ftnt113">[113]</a><span class="c6">&nbsp;For reasons to doubt that our universe&rsquo;s past or future are net positive, see Anthis (2018; 2022), Benatar (2008), Gloor (2016; 2018), and MacAskill (2022: Ch. 9). </span><span class="c12">For motivation for risk-averse decision theory and arguments that it recommends extinction-hastening interventions over extinction-preventing ones (given longtermist moral assumptions), see Pettigrew (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref114" id="ftnt114">[114]</a><span class="c6">&nbsp;One way to motivate asymmetrically diminishing returns is to note that (1) some bads, such as uncompensated suffering, seem not to diminish at all in disvalue as their quantity increases and (2) countenancing diminishing returns for goods provides an escape from what&rsquo;s known in population ethics as the Repugnant Conclusion, which is the (supposedly) implausible claim that &ldquo;For any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living&rdquo; (Parfit, 1984: 342). For objections to one form of asymmetrical diminishing returns, see </span><span class="c6 c17">ibid</span><span class="c4">&nbsp;(&sect;134). For reasons to doubt that avoiding the Repugnant Conclusion is a requirement for an adequate approach to population ethics, see Zuber et al. (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref115" id="ftnt115">[115]</a><span class="c4">&nbsp;Cf. Ord (2020: Ch. 2).</span></p></div><div><p class="c3"><a href="#ftnt_ref116" id="ftnt116">[116]</a><span class="c12">&nbsp;The Unilateralist Curse provides a further reason for caution here: when large numbers of at least somewhat error-prone, altruistically motivated agents are each in a position to unilaterally act in ways that affect others, we should expect unilateral interventions to happen more often than is optimal (Bostrom et al., 2016). &nbsp;In the case at hand, the curse we face is: if a sufficiently large number of error-prone, altruistic agents will be in a position to unilaterally take extinction-inducing interventions and their evidence renders extinction non-optimal in expectation, we should expect some such agent to mishandle their evidence, overestimate the value of such interventions, take them, and thereby cause extinction. &nbsp;For general suggestions for avoiding the Curse that can be applied in this case, see </span><span class="c12 c17">ibid</span><span class="c4">.</span></p></div><div><p class="c3"><a href="#ftnt_ref117" id="ftnt117">[117]</a><span class="c6">&nbsp;Religious discussions of catastrophic risks that I am familiar with tend not to discuss non-religious catastrophic risks. </span><span class="c12">There are a few notable exceptions.</span><span class="c6">&nbsp;Riedener (2021)</span><span class="c12">&nbsp;</span><span class="c6">argues that existential risk reduction is extremely important from a Thomist Christian perspective. &nbsp;Danaher (2015) brings some ideas from philosophy of religion to bear on catastrophic risk posed by AI: he explores an analogy between the &ldquo;skeptical theist&rdquo; view that appearances of evil are not strong evidence against the existence of a benevolent God with Bostrom&rsquo;s (2014: Ch. 8) &ldquo;treacherous turn&rdquo; concern that AI systems may behave cooperatively (e.g. while boxed within a simulated environment) and hence appear benign before abruptly changing their behavior (in a potentially catastrophic manner) to pursue their final goals. &nbsp;</span><span class="c12">The (pseudonymous) author of this </span><span class="c12 c28"><a class="c13" href="https://coldbuttonissues.substack.com/p/the-base-rate-of-longtermism-is-bad?utm_source%3Dprofile%26utm_medium%3Dreader2">post</a></span><span class="c12">&nbsp;argues that longtermist movements have a dismal track-record, that religions have been (for good or ill) influential long-termist movements, and that catastrophic risks that concern longtermists may be more effectively mitigated through near-term focused approach.</span></p></div><div><p class="c3"><a href="#ftnt_ref118" id="ftnt118">[118]</a><span class="c4">&nbsp;For an argument that religious catastrophic risks are important, tractable, and neglected relative to other catastrophic risks, see Sampson (2022).</span></p></div><div><p class="c3"><a href="#ftnt_ref119" id="ftnt119">[119]</a><span class="c4">&nbsp;For discussion of the religious implications of the simulation argument or hypothesis, see Bostrom (2003: 254), Chalmers (2022: Ch. 7), Crummett (2020), Dainton (2020), Johnson (2011), and Steinhart (2010). &nbsp;</span></p></div><div><p class="c3"><a href="#ftnt_ref120" id="ftnt120">[120]</a><span class="c4">&nbsp;This support may be somewhat attenuated by the fact that religions that posit catastrophes tend not to countenance a flawed or indifferent simulator of our universe; hence, the supposition that our universe has such a creator tells against those specific catastrophes and so in that respect detracts from overall risk.</span></p></div><div><p class="c3"><a href="#ftnt_ref121" id="ftnt121">[121]</a><span class="c4">&nbsp;For multiverse solutions to theistic problems that do not invoke simulations, see Kraay (2010), Leslie (1989), &nbsp;and Megill (2011). For an overview of surrounding literature, see Kraay (2014: 9-11).</span></p></div><div><p class="c3"><a href="#ftnt_ref122" id="ftnt122">[122]</a><span class="c4">&nbsp;The problem of evil is variously formulated&mdash;see, e.g., Benton et al. (2016). &nbsp;The same goes for the other problems considered below.</span></p></div><div><p class="c3"><a href="#ftnt_ref123" id="ftnt123">[123]</a><span class="c4">&nbsp;See Everitt (2004: Ch. 11).</span></p></div><div><p class="c3"><a href="#ftnt_ref124" id="ftnt124">[124]</a><span class="c4">&nbsp;See, e.g., the essays in Green &amp; Stump (2015) along with Schellenberg (1996; 2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref125" id="ftnt125">[125]</a><span class="c4">&nbsp;See, e.g., Adams (2000), Crummett (2017), and Stump (1985).</span></p></div><div><p class="c3"><a href="#ftnt_ref126" id="ftnt126">[126]</a><span class="c6">&nbsp;For an overview, see Friederich (2018). &nbsp;There are also less explored non-cosmological varieties of fine-tuning such as the match between the universe&rsquo;s boundary conditions with its laws (Cutter &amp; Saad, </span><span class="c12">forthcoming</span><span class="c6">) and the fine-tuning of experiences&rsquo; causal profiles with their rational profiles (Chalmers, 2020; Goff, 2018, Saad (2019; 2020; forthcoming</span><span class="c6 c17">a</span><span class="c4">), James (1890), M&oslash;rch (2018), Pautz (2010; 2020). &nbsp;Cutter &amp; Crummett (forthcoming) defend an argument for theism that appeals to psychophysical fine-tuning. &nbsp;While both cosmological and psychophysical fine-tuning can be used to argue for theism, they interact with the simulation hypothesis in different ways. &nbsp;For instance, whereas an ensemble of simulation universes might explain cosmological fine-tuning, it is not clear that they could explain psychophysical fine-tuning&mdash;e.g., if the psychophysical laws have a functionalist character at the base level, then they will not be manipulable independently of other factors in the simulations. &nbsp;Similarly, if they involve a non-functional feature that cannot be freely varied in the simulation, the simulators will not be able to vary it. Even if simulators could in principle vary psychophysical laws in simulations, epistemological obstacles associated with consciousness may prevent simulators from figuring out how to do so. &nbsp;And even if simulators could vary the psychophysical laws in simulations and figure out how to do so, they may have no incentive to do so, as they may only be concerned with outputs of the simulation. For simplicity, I mostly hereafter set aside psychophysical fine-tuning and how different varieties of it interact with the simulation hypothesis and associated catastrophic risks.</span></p></div><div><p class="c3"><a href="#ftnt_ref127" id="ftnt127">[127]</a><span class="c6">&nbsp;For discussion of the simulation hypothesis and fine-tuning, see Chalmers (2022: Ch. 7) and Steinhart (2010). For an objection to some multiverse hypotheses on the ground that they require a complicated set of basic laws, see Cutter &amp; Saad (</span><span class="c12">forthcoming</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref128" id="ftnt128">[128]</a><span class="c6">&nbsp;For discussion of this theoretical vice, see, e.g., Chalmers (1996: 213-4)</span><span class="c12">, Cutter &amp; Saad (forthcoming), and Sider (2020: 102).</span></p></div><div><p class="c3"><a href="#ftnt_ref129" id="ftnt129">[129]</a><span class="c4">&nbsp;See Chalmers (2022: Ch. 7).</span></p></div><div><p class="c3"><a href="#ftnt_ref130" id="ftnt130">[130]</a><span class="c4">&nbsp;Though the same goes for the simulation argument.</span></p></div><div><p class="c3"><a href="#ftnt_ref131" id="ftnt131">[131]</a><span class="c6">&nbsp;Notable work on the topic includes Bostrom (2002</span><span class="c6 c17">b</span><span class="c4">), Dorr &amp; Arntzenius (2017), Elga (2000; 2004), Lewis (2001), Titelbaum (2013), Isaacs et al. (forthcoming), and Manley (ms).</span></p></div><div><p class="c3"><a href="#ftnt_ref132" id="ftnt132">[132]</a><span class="c4">&nbsp;For discussion of the inward-outward distinction, see Manley (ms).</span></p></div><div><p class="c3"><a href="#ftnt_ref133" id="ftnt133">[133]</a><span class="c6">&nbsp;See Bostrom (2002</span><span class="c6 c17">b</span><span class="c4">: Ch. 4) and Dorr &amp; Arntzenius (2017). </span></p></div><div><p class="c3"><a href="#ftnt_ref134" id="ftnt134">[134]</a><span class="c6">&nbsp;This is close to what Bostrom (2002</span><span class="c6 c17">b</span><span class="c6">: 66) calls the </span><span class="c6 c17">self-indication assumption</span><span class="c6">, which claims that &nbsp;&ldquo;[g]iven the fact that you exist, you should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.&rdquo; Like number boosting, this formulation of the self-indication assumption does not specify how big of a boost hypotheses with more observers should receive. &nbsp;However, Bostrom goes on to offer as a formalization of the self-indication assumption a principle which specifies the size of the boost for pairs of hypotheses and argue that the self-indication assumption should be rejected because of the implausible extent to which it favors certain hypotheses over others (</span><span class="c6 c17">ibid</span><span class="c4">: 122-6). &nbsp;The implausible consequences Bostrom derives from what he offers as a formalization of the self-indication assumption do not follow from his initial formulation of the principle, since it does not specify the size of the boost. &nbsp;For the same reason, his criticisms of the assumption do not apply to number boosting, and analogous criticisms would not apply to proportion boosting.</span></p></div><div><p class="c3"><a href="#ftnt_ref135" id="ftnt135">[135]</a><span class="c4">&nbsp;To yield a precise boost from number or proportion boosting in a given case, five parameters need to be set: </span></p><p class="c3 c18"><span class="c4">(i) your observer type(s), </span></p><p class="c3 c18"><span class="c4">(ii) your observation type(s), </span></p><p class="c3 c18"><span class="c4">(iii) the boosted observer type(s), </span></p><p class="c3 c18"><span class="c4">(iv) the boosted observation type(s), and </span></p><p class="c3 c18"><span class="c4">(v) the size of the boost.</span></p><p class="c3"><span class="c6">It is extremely difficult to find a plausible general principle for setting these parameters (cf. </span><span class="c6 c17">ibid</span><span class="c6">). That said, reflection on cases show that there are reasonable and unreasonable ways of setting the parameters and applying number and proportion boosting. &nbsp;This mirrors the case of inductive inferences </span><span class="c6 c17">not </span><span class="c4">concerning observers: while reasonable and unreasonable inferences are easy enough to find, the prospects for precisely delineating the reasonable inferences in that class seem bleak. This analogy is no accident: number and proportion boosting are self-locating inferences that are special cases of inductive inference from observations of something to a conclusion about a broader reference class that includes that thing. &nbsp;Noticing this should help allay any worry that, in the absence of a worked out account, the subject-involving character of number and proportion boosting renders their epistemology suspect. &nbsp;</span></p></div><div><p class="c3"><a href="#ftnt_ref136" id="ftnt136">[136]</a><span class="c6">&nbsp;See Chalmers (2010</span><span class="c6 c17">b</span><span class="c4">) and Morevac (1976; 1999); cf. Shulman &amp; Bostrom (2012).</span></p></div><div><p class="c3"><a href="#ftnt_ref137" id="ftnt137">[137]</a><span class="c4">&nbsp;See Cotra (2020) and Shulman &amp; Bostrom (2012: &sect;1.2). </span></p></div><div><p class="c3"><a href="#ftnt_ref138" id="ftnt138">[138]</a><span class="c4">&nbsp;For discussion of interactions between superintelligence and the simulation argument, see Prinz (2012).</span></p></div><div><p class="c3"><a href="#ftnt_ref139" id="ftnt139">[139]</a><span class="c4">&nbsp;See Shulman &amp; Bostrom (2012: &sect;&sect;3-4).</span></p></div><div><p class="c3"><a href="#ftnt_ref140" id="ftnt140">[140]</a><span class="c4">&nbsp;Cf. Carter (1983).</span></p></div><div><p class="c3"><a href="#ftnt_ref141" id="ftnt141">[141]</a><span class="c4">&nbsp;See Shulman &amp; Bostrom (2012: &sect;5) and Snyder-Beattie et al. (2021). </span></p></div><div><p class="c3"><a href="#ftnt_ref142" id="ftnt142">[142]</a><span class="c6">&nbsp;The argument traces back at least to Brandon Carter (who did not publish on it). Versions of it were later advanced by Gott (1993) and Leslie (1989: 214; 1996). I work with a formulation that is closer to Leslie&rsquo;s, as his formulation is widely regarded as yielding a more plausible (though still controversial) argument than Gott&rsquo;s. &nbsp;For background and critical discussion of much of the literature on the doomsday argument, see Bostrom (2002</span><span class="c6 c17">b</span><span class="c6">: Chs. 6-7). &nbsp;For notable variations of the argument that have received relatively little attention, see Grace (2010), Mogensen (2019</span><span class="c6 c17">a</span><span class="c4">), and Turchin (2018).</span></p></div><div><p class="c3"><a href="#ftnt_ref143" id="ftnt143">[143]</a><span class="c6">&nbsp;Cf. Bostrom (2002</span><span class="c6 c17">b</span><span class="c4">: 92-3).</span></p></div><div><p class="c3"><a href="#ftnt_ref144" id="ftnt144">[144]</a><span class="c4">&nbsp;See also Lewis (2013) and Richmond (2017).</span></p></div><div><p class="c3"><a href="#ftnt_ref145" id="ftnt145">[145]</a><span class="c6">&nbsp;For discussion and references, see Bostrom (2002</span><span class="c6 c17">b</span><span class="c4">: 122-6; Ch. 7, fn11).</span></p></div><div><p class="c3"><a href="#ftnt_ref146" id="ftnt146">[146]</a><span class="c4">&nbsp;For discussion, see &#262;irkovi&#263; (2018: &sect;4.5). </span></p></div><div><p class="c3"><a href="#ftnt_ref147" id="ftnt147">[147]</a><span class="c4">&nbsp;See Dainton (2020: 220).</span></p></div><div><p class="c3"><a href="#ftnt_ref148" id="ftnt148">[148]</a><span class="c4">&nbsp;This sort of strategy was (famously) promoted by Stephen Hawking&mdash;see, e.g., BBC (2010).</span></p></div><div><p class="c3"><a href="#ftnt_ref149" id="ftnt149">[149]</a><span class="c4">&nbsp;The import of this evidence is, however, blocked by Early Filter solutions on which the dearth of intelligent life is explained by a filter that renders life rare. Compare Armstrong (2014): &ldquo;The Great Filter is early, or AI is hard&rdquo;. &nbsp;If correct, this points to a worrying corollary of progress in AI: insofar as it gives us reason to think engineering human-level AI is easier than we thought, it also gives us reason to doubt Early Filter solutions to Fermi&rsquo;s paradox and hence to be more confident that the Filter lies in our future. &nbsp;But see Miller (2019) for argument that evidence for both an Early Filter and AI-based existential risks is less concerning than evidence for either alone would be. &nbsp;See also Hanson et al. (2021) for a solution to Fermi&rsquo;s paradox that lends to an explanation of how an Early Filter could cohere with human-level AI being relatively easy to engineer.</span></p></div><div><p class="c3"><a href="#ftnt_ref150" id="ftnt150">[150]</a><span class="c4">&nbsp;See Ward &amp; Brownlee (2000).</span></p></div><div><p class="c3"><a href="#ftnt_ref151" id="ftnt151">[151]</a><span class="c4">&nbsp;See Monton (2009: 99) for references.</span></p></div><div><p class="c3"><a href="#ftnt_ref152" id="ftnt152">[152]</a><span class="c4">&nbsp;See Zackrisson et al. (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref153" id="ftnt153">[153]</a><span class="c6">&nbsp;Cf. Carter (1983). Note, however, that it remains a live possibility that the universe has an infinite number of Earth-like planets and that this would in effect render it certain that there are other such planets that give rise to intelligent life no matter how unlikely it is that a given planet would&mdash;see Monton (2009: 102-4). &nbsp;The existence of an infinite number of observers also raises technical difficulties for the application of indifference principles&mdash;see Bostrom (2002</span><span class="c6 c17">b</span><span class="c4">), Dorr &amp; Arntzenius (2017), and White (2018).</span></p></div><div><p class="c3"><a href="#ftnt_ref154" id="ftnt154">[154]</a><span class="c4">&nbsp;For background on the physical theories, see Carroll (2020). &nbsp;For discussion, see Kotzen (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref155" id="ftnt155">[155]</a><span class="c4">&nbsp;For discussion of similarities between the simulation argument and the Boltzmann brain problem, see Crawford (2013).</span></p></div><div><p class="c3"><a href="#ftnt_ref156" id="ftnt156">[156]</a><span class="c4">&nbsp;Of course, a world in which almost all observers like us are Boltzmann brains would itself arguably involve catastrophe in the form of premature death on a cosmic scale. &nbsp;However, if there is a real risk of such a catastrophe, the catastrophe is presumably already underway and there is nothing we can do to mitigate it, with the possible exception of non-causal interventions like those discussed in &sect;13. In any event, I will set this sort of catastrophe aside.</span></p></div><div><p class="c3"><a href="#ftnt_ref157" id="ftnt157">[157]</a><span class="c4">&nbsp;See Chen (forthcoming).</span></p></div><div><p class="c3"><a href="#ftnt_ref158" id="ftnt158">[158]</a><span class="c6">&nbsp;See Saad (</span><span class="c12">forthcoming</span><span class="c12 c17">c</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref159" id="ftnt159">[159]</a><span class="c4">&nbsp;See Carroll (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref160" id="ftnt160">[160]</a><span class="c4">&nbsp;To my knowledge, nothing has been published on reducing catastrophic risks by using simulations as non-causal interventions. However, after submitting a research proposal to the Center on Long-Term Risk on the topic, I was informed that the idea had already been considered there. &nbsp;I plan to expand the ideas in this section into a stand-alone paper.</span></p></div><div><p class="c3"><a href="#ftnt_ref161" id="ftnt161">[161]</a><span class="c4">&nbsp;Cf. Bostrom (2003: 253). </span></p></div><div><p class="c3"><a href="#ftnt_ref162" id="ftnt162">[162]</a><span class="c4">&nbsp;See Leslie (1991).</span></p></div><div><p class="c3"><a href="#ftnt_ref163" id="ftnt163">[163]</a><span class="c4">&nbsp;In a recent survey of professional philosophers, participants were asked about their view of Newcomb&rsquo;s problem, a case that is standardly taken to elicit different recommendations from causal decision theory and evidential decision theory. &nbsp;31.2% favored the response standardly associated with evidential decision theory while 39.0% favored the response standardly associated with causal decision theory (Bourget &amp; Chalmers, 2021). &nbsp;It is thus natural to interpret these results as indicating that causal decision theory is more widely favored. &nbsp;However, there is also debate about whether Newcomb&rsquo;s problem bears on causal decision theory and evidential decision theory in the way that is standardly supposed&mdash;see Knab (2019: Ch. 3) for references and reasons to think not. </span></p></div><div><p class="c3"><a href="#ftnt_ref164" id="ftnt164">[164]</a><span class="c4">&nbsp;See Easwaran (2021) for an illuminating taxonomy and references.</span></p></div><div><p class="c3"><a href="#ftnt_ref165" id="ftnt165">[165]</a><span class="c4">&nbsp;See Easwaran (2021: &sect;3.2), Kagan (2000), and Nozick (1993).</span></p></div><div><p class="c3"><a href="#ftnt_ref166" id="ftnt166">[166]</a><span class="c4">&nbsp;See MacAskill (2016).</span></p></div><div><p class="c3"><a href="#ftnt_ref167" id="ftnt167">[167]</a><span class="c4">&nbsp;See MacAskill et al. (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref168" id="ftnt168">[168]</a><span class="c4">&nbsp;See Lockhart (2000), Bykvyst (2017), and MacAskill et al. (2020).</span></p></div><div><p class="c3"><a href="#ftnt_ref169" id="ftnt169">[169]</a><span class="c4">&nbsp;For a decision theory that treats risk aversion as a basic parameter, see Buchak (2013).</span></p></div><div><p class="c3"><a href="#ftnt_ref170" id="ftnt170">[170]</a><span class="c4">&nbsp;Respectively, see Greaves &amp; Cotton-Barratt (2019) and Newberry &amp; Ord (2021).</span></p></div><div><p class="c3"><a href="#ftnt_ref171" id="ftnt171">[171]</a><span class="c4">&nbsp;See Parfit (1984: &sect;95). </span></p></div><div><p class="c3"><a href="#ftnt_ref172" id="ftnt172">[172]</a><span class="c6">&nbsp;See Saad (forthcoming</span><span class="c6 c17">b</span><span class="c4">).</span></p></div><div><p class="c3"><a href="#ftnt_ref173" id="ftnt173">[173]</a><span class="c4">&nbsp;See Lewis (2004).</span></p></div><div><p class="c3"><a href="#ftnt_ref174" id="ftnt174">[174]</a><span class="c4">&nbsp;Cf. White (2005) and Wright (2004).</span></p></div><div><p class="c3"><a href="#ftnt_ref175" id="ftnt175">[175]</a><span class="c4">&nbsp;Cf. Dainton (2012: 68).</span></p></div></body></html>
</div>


    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
              © 2017–2023 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
