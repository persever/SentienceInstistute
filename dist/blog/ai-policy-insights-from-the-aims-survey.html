<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    

    
    <meta property="description" content="In this blog post, we summarize key AI policy insights from the Artificial Intelligence, Morality and Sentience (AIMS) survey to elucidate public opinion on pivotal AI safety issues." />
    <meta property="og:description" content="In this blog post, we summarize key AI policy insights from the Artificial Intelligence, Morality and Sentience (AIMS) survey to elucidate public opinion on pivotal AI safety issues." />
    
    
    <title>Sentience Institute | AI Policy Insights from the AIMS Survey</title>
    <meta property="title" content="AI Policy Insights from the AIMS Survey" />
    <meta property="og:title" content="AI Policy Insights from the AIMS Survey" />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/blog/ai-policy-insights-from-the-aims-survey" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/blog/240222.png" />
    <meta name="twitter:image" content="http://www.sentienceinstitute.org/img/blog/240222.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/reports">Reports</a></li>
                <li><a href="/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) Survey</a></li>
                <li><a href="/aft-survey">Animals, Food, and Technology (AFT) Survey</a></li>
                <!-- <li><a href="/blog">Blog</a></li> -->
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <!-- <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li> -->
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <!-- <li><a href="/mission">Our Mission</a></li> -->
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <!-- <li><a href="/faq">FAQ</a></li> -->
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container" img-id="240222" style="background-image: url(/img/blog/240222.png);">
  
  
  

<div data-nosnippet class="container image-info-container">
  <div class="image-credit">
    <i class="material-icons photo-icon">photo_camera</i>
    <span>
      Michael Dello-Iacovo / Midjourney
    </span>
  </div>
  <div class="image-title">
    <div class="image-title-text">
      a futuristic parliament house with no people inside, --ar 16:9 --v 6.0
    </div>
    <div class="arrow-down"></div>
  </div>
</div>


  
</div>

<div class="container first-container gdoc-html-container blog-container ai-policy-insights-from-the-aims-survey-container">
  <div class="title">
    AI Policy Insights from the AIMS Survey
  </div>
  <div class="author-info">
    
    <div class="author">
      <div class="author-img"><img src="../img/team/justin.jpg"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Justin Bullock</div>
      <div class="author-role">Research Fellow</div>
    </div>
  </div>
  
    <div class="author">
      <div class="author-img"><img src="../img/team/janet.png"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Janet Pauketat</div>
      <div class="author-role">Research Fellow</div>
    </div>
  </div>
  
  </div>
  <div class="date">
    February 22, 2024
  </div>
  <html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c29 doc-content"><div><p class="c4 c25"><span class="c20"></span></p></div><p class="c1"><span class="c7 c0">Edited by Jacy Reese Anthis, Spencer Case, and Michael Dello-Iacovo. Thanks to Christopher DiCarlo and Elliot McKernon for their insights.</span></p><h1 class="c3" id="h.57bo537gvmch"><span class="c15">Abstract</span></h1><p class="c1"><span class="c16">The accelerating development of artificial intelligence (AI) systems has raised widespread concerns about risks from advanced AI. This poses important questions for policymakers regarding appropriate governance strategies to address these risks. However, effective policy requires understanding public attitudes and priorities regarding AI risks and interventions. In this blog post, we summarize key AI policy insights from the </span><span class="c6 c26"><a class="c5" href="https://www.sentienceinstitute.org/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) survey</a></span><span class="c16 c19">&nbsp;to elucidate public opinion on pivotal AI safety issues.</span></p><p class="c1 c4"><span class="c16 c19"></span></p><p class="c1"><span class="c16">Insights</span><span class="c16 c19">&nbsp;include: </span></p><p class="c1 c4"><span class="c16 c19"></span></p><ul class="c14 lst-kix_h3ir9jj7eguq-0 start"><li class="c1 c21 li-bullet-0"><span class="c16 c19">High concern about the fast pace of AI development; </span></li><li class="c1 c21 li-bullet-0"><span class="c16 c19">Expectation of artificial general intelligence (AGI) within 5 years; </span></li><li class="c1 c21 li-bullet-0"><span class="c16 c19">Widespread concern about AI catastrophic and existential threats; </span></li><li class="c1 c21 li-bullet-0"><span class="c16 c19">Support for regulations and bans to slow AI;</span></li><li class="c1 c21 li-bullet-0"><span class="c16">Surprisingly high concern for AI welfare.</span></li></ul><p class="c1 c4"><span class="c16 c19"></span></p><p class="c1 c30"><span class="c16">These findings imply that AI safety proponents may expect to be relatively successful with strong proposals for risk mitigation. When forecasting the future trajectory of AI</span><span class="c16">, policies attuned to widely shared public sentiments are more likely to be accepted.</span><span class="c16 c19">&nbsp;Acting decisively yet prudently on public priorities will be critical as advanced AI capabilities continue to emerge.</span></p><h1 class="c30 c1 c31" id="h.xx97zjgjhxxj"><span class="c15">Table of Contents</span></h1><p class="c18"><span class="c8 c6"><a class="c5" href="#h.kbb9fb2ul5zk">Introduction</a></span></p><p class="c18"><span class="c8 c6"><a class="c5" href="#h.w1fcjuh7fbu7">1. The U.S. public is seriously concerned about the pace of AI development.</a></span></p><p class="c18"><span class="c8 c6"><a class="c5" href="#h.gei5yopdawll">2. The U.S. public seems to expect that very advanced AI systems will be developed within the next 5 years.</a></span></p><p class="c18"><span class="c8 c6"><a class="c5" href="#h.61b4u5n0rgv7">3. The U.S. public is concerned about the risks AI presents to humanity, including risks of extinction, and concerned about risks of harm to the AIs themselves.</a></span></p><p class="c18"><span class="c8 c6"><a class="c5" href="#h.nttnyqgnggst">4. Specific policy tools designed for increasing AI safety are supported by a majority of the U.S. public.</a></span></p><p class="c18"><span class="c6 c8"><a class="c5" href="#h.aw9ghsqau2ut">5. The U.S. public is generally opposed to the development of AI sentience and human-AI integration.</a></span></p><h1 class="c3" id="h.kbb9fb2ul5zk"><span>Introduction</span></h1><p class="c1"><span>2023 was a pivotal year for AI development. OpenAI&rsquo;s November 2022 release of ChatGPT sparked massive public interest in AI. By February 2023, </span><span>ChatGPT had </span><span class="c6"><a class="c5" href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">100 million monthly active users</a></span><span>. This fueled a </span><span class="c6"><a class="c5" href="https://www.cbsnews.com/news/microsoft-ceo-satya-nadella-new-ai-search-engine/">race</a></span><span>&nbsp;to produce advanced large language models </span><span>(LLMs)</span><span>, the architecture behind ChatGPT. </span><span>Microsoft launched </span><span class="c6"><a class="c5" href="https://www.microsoft.com/en-us/edge/features/bing-chat">Bing Chat</a></span><span>&nbsp;in February 2023.</span><span>&nbsp;</span><span class="c6"><a class="c5" href="https://openai.com/research/gpt-4">GPT-4</a></span><span>&nbsp;was released</span><span>&nbsp;in March 2023.</span><span>&nbsp;Google released</span><span>&nbsp;</span><span class="c6"><a class="c5" href="https://bard.google.com/">Bard</a></span><span>&nbsp;on March 21, followed by </span><span class="c6"><a class="c5" href="https://ai.google/discover/palm2/">PaLM 2</a></span><span>&nbsp;in May</span><span>.</span><span>&nbsp;</span><span>In July 2023, Meta released </span><span class="c6"><a class="c5" href="https://ai.meta.com/llama/">LLama 2</a></span><span>.</span><span>&nbsp;</span><span>The accessibility of this new generation of AI models to the public </span><span>increased their use of chatbots and LLMs for routine administrative work, coding, video games, social media content, and information searches such as Microsoft&rsquo;s </span><span>Bing</span><span>&nbsp;search engine. Prior to these developments there was public concern about </span><span>AI in domains like social media, surveillance, policing, and warfare, but the rapid proliferation of easy-to-access LLMs amplified worries about AI safety and governance</span><span class="c2">. </span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>The release and subsequent popularity of these models galvanized researchers to study the risks they pose to human extinction. Prominent AI researchers </span><span class="c6"><a class="c5" href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html">Geoffrey Hinton</a></span><span>&nbsp;and </span><span class="c6"><a class="c5" href="https://www.lesswrong.com/posts/8kHgaLYamxQdE2zk7/yoshua-bengio-how-rogue-ais-may-arise">Yoshua Bengio</a></span><span>&nbsp;joined a chorus of voices including</span><span>&nbsp;</span><span class="c6"><a class="c5" href="https://www.wsj.com/tech/ai/ai-expert-max-tegmark-warns-that-humanity-is-failing-the-new-technologys-challenge-4d423bee">Max Tegmark</a></span><span>&nbsp;and </span><span class="c6"><a class="c5" href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">Eliezer Yudkowsky</a></span><span>&nbsp;</span><span class="c2">calling attention to the risks from rapid expansion of frontier AI systems without appropriate safety protocols or socio-political preparation. </span></p><p class="c1"><span>Further, these AI developments renewed attention to an issue long debated in philosophy, computer science, and cognitive science: </span><span class="c6"><a class="c5" href="https://arxiv.org/abs/2303.07103">AI sentience</a></span><span>.</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span>&nbsp;Blake Lemoine, who had been an engineer working on Google&rsquo;s LLM, </span><span class="c6"><a class="c5" href="https://blog.google/technology/ai/lamda/">LaMDA</a></span><span>, speculated that LaMDA </span><span class="c6"><a class="c5" href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">was self-aware</a></span><span>. </span><span>This alerted the public to the near-future possibility of living with sentient AIs</span><span>, although scientists and philosophers generally would not agree LaMDA is sentient. As LLMs are trained on larger portions of human language and history, their behaviors have become more complex, prompting increased interest in the possibility of </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/key-questions-for-digital-minds">digital minds</a></span><span class="c2">&nbsp;and a need for research on public perceptions of digital minds. </span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>The Sentience Institute tracks U.S. public opinion on the moral and social perception of different types of AIs, particularly sentient AIs, in the nationally representative </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS)</a></span><span>&nbsp;survey. In 2023, a </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/aims-survey-supplement-2023">supplemental</a></span><span>&nbsp;AIMS survey was conducted on responses to current developments in AI with a focus on attitudes towards </span><span>AI safety</span><span>, AI </span><span class="c6"><a class="c5" href="https://futureoflife.org/existential-risk/existential-risk/">existential risks</a></span><span>&nbsp;(x-risks), and LLMs. With advanced AI systems like ChatGPT becoming more accessible to the public,</span><span class="c2">&nbsp;it is crucial for policymakers to grasp the public&#39;s perspectives on certain issues, such as:</span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_61p5icd1ukyq-0 start"><li class="c1 c21 li-bullet-0"><span class="c2">The pace of AI advancement</span></li><li class="c1 c21 li-bullet-0"><span>When advanced </span><span>AI systems</span><span class="c2">&nbsp;will emerge</span></li><li class="c1 c21 li-bullet-0"><span>Catastrophic and existential risk</span><span class="c2">s to humanity and moral concern for AI welfare</span></li><li class="c1 c21 li-bullet-0"><span>H</span><span>uman-AI integration</span><span class="c2">, enhancement, and hybridization</span></li><li class="c1 c21 li-bullet-0"><span class="c2">Policy interventions such as campaigns, regulations, and bans</span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>This would empower </span><span>policymakers to</span><span class="c2">:</span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_i7047rtj9u0-0 start"><li class="c1 c21 li-bullet-0"><span class="c2">Gauge the level of policy assertiveness needed to align with public concerns.</span></li><li class="c1 c21 li-bullet-0"><span class="c2">Craft policies that are viable given public acceptability.</span></li><li class="c1 c21 li-bullet-0"><span>Build trust in governance by addressing risks that resonate with the public&#39;s concerns</span><span class="c2">.</span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>We </span><span>highlight five specific insights from the AIMS survey:</span></p><p class="c1 c4"><span class="c2"></span></p><ol class="c14 lst-kix_em9iqfsu6r9b-0 start" start="1"><li class="c1 c21 li-bullet-0"><span class="c10">The U.S. public is seriously concerned about the pace of AI development</span><span class="c10">. </span></li></ol><ul class="c14 lst-kix_em9iqfsu6r9b-1 start"><li class="c1 c12 li-bullet-0"><span class="c7 c0">49% believe the pace of AI development is too fast.</span></li></ul><p class="c1 c4 c24"><span class="c7 c0"></span></p><ol class="c14 lst-kix_em9iqfsu6r9b-0" start="2"><li class="c1 c21 li-bullet-0"><span class="c10">The U.S. public seems to expect that very advanced AI systems will be developed within the next 5 years</span><span class="c11 c10">. </span></li></ol><ul class="c14 lst-kix_em9iqfsu6r9b-1 start"><li class="c1 c12 li-bullet-0"><span class="c0">The median predictions for when Artificial General Intelligence (AGI), Human-Level Artificial Intelligence (HLAI), and Artificial Superintelligence (ASI) will be created are </span><span class="c0">2 years, 5 years, and 5 years.</span></li></ul><p class="c1 c4 c24"><span class="c7 c0"></span></p><ol class="c14 lst-kix_em9iqfsu6r9b-0" start="3"><li class="c1 c21 li-bullet-0"><span class="c10">The U.S. public is concerned about the risks AI presents to humanity, including risks of extinction, and concerned about risks of harm to the AIs themselves</span><span class="c10">.</span></li></ol><ul class="c14 lst-kix_em9iqfsu6r9b-1 start"><li class="c1 c12 li-bullet-0"><span class="c0">48% </span><span class="c7 c0">are concerned that, &ldquo;AI is likely to cause human extinction.&rdquo; </span></li><li class="c1 c12 li-bullet-0"><span class="c7 c0">52% are concerned about &ldquo;the possibility that AI will cause the end of the human race on Earth.&rdquo; </span></li><li class="c1 c12 li-bullet-0"><span class="c7 c0">53% support campaigns against the exploitation of AIs.</span></li><li class="c1 c12 li-bullet-0"><span class="c7 c0">43% support welfare standards that protect the wellbeing of AIs.</span></li><li class="c1 c12 li-bullet-0"><span class="c7 c0">68% agree that we must not cause unnecessary suffering for LLMs if they develop the capacity to suffer.</span></li></ul><p class="c1 c4 c24"><span class="c7 c0"></span></p><ol class="c14 lst-kix_em9iqfsu6r9b-0" start="4"><li class="c1 c21 li-bullet-0"><span class="c10">Specific policy tools designed for increasing AI safety are supported by a majority of the U.S. public. </span></li></ol><ul class="c14 lst-kix_em9iqfsu6r9b-1 start"><li class="c1 c12 li-bullet-0"><span class="c0">71</span><span class="c0">%</span><span class="c0">&nbsp;support public campaigns to slow AI development.</span><span class="c7 c0">&nbsp;</span></li><li class="c1 c12 li-bullet-0"><span class="c0">71%</span><span class="c0">&nbsp;support government regulation that slows AI development</span><span class="c0">; when asked instead about opposition, </span><span class="c0">37% oppose it.</span></li><li class="c1 c12 li-bullet-0"><span class="c0">63</span><span class="c0">%</span><span class="c0">&nbsp;support banning the development of artificial general intelligence that is smarter than humans.</span><span class="c7 c0">&nbsp; </span></li><li class="c1 c12 li-bullet-0"><span class="c0">64</span><span class="c0">%</span><span class="c7 c0">&nbsp;support a global ban on data centers that are large enough to train AI systems that are smarter than humans. </span></li><li class="c1 c12 li-bullet-0"><span class="c0">69%</span><span class="c7 c0">&nbsp;support a six-month pause on AI development. </span></li></ul><p class="c1 c4 c24"><span class="c0 c7"></span></p><ol class="c14 lst-kix_em9iqfsu6r9b-0" start="5"><li class="c1 c21 li-bullet-0"><span class="c10">The U.S. public is generally opposed to the development of </span><span class="c10">AI sentience and human-AI integration</span><span class="c10">.</span></li></ol><ul class="c14 lst-kix_em9iqfsu6r9b-1 start"><li class="c1 c12 li-bullet-0"><span class="c0">70</span><span class="c0">%</span><span class="c0">&nbsp;support a global ban on the development of sentience in AIs.</span><span class="c7 c0">&nbsp;</span></li><li class="c1 c12 li-bullet-0"><span class="c0">71%</span><span class="c7 c0">&nbsp;support a global ban on the development of AI-enhanced humans. </span></li><li class="c1 c12 li-bullet-0"><span class="c0">72</span><span class="c0">%</span><span class="c0">&nbsp;support a global ban on the development of robot-human hybrids.</span></li><li class="c1 c12 li-bullet-0"><span class="c0">59%</span><span class="c0">&nbsp;oppose </span><span class="c0">uploading human minds to computers (i.e., </span><span class="c0">&ldquo;</span><span class="c0">mind uploading</span><span class="c0">&rdquo;).</span></li></ul><h1 class="c3" id="h.w1fcjuh7fbu7"><span>1. The U.S. public is seriously concerned about the pace of AI development</span><span>.</span></h1><p class="c1"><span class="c0">Key AIMS finding: </span><span class="c0">49%</span><span class="c7 c0">&nbsp;believe the pace of AI development is too fast.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>In 2023, </span><span class="c10">49% of AIMS respondents believed that AI development is too fast</span><span>. Only 2.5% thought that AI development is too slow, 19% were uncertain, and 30% believed the pace of development is fine. Alongside </span><span class="c6"><a class="c5" href="https://theaipi.org/poll-shows-overwhelming-concern-about-risks-from-ai-as-new-institute-launches-to-understand-public-opinion-and-advocate-for-responsible-ai-policies/">the Artificial Intelligence Policy Institute&rsquo;s (AIPI) poll result</a></span><span>&nbsp;that 72% of Americans prefer slowing down the development of AI and 8% prefer speeding it up (and 72% and 15% in the version of that question with more detail), there is clear evidence of the U.S. public&rsquo;s support for slowing AI development.</span><span class="c2">&nbsp;</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Leading AI experts are also concerned about the pace of AI development. An open letter advocating for pausing the development of AIs more powerful than GPT-4 spearheaded by the </span><span class="c6"><a class="c5" href="https://futureoflife.org">Future of Life Institute</a></span><span>&nbsp;was signed by over 33,700 experts. An </span><span class="c6"><a class="c5" href="https://www.safe.ai/faq">open statement</a></span><span>&nbsp;by the </span><span class="c6"><a class="c5" href="https://www.safe.ai">Center for AI Safety</a></span><span>&nbsp;(CAIS) attracted a wide range of prominent signatories to its simple statement that, </span><span>&ldquo;</span><span>Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.&rdquo;</span><span class="c2">&nbsp;</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>To explain the survey and real-world evidence of widespread concern, we would place the factors that drive public opinion and expert concern into four categories: </span><span>science fiction and mass media, </span><span>past issues, risks of rapid integration into human socio-political institutions, and the problematic behavior of current AI systems. </span></p><p class="c1 c4 c22"><span class="c7 c0"></span></p><ul class="c14 lst-kix_f0h0wyg7ik10-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">On science fiction: </span><span>Public opinion on AI is presumably </span><span class="c6"><a class="c5" href="https://academic.oup.com/isq/article/62/3/562/5077057">shaped</a></span><span>&nbsp;by the many famous portrayals of dangerous AI in science fiction, such as </span><span class="c0">The Terminator</span><span>&nbsp;and </span><span class="c0">2001: A Space Odyssey</span><span>.</span><span>&nbsp;</span><span>Nonexperts</span><span>&nbsp;</span><span class="c6"><a class="c5" href="https://www.pnas.org/doi/full/10.1073/pnas.1320645111">get most of their information</a></span><span>&nbsp;about science concepts from mass media. </span><span>Narratives and narrative-style mass media increase comprehension and </span><span class="c6"><a class="c5" href="https://www.sciencedirect.com/science/article/abs/pii/0010028577900056">recall</a></span><span>, and are engaging and </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/mass-media-propaganda-and-social-influence">persuasive</a></span><span>. </span><span class="c2">&nbsp;</span></li></ul><p class="c1 c4 c22"><span class="c11 c10"></span></p><ul class="c14 lst-kix_f0h0wyg7ik10-0"><li class="c1 c21 li-bullet-0"><span class="c10">On past issues: </span><span>There have already been clear societal harms from earlier generations of AI. For example, social media recommender algorithms have played a </span><span class="c6"><a class="c5" href="https://www.nytimes.com/2022/10/13/technology/misinformation-integrity-institute-report.html">significant role</a></span><span>&nbsp;</span><span>in the vast spread of misinformation and disinformation</span><span>.</span><span>&nbsp;AIs deployed in other contexts (e.g., healthcare, judicial sentencing, policing) have exhibited bias, which is partly a result of the </span><span class="c6"><a class="c5" href="https://dl.acm.org/doi/abs/10.1145/3457607">disparities</a></span><span>&nbsp;embedded within their training data.</span><span>&nbsp;</span><span>The ethical deployment of AI also has been questioned given some uses to </span><span class="c6"><a class="c5" href="https://freedomhouse.org/report/freedom-net/2023/repressive-power-artificial-intelligence">repress</a></span><span>&nbsp;populations lacking protections against these </span><span>abuses</span><span>. </span></li></ul><p class="c1 c4"><span class="c8 c13 c27"></span></p><ul class="c14 lst-kix_26o0k2p4ttod-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">On rapid integration: </span><span>Awareness of </span><span class="c6"><a class="c5" href="https://arxiv.org/abs/2108.07258">foundation models</a></span><span>&rsquo; </span><span>(e.g., ChatGPT, DALL-E) powerful capabilities </span><span>is quickly</span><span>&nbsp;spreading among the public, industry, and elites in powerful institutions</span><span>. </span><span>For example, the </span><span>U.S</span><span>.</span><span>&nbsp;General Services Administration</span><span class="c13">&nbsp;</span><span class="c6"><a class="c5" href="https://www.challenge.gov/?challenge%3Dapplied-ai-challenge">held a competition</a></span><span class="c13">&nbsp;</span><span>to find the best ways to integrate AI into government agencies. </span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1 c22"><span>Although the integration of AI into organizational decision-making can </span><span>yield</span><span>&nbsp;benefits (e.g., efficiency), it presents risks as well. In </span><span class="c6"><a class="c5" href="https://academic.oup.com/ppmg/article-abstract/4/3/244/6218935">Young et al. (2021)</a></span><span>, we proposed</span><span>&nbsp;that the risks from automating tasks within organizations include </span><span>AI exuberance, quantification bias, and organizational value misalignment. As </span><span>AIs become more capable, they may be able to complete more general sets of tasks and may be used before they are effective substitutes for the nuances of human judgment. This is particularly challenging for holding governments democratically accountable and can set the stage for increases in </span><span class="c6"><a class="c5" href="https://sk.sagepub.com/books/unmasking-administrative-evil">&ldquo;administrative evil,&rdquo;</a></span><span>&nbsp;which we view as the infliction of unnecessary suffering in public </span><span>administration</span><span>. </span><span>For example, a worthy non-traditional applicant to a social benefits program might be denied because an AI makes decisions in a way tuned to the more common, traditional applicants and lacks the nuance to correctly categorize the non-traditional applicant</span><span>. </span></p><p class="c1 c4 c22"><span class="c2"></span></p><p class="c1 c22"><span class="c6"><a class="c5" href="https://www.brookings.edu/articles/preparing-for-the-non-existent-future-of-work/">Job loss</a></span><span>&nbsp;is another risk of organizational restructuring based on increased awareness and use of AI</span><span>. </span><span>Organizational restructuring</span><span>&nbsp;that puts job performance evaluation, monitoring, and hiring or firing decision-making in the purview of AIs rather than humans is opposed by a majority of Americans, according to the </span><span class="c6"><a class="c5" href="https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-views-of-artificial-intelligence/">Pew Research Center</a></span><span>. </span><span class="c6"><a class="c5" href="https://www.pewresearch.org/short-reads/2019/04/08/how-americans-see-automation-and-the-workplace-in-7-charts/">American workers</a></span><span class="c13">&nbsp;</span><span>expect automation to shape the future of work for the worse. However, </span><span class="c6"><a class="c5" href="https://www.imf.org/en/Publications/WP/Issues/2019/12/20/Automation-Skills-and-the-Future-of-Work-What-do-Workers-Think-48791">workers in general, and especially in emerging markets</a></span><span class="c13">, feel more positive than negative</span><span class="c2">&nbsp;about automation and job loss. </span></p><p class="c1 c4"><span class="c8 c13 c27"></span></p><ul class="c14 lst-kix_o4pbhsi9dz3g-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">On problematic behavior:</span><span>&nbsp;</span><span>AI</span><span>&nbsp;capabilities have </span><span class="c6"><a class="c5" href="https://time.com/6300942/ai-progress-charts/">significantly increased</a></span><span class="c13">&nbsp;</span><span>on</span><span class="c13">&nbsp;</span><span class="c6"><a class="c5" href="https://ourworldindata.org/brief-history-of-ai">many dimensions</a></span><span>,</span><span>&nbsp;and they are exhibiting more evidence of having mental faculties that constitute </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/key-questions-for-digital-minds">digital minds</a></span><span>. Generative AIs sometimes </span><span>behave in</span><span class="c13">&nbsp;</span><span class="c6"><a class="c5" href="https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says">harmful</a></span><span class="c13">&nbsp;</span><span class="c6"><a class="c5" href="https://arxiv.org/abs/2309.01219">ways</a></span><span class="c13">&nbsp;</span><span>that have been intensified in the new generation of deep neural networks. Some generative AI behaviors challenge assumptions that AIs will be benign tools that are completely under human control.</span><span>&nbsp;The pace of AI development suggests a race to the bottom</span><span class="c2">&nbsp;as AI labs work to increase the capacities of AIs and publicly release their models in the marketplace to gain first mover advantages and a stronger reputation. </span></li></ul><h1 class="c3" id="h.gei5yopdawll"><span>2. </span><span>The U.S. public seems to expect that very advanced AI systems will be developed within the next 5 years</span><span class="c15">.</span></h1><p class="c1"><span class="c7 c0">Key AIMS Finding: The median predictions for when Artificial General Intelligence (AGI), Human-Level Artificial Intelligence (HLAI), and Artificial Superintelligence (ASI) will be created are 2 years, 5 years, and 5 years.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>T</span><span>hree thresholds of AI development have particularly captured the attention of experts and forecasters: (1) Artificial General Intelligence (AGI), (2) Human-Level Artificial Intelligence (HLAI), and (3) Artificial Superintelligence (ASI). </span><span>AIMS respondents forecasted about the emergence of these advanced AIs, providing a novel third perspective. </span><span>The AIMS survey data suggest that the median U.S. adult expects AI to first become general in its capabilities and shortly thereafter to show human-level capabilities and superhuman intelligence. </span><span>The U.S. public seems to expect these changes soon.</span><span class="c2">&nbsp;</span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_fjdreu19cx3x-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">Artificial General Intelligence (AGI)</span><span class="c10">&nbsp;timelines: </span><span>The modal estimate for when AGI will be created is 2 years from 2023. Thirty-four percent</span><span>&nbsp;of the U.S. public believes that AGI already exists, and only </span><span>2%</span><span>&nbsp;believe AGI will never happen</span><span>&nbsp;w</span><span>hen asked, &ldquo;If you had to guess, how many years from now do you think that the first artificial general intelligence will be created?&rdquo; </span></li></ul><p class="c1 c4 c24"><span class="c2"></span></p><ul class="c14 lst-kix_fjdreu19cx3x-0"><li class="c1 c21 li-bullet-0"><span class="c10">Human-Level Artificial Intelligence (HLAI) timelines: </span><span>The modal estimate for when HLAI will be created is 5 years from 2023.</span><span class="c10">&nbsp;</span><span>Twenty-three percent</span><span>&nbsp;of the U.S. public believes that HLAI already exists, and only </span><span>4%</span><span class="c2">&nbsp;believe it will never happen when asked, &ldquo;If you had to guess, how many years from now do you think that the first human-level AI will be created?&rdquo; </span></li></ul><p class="c1 c4 c24"><span class="c2"></span></p><ul class="c14 lst-kix_fjdreu19cx3x-0"><li class="c1 c21 li-bullet-0"><span class="c10">Artificial Superintelligence (ASI) timelines: </span><span>The modal guess for when ASI will be created is 5 years from 2023. Twenty-four percent</span><span>&nbsp;of the U.S. public believes ASI already exists, and only </span><span>3%</span><span class="c2">&nbsp;believe it will never happen when asked, &ldquo;If you had to guess, how many years from now do you think that the first artificial superintelligence will be created?&rdquo; </span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Definitions were not supplied for these terms. Instead, the AIMS survey relied on the general public&rsquo;s understanding of and intuition about the meanings of these terms in order to create a baseline of data that can be built on in the future as discourse evolves. Many </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/artificial-sentience-terminology">AI terms</a></span><span class="c2">&nbsp;have no agreed upon definition among experts, forecasters, or the general public. Even with a particular definition of capabilities, there is significant disagreement among researchers on what empirical evidence would show those capabilities.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>With these qualifications in mind</span><span>, the public&rsquo;s timelines are shorter than both current prediction markets and recent expert surveys (see below), both of which have intimated specific definitions and received more attention.</span><span>&nbsp;The public&rsquo;s expectations matter insofar as they signal what people care about and whether or not there would be public support for regulation or legislative change. Public opinion has been documented as having a substantial impact on </span><span class="c6"><a class="c5" href="https://journals.sagepub.com/doi/abs/10.1177/106591290305600103">policymaking</a></span><span>&nbsp;and </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/scotus">Supreme Court decisions</a></span><span class="c2">. In the context of AI, if people expect AGI, HLAI, or ASI to emerge in the next 5 years, policymakers should expect greater public support for AI-related legislation in the near future. The short timelines may also be reflected in policymakers&rsquo; personal timelines, also increasing their prioritization of AI issues.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Forecasters offer one of the other more frequently cited timelines to advanced AI. </span><span class="c6"><a class="c5" href="https://www.metaculus.com/ai/">Metaculus</a></span><span>, one of the most prominent forecasting platforms, hosts ongoing predictions for AGI. As of December 2023, the ongoing predictions from the Metaculus community put </span><span class="c6"><a class="c5" href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">AGI in August 2032</a></span><span>, or 9 years from 2023, though there is much to be contested </span><span>in</span><span>&nbsp;how it operationalizes AGI, and it is unclear what incentives Metaculus users have for accurate forecasts. Nonetheless, at face value, this is notably 4</span><span>x</span><span>&nbsp;longer than the U.S. public&rsquo;s median timeline. Other Metaculus predictions forecast </span><span class="c6"><a class="c5" href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/">weak AGI in October 2026</a></span><span>&nbsp;(close to the 2 years to AGI expected by the public) and </span><span class="c6"><a class="c5" href="https://www.metaculus.com/questions/9062/time-from-weak-agi-to-superintelligence/">ASI 2 years after that</a></span><span>&nbsp;(i.e., in approximately 12 years; yes, this implies ASI 4 years before AGI, even though scholars typically consider AGI to be weaker than ASI). This is just one example of the challenges with forecasting such events in practice, but we nonetheless include these estimates for context.) Ongoing predictions about </span><span class="c6"><a class="c5" href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/">weakly general AI systems</a></span><span>&nbsp;put weak AGI </span><span>emergence in </span><span>March 2026</span><span>, approximately 3 years from 2023, and very close to the U.S. public&rsquo;s expectation for AGI. </span><span>Another Metaculus question suggests a </span><span class="c6"><a class="c5" href="https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/">95% chance</a></span><span class="c2">&nbsp;of &ldquo;human-machine intelligence parity before 2040,&rdquo; which could be considered an HLAI prediction for comparison to the AIMS results, though, as another example of the qualification necessary with such estimates, this operationalization only requires that a machine could pass a Turing-like test with three human judges, which could occur even if, say, 50 such tests are run, and one set of judges happen to be easily persuaded. </span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Question framing and terminology in questions shape the answers of experts and the general public alike. Metaculus forecasters&rsquo; varying timelines in response to the differently framed questions about weak AGI are a case in point. With the framing of our survey (i.e., just sharing terms with no explanation), the U.S. public expects AGI first then </span><span>HLAI and ASI at approximately the same time, but the wording of the Metaculus questions suggest HLAI is easiest, followed by AGI then ASI.</span><span class="c2">&nbsp;</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>In addition to Metaculus and public timelines, other surveys include the three covered by </span><span class="c6"><a class="c5" href="https://ourworldindata.org/ai-timelines">Roser (2023)</a></span><span>, which are also documented in Figure 1</span><span>. </span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_rd4xkissnp35-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">2018:</span><span>&nbsp;One-hundred and sixty-five AI experts were asked when AI systems will collectively be able to accomplish 99% of tasks that humans are paid to do at or above the level of a typical human. Half of the experts gave a date before 2068 (median:</span><span>&nbsp;45 years</span><span class="c2">).</span></li></ul><p class="c1 c4 c22"><span class="c2"></span></p><ul class="c14 lst-kix_rd4xkissnp35-0"><li class="c1 c21 li-bullet-0"><span class="c10">2019:</span><span>&nbsp;Two-hundred and ninety-six AI experts were asked when machines will collectively be able to perform more than 90% of all tasks that are economically relevant better than the median human paid to do that task. Half of the experts gave a date before 2060 (</span><span>median:</span><span class="c2">&nbsp;37 years). </span></li></ul><p class="c1 c4 c22"><span class="c2"></span></p><ul class="c14 lst-kix_rd4xkissnp35-0"><li class="c1 c21 li-bullet-0"><span class="c10">2022:</span><span>&nbsp;Three-hundred and fifty-six AI experts were asked when unaided machines will be able to accomplish every task better and more cheaply than human workers. Half of the experts gave a date before 2061 (</span><span>median:</span><span class="c2">&nbsp;38 years). </span></li></ul><p class="c1 c4 c22"><span class="c2"></span></p><p class="c1"><span class="c0">Figure 1: </span><span>Expert Timelines for Human-Level Artificial Intelligence from 2018-2022 Surveys</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1 c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 734.51px; height: 250.83px;"><img alt="" src="images/ai-policy-insights-from-the-aims-survey/image1.png" style="width: 734.51px; height: 250.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span class="c0">Note: This image was sourced from </span><span class="c6 c0"><a class="c5" href="https://ourworldindata.org/ai-timelines">Our World in Data</a></span><span class="c7 c0">.</span></p><p class="c1 c4"><span class="c7 c0"></span></p><p class="c1"><span>A </span><span class="c6"><a class="c5" href="https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things">2023 survey</a></span><span>&nbsp;of 2,778 AI researchers found an estimated 50% chance that autonomous machines would outperform humans in every possible task by 2047 and a 10% chance by 2027 if science were to continue undisrupted. A majority, 70% of respondents, indicated their desire to see more research on minimizing risks of AI systems.</span></p><h1 class="c3" id="h.61b4u5n0rgv7"><span>3.</span><span>&nbsp;The U.S. public is concerned about the risks AI presents to humanity, including risks of extinction, and concerned about risks of harm to the AIs themselves</span><span>.</span></h1><p class="c1"><span class="c0">Key AIMS Findings: 4</span><span class="c0">8% </span><span class="c0">are concerned that, &ldquo;AI is likely to cause human extinction.</span><span class="c0">&rdquo; 52% are concerned about &ldquo;the possibility that AI will cause the end of the human race on Earth.&rdquo; 53% support campaigns against the exploitation of AIs. 43% support welfare standards that protect the wellbeing of robots/AIs. 68% agree that we must not cause unnecessary suffering for LLMs i</span><span class="c0">f they develop the capacity to suffer</span><span class="c7 c0">.</span></p><p class="c1 c4"><span class="c7 c0"></span></p><p class="c1"><span>AI development towards more capable, generally skilled, intelligent systems is concerning to both the U.S. public and AI experts. </span><span>Fast AI development brings significant risks to humanity and to the general existential trajectory of humanity and AIs. Additionally, the ongoing harms that the unreflective use of current AI systems bring to society (e.g., data privacy, discrimination in healthcare diagnosis) may be exacerbated by development that is too fast. Moreover, humans face threats to their long-term survival and potential to build a </span><span class="c6"><a class="c5" href="https://doi.org/10.1007/%2520s13347-020-00421-8">harmonious</a></span><span>, integrated human-AI society&mdash;which may include various hybrid or uploaded beings.</span><span>&nbsp;We believe the U.S. public is accurately detecting the presence of potential long-term catastrophic risks (e.g., </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/simulations-and-catastrophic-risks">simulations and suffering</a></span><span>) and four </span><span class="c6"><a class="c5" href="https://arxiv.org/abs/2306.12001">near-term catastrophic risks</a></span><span>&nbsp;that AI and ML safety researchers have recently classified: malicious use, AI race, organizational risks, and rog</span><span>ue AI.</span><span>&nbsp;</span><span>Current frontier AI models pose these not mutually exclusive risks and </span><span>already enact harms across the first three</span><span>. </span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_5tlpo991er2l-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">Malicious use: </span><span>Future intentional malicious uses, for example by </span><span class="c6"><a class="c5" href="https://centerforreducingsuffering.org/research/risk-factors-for-s-risks/#Malevolent_actors">malevolent actor</a></span><span class="c6"><a class="c5" href="https://centerforreducingsuffering.org/research/risk-factors-for-s-risks/#Malevolent_actors">s</a></span><span>,</span><span>&nbsp;including </span><span>governments, terrorists, and profit-seeking corporations, are an ongoing and long-term risk. Even seemingly neutral uses of LLMs such as </span><span class="c6"><a class="c5" href="https://www.cbc.ca/news/world/india-religious-chatbots-1.6896628">Gita chatbots</a></span><span>&nbsp;that interpret ancient religious texts can incidentally spread </span><span>misinformation and</span><span>&nbsp;provoke violence. </span></li></ul><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_yae2kowvr0dh-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">AI race: </span><span>The </span><span>AI arms race</span><span>&nbsp;to produce and release increasingly capable and powerful LLMs and other advanced AI systems may directly lead to an ignorance of public and expert concerns. The corporations and governments engaged in the AI arms race seem at risk of overlooking the negative externalities of AI development that are passed on to the public. </span><span>For example, </span><span class="c6"><a class="c5" href="https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails">unbounded iterative AI development strategies</a></span><span>&nbsp;threaten</span><span>&nbsp;public harms from interactions with AI systems that haven&rsquo;t been properly vetted. </span><span>The </span><span>public is concerned about this dynamic. </span><span>More people distrust than trust the companies building LLMs. Only 23% of respondents in the AIMS supplemental survey trusted LLM creators to put safety over profits, and only 27% trusted AI creators to control all current and future versions of an AI. The public has additional reason to worry about this dynamic because corporations and governments have shifted harms onto them in other contexts, particularly </span><span class="c6"><a class="c5" href="https://www.brookings.edu/articles/paying-the-cost-of-climate-change/">climate change</a></span><span class="c2">.</span></li></ul><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_dqs79yjahyvr-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">Organizational risks: </span><span>Another category of risks that frontier AI systems pose are organizational risks. These risks stem from unsafe or unsecure organizational cultures, environments, or power structures.</span><span class="c10">&nbsp;</span><span>O</span><span>rganizational risks include outcomes such as catastrophic accidents that destroy the organization, sensitive information about the AI or AI safety tests being leaked to malicious actors, and a lack of any safety testing due to a lack of concern for AI safety.</span><span>&nbsp;Hendrycks and colleagues (2023)</span><span>&nbsp;highlighted that (</span><span>1) catastrophes are hard to avoid even when competitive pressures are low, (2) AI accidents could be catastrophic, (3) it often takes years to discover severe flaws or risks, and (4) safety washing can undermine genuine efforts to improve AI safety.</span><span class="c2">&nbsp;</span></li></ul><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_culzjrlk1fu5-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">Rogue AI: </span><span>Rogue AI risks occur when a misaligned or unaligned AI model has significant </span><span>autonomy</span><span>&nbsp;to make decisions and act with little human oversight. In this context, advanced AIs could deliberately alter them</span><span>se</span><span>lves to take control over their own goals, act on their initial misalignment in an unexpected way, engage in deception, or exhibit goal drift as a result of adapting to changes in their environments.</span><span>&nbsp;Rogue </span><span>AIs may lack inner alignment or outer alignment. That is, AI systems can be misaligned both with their instructions and with human values.</span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>In addition to concern about AI risks to humans, the U.S. public showed surprisingly high concern for the welfare of AIs </span><span>in the AIMS supplemental survey</span><span>. The majority of Americans supported campaigns against the exploitation of AIs and agreed that we must not cause unnecessary suffering for LLMs with the capacity to suffer. Forty-three percent of Americans supported welfare standards that protect the wellbeing of robots or AIs. This suggests that the public is concerned with ensuring a broadly good future, in which humans survive and thrive, and in which the welfare of AIs is taken seriously. Critically, the U.S. public perceives some moral obligations towards AIs already, supporting recent </span><span class="c6"><a class="c5" href="https://www.sciencedirect.com/science/article/abs/pii/S0747563222001947">empirical</a></span><span>&nbsp;</span><span>and </span><span class="c6"><a class="c5" href="https://link.springer.com/article/10.1007/s43681-023-00260-1">theoretical</a></span><span>&nbsp;</span><span class="c2">perspectives on the extension of moral consideration to at least some AIs.</span></p><h1 class="c3" id="h.nttnyqgnggst"><span>4. Specific policy tools designed for increasing AI safety are supported by a majority of the U.S. public. </span></h1><p class="c1"><span class="c0">Key AIMS Findings: 71% support public campaigns to slow AI development. </span><span class="c0">71%</span><span class="c0">&nbsp;support government regulation that slows AI development</span><span class="c0">; when asked instead about opposition, 37% oppose it.</span><span class="c7 c0">&nbsp;63% support banning the development of artificial general intelligence that is smarter than humans. 64% support a global ban on data centers that are large enough to train AI systems that are smarter than humans. 69% support a six-month pause on AI development. </span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span class="c2">AI experts and the U.S. public are concerned about the pace of AI development and the possibility of catastrophic outcomes for humans and AIs alike. Nevertheless, AI experts, prediction markets, and the public expect even more powerful and human-like AI systems to be developed in the near future. Given this, what, if anything, should be done about it?</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>We need a toolkit of strategies to address the complex interests involved in the promotion and development of safe AI. Early in 2023, FLI prominently called for </span><span class="c6"><a class="c5" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">pausing</a></span><span>&nbsp;the development of training frontier models significantly more capable than current models. This letter asked AI labs to implement a 6-month voluntary pause on large-scale experiments until we better understand current frontier model behaviors. </span><span>In the days following its publication, </span><span class="c6"><a class="c5" href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">Y</a></span><span class="c6"><a class="c5" href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">udkowsky (2023)</a></span><span>&nbsp;argued that the pause letter understates the problem</span><span>, &ldquo;If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.&rdquo; </span><span>Yudkowsky further suggested treating the development of AI like the development of nuclear weapons, a call echoed by </span><span class="c6"><a class="c5" href="https://www.safe.ai/statement-on-ai-risk">CAIS</a></span><span>. Even before the now-famous &ldquo;pause letter</span><span>,</span><span>&rdquo; policy experts were calling for </span><span>bans on harmful applications of AI, compensation for those harmed by AI, licensing regimes to limit the spread of harmful AIs, and strict liability of AI companies for the harms caused by their products.</span><span>&nbsp;These calls have intensified. Acting on them both through research and legislation is necessary, although legislation </span><span class="c6"><a class="c5" href="https://www.technologyreview.com/2014/04/15/172377/laws-and-ethics-cant-keep-pace-with-technology/">tends to lag</a></span><span class="c2">&nbsp;behind actual development.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>The AIMS supplemental survey data and other survey datasets suggest that the U.S. public is broadly supportive of a variety of safety policies that protect humanity from the development of powerful, advanced, and potentially uncontrollable frontier AI models. More than two-thirds of the U.S. public supported pausing AI development (69%) and public campaigns advocating slowdowns (71%). </span><span>This level of support for regulation is even more striking given that the same AIMS data showed the least amount of</span><span>&nbsp;trust placed i</span><span>n the governmental part of the AI ecosystem when compared to</span><span class="c2">&nbsp;companies, engineers, AI output, training data, and algorithms.</span></p><p class="c1"><span>The U.S. public supports outright bans on the development of some technologies. More than half of respondents (63%) supported banning the development of AGI that is smarter than humans. More than two-thirds (69%) supported banning the development of sentient AIs and 64% supported a ban on data centers that are large enough to train AI systems that are smarter than humans. </span><span>Banning the development of a particular technology is typically considered a punitive and blunt tool</span><span class="c2">, but it&rsquo;s a tool that the U.S. public is willing to consider when it comes to the development of advanced AI technologies and a relatively simple proposal on which to elicit public opinion. This suggests that the public may be more open to disruptive interventions than commonly assumed.</span></p><h1 class="c3" id="h.aw9ghsqau2ut"><span>5. </span><span>The U.S. public is generally opposed to the development of AI sentience and </span><span>h</span><span>uman-AI integration.</span></h1><p class="c1"><span class="c0">Key AIMS Findings: 70% support a global ban on the development of sentience in robots/AIs. 71</span><span class="c0">%</span><span class="c0">&nbsp;support a global ban on the development of AI-enhanced humans. 72</span><span class="c0">%</span><span class="c0">&nbsp;support a global ban on the development of robot-human hybrids. </span><span class="c0">59%</span><span class="c0">&nbsp;oppose uploading human minds to computers (i.e., &ldquo;</span><span class="c0">mind uploading</span><span class="c0">&rdquo;)</span><span class="c7 c0">.</span></p><p class="c1 c4"><span class="c7 c0"></span></p><p class="c1"><span>As more complex, capable, and intelligent AIs emerge, some experts anticipate radical developments in: (1) de novo artificial sentience, and (2) a closer coupling with humans, leading to AI-enhanced humans or cyborgs (e.g., robot-human hybrids). </span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Artificial sentience comes with a host of conceptual and terminological difficulties. The Sentience Institute has begun examining some of these challenges: </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/assessing-sentience-in-artificial-entities">Ladak (2021)</a></span><span>&nbsp;evaluated potential methods to evaluate sentience in AI systems</span><span>, and </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/artificial-sentience-terminology">Pauketat (2021)</a></span><span>&nbsp;considered the terminology of artificial sentience</span><span>, </span><span>providing definitions for &ldquo;artificial sentience&rdquo;</span><span class="c0">&nbsp;</span><span>and referencing </span><span class="c6"><a class="c5" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00060/full">Graziano&rsquo;s (2017)</a></span><span>&nbsp;and </span><span class="c6"><a class="c5" href="https://www.scirp.org/(S(czeh2tfqw2orz553k1w0r45))/reference/referencespapers.aspx?referenceid%3D2216579">Reggia&rsquo;s (2013)</a></span><span class="c2">&nbsp;definitions of &ldquo;artificial consciousness&rdquo;: </span></p><p class="c1 c4"><span class="c2"></span></p><ul class="c14 lst-kix_jpq56qe5ir1e-0 start"><li class="c1 c21 li-bullet-0"><span class="c10">Artificial sentience</span><span class="c2">: </span></li></ul><ul class="c14 lst-kix_jpq56qe5ir1e-1 start"><li class="c1 c12 li-bullet-0"><span class="c2">&ldquo;artificial entities with the capacity for positive and negative experiences, such as happiness and suffering&rdquo;</span></li><li class="c1 c12 li-bullet-0"><span class="c2">&ldquo;the capacity for positive and negative experiences manifested in artificial entities&rdquo; </span></li></ul><p class="c1 c4 c24"><span class="c2"></span></p><ul class="c14 lst-kix_jpq56qe5ir1e-0"><li class="c1 c21 li-bullet-0"><span class="c11 c10">Artificial consciousness:</span></li></ul><ul class="c14 lst-kix_jpq56qe5ir1e-1 start"><li class="c1 c12 li-bullet-0"><span>&ldquo;a machine that contains a rich internal model of what consciousness is, attributes that property of consciousness to itself and to the people it interacts with, and uses that attribution to make predictions about human behavior. Such a machine would &lsquo;believe&rsquo; it is conscious and act like it is conscious, in the same sense that the human &ldquo;machine&rdquo; believes and acts&rdquo;</span><span class="c2">&nbsp;</span></li><li class="c1 c12 li-bullet-0"><span class="c2">&ldquo;computational models of various aspects of the conscious mind, either with software on computers or in physical robotic devices&rdquo; </span></li></ul><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span class="c6"><a class="c5" href="https://www.lesswrong.com/posts/e6gg8S8AfJmwE8HLh/machine-evolution#Technoselection">Bullock and colleagues (2023)</a></span><span>&nbsp;recently</span><span>&nbsp;explored the consequences of machines evolving greater cognitive capabilities, whether or not machine consciousness would arise, and, if so, whether this consciousness would resemble human consciousness. This remains a challenging and unanswered question. Regardless of the conceptual challenges associated with &ldquo;sentience&rdquo; and &ldquo;consciousness,&rdquo;</span><span>&nbsp;the U.S. public is opposed to the development of artificial sentience in this context. A majority of those surveyed in AIMS (70%) supported a global ban on the development of sentience in robots or AIs.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Leading AI and consciousness experts are also concerned about the creation of sentient AI. An </span><span class="c6"><a class="c5" href="https://amcs-community.org/open-letters/">open letter</a></span><span>&nbsp;by the Association for Mathematical Consciousness Science in 2023 called for the inclusion of consciousness research in the development of AI to address ethical, legal, and political concerns. Philosopher Thomas Metzinger has </span><span class="c6"><a class="c5" href="https://philpapers.org/archive/METASA-4.pdf">called for a moratorium</a></span><span class="c2">&nbsp;until the year 2050 on the development of &ldquo;synthetic phenomenology&rdquo; based on the risk of creating new forms of suffering.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>AI-enhanced humans and robot-human hybrids are likely to span a continuum. On one end are humans who are not integrated with any AI or robotic components. On the other end are the disembodied AIs that are not integrated with any human components or directly physical capabilities. </span><span>This continuum could include current cases such as humans who use AI-powered assistive devices to hear or move (e.g., </span><span class="c6"><a class="c5" href="https://www.soundly.com/blog/best-ai-hearing-aids">AI-powered hearing aids</a></span><span>, </span><span class="c6"><a class="c5" href="https://livingwithamplitude.com/artificial-intelligence-prosthetic-technology/">AI-powered prosthetic limbs</a></span><span>). Science fiction is filled with examples of AI-enhanced humans and robot-human hybrids (e.g., </span><span class="c6 c0"><a class="c5" href="https://www.imdb.com/title/tt0093870/">Robocop</a></span><span>, </span><span class="c6 c0"><a class="c5" href="https://www.imdb.com/title/tt0407362/">Battlestar Galactica</a></span><span>) that exemplify physically integrated human-AI/robotic systems</span><span>. These systems could manifest in other ways, such as AI tools on smartphones to extend our limited cognitive capacities (e.g., extended memory capacity, increased processing speed for calculations) and the </span><span class="c6"><a class="c5" href="https://www.forbes.com/sites/borislavmarinov/2022/08/16/saber-brings-mass-adoption-of-military-exoskeletons-one-step-closer-to-reality/">robotic exoskeletons</a></span><span>&nbsp;designed for soldiers. </span><span>Similarly, </span><span class="c6"><a class="c5" href="https://researchfeatures.com/rethinking-consciousness/">mind uploads</a></span><span>, a possible future technology achieved through methods such as </span><span class="c6"><a class="c5" href="https://80000hours.org/problem-profiles/whole-brain-emulation/">whole brain emulation</a></span><span>,</span><span>&nbsp;might also be classified as human-AI hybrids or A</span><span>I-enhanced humans. We found that over half of Americans (59%) opposed mind uploading.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>Like with artificial sentience, there is no general consensus for the definitions of &ldquo;AI-enhanced humans&rdquo; and &ldquo;robot-human hybrids.&rdquo; It is unclear what the general public has in mind when they respond to questions asking about integration, enhancement, and hybridization. Without imposing any particular definition, the AIMS supplement survey showed that 71% of Americans support a global ban on the development of AI-enhanced humans and 72% support a global ban on the development of robot-human hybrids. Presumably,</span><span>&nbsp;the general public is thinking about the futuristic cases of integration and hybridization rather than AI-powered assistive devices like hearing aids or prosthetic limbs when they endorse bans on integrative technologies. We should </span><span>scrutinize</span><span>&nbsp;public support for bans on integrative technologies because this might reflect a generalized perceived risk of all advanced AI technologies. It might also arise from </span><span class="c6"><a class="c5" href="https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience">substratist</a></span><span>&nbsp;prejudice and fears over substrate-based contamination. More research is needed </span><span class="c2">before policy directives are made.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>People may have developed something of a precautionary principle towards advanced, and the advancement of, AI. People may be responding to threats to human distinctiveness and acting to preserve their perceived ownership over faculties like sentience that are viewed as distinctly, or innately, human. Threats to </span><span class="c6"><a class="c5" href="https://www.sciencedirect.com/science/article/abs/pii/S0022103123000215">human distinctiveness</a></span><span>&nbsp;or </span><span class="c6"><a class="c5" href="https://psycnet.apa.org/record/2017-07486-005">uniqueness</a></span><span>&nbsp;might also provoke fears that advanced AIs, sentient AIs, or human-AI hybrids will supersede </span><span class="c6"><a class="c5" href="https://www.frontiersin.org/articles/10.3389/fdata.2022.1017677/full">human control</a></span><span>&nbsp;and change </span><span class="c6"><a class="c5" href="https://link.springer.com/article/10.1007/s11023-021-09579-2">human autonomy</a></span><span>. In parallel to these threats, concern over advanced AI technologies may stem from a form of </span><span class="c6"><a class="c5" href="https://www.verywellmind.com/status-quo-bias-psychological-definition-4065385">status quo bias</a></span><span class="c2">&nbsp;in which humans prefer to maintain their current situation, regardless of whether change would be harmful or beneficial.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1"><span>In an AI landscape that is rapidly evolving, public opinion provides policymakers with an important anchor point. The AIMS survey offers insights into the U.S. public&rsquo;s perspective at this critical juncture for AI governance. Policymakers need to act decisively, but thoughtfully, to implement policies that address the public&#39;s pressing concerns regarding advanced AI. Policymakers need also be aware that public opinion shapes policymaking and the acceptance of those policies. If policies appear too lenient or </span><span>extreme</span><span class="c2">&nbsp;compared to public opinion, backlash could occur that harms progress towards effective governance and building a harmonious human-AI future. Policymakers need to strike a balance between assertive risk mitigation and maintaining public trust through transparency, democratic process, and reasonable discourse.</span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1 c4"><span class="c2"></span></p><p class="c1 c4"><span class="c2"></span></p><hr class="c28"><div><p class="c9"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c23">&nbsp;We use &ldquo;sentience&rdquo; here to avoid much of the baggage that accompanies &ldquo;consciousness&rdquo; and to indicate our understanding of this concept as the capacity to have experiences such as pain and pleasure. </span></p></div></body></html>

</div>




    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
              © 2017–2024 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
