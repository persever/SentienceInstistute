<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="author" content="Sentience Institute" />

    <meta property="og:site_name" content="Sentience Institute" />
    <meta property="fb:app_id" content="302735083502826" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@sentienceinst" />

    
    

    
    <meta property="description" content="When people in the effective altruism (EA) community have worked to affect the far future, they’ve typically focused on reducing extinction risk, especially risks associated with superintelligence or general artificial intelligence (AI). I agree with the arguments for the far future being extremely important in our EA decisions, but I tentatively favor improving the quality of the far future by expanding humanity’s moral circle more than increasing the likelihood of the far future or humanity’s continued existence by reducing AI-based extinction risk" />
    <meta property="og:description" content="When people in the effective altruism (EA) community have worked to affect the far future, they’ve typically focused on reducing extinction risk, especially risks associated with superintelligence or general artificial intelligence (AI). I agree with the arguments for the far future being extremely important in our EA decisions, but I tentatively favor improving the quality of the far future by expanding humanity’s moral circle more than increasing the likelihood of the far future or humanity’s continued existence by reducing AI-based extinction risk" />
    
    
    <title>Sentience Institute | Why I Prioritize Moral Circle Expansion Over Reducing Extinction Risk Through Artificial Intelligence Alignment</title>
    <meta property="title" content="Why I Prioritize Moral Circle Expansion Over Reducing Extinction Risk Through Artificial Intelligence Alignment" />
    <meta property="og:title" content="Why I Prioritize Moral Circle Expansion Over Reducing Extinction Risk Through Artificial Intelligence Alignment" />
    
    
    
    
    <meta property="og:url" content="http://www.sentienceinstitute.org/blog/moral-circle-expansion-vs-reducing-extinction-risk" />
    <meta property="og:image" content="http://www.sentienceinstitute.org/img/blog/180220.png" />
    <meta name="twitter:image" content="http://www.sentienceinstitute.org/img/blog/180220.png" />
    


    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?v=1">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="../css/sentienceinstitute.css?v=2.0.1" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-toggleable-sm fixed-top">
      <div class="container">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarText" aria-controls="navbarText" aria-expanded="false">
          <span class="navbar-toggler-icon"></span>
        </button>
        <a class="navbar-brand" href="/">
          <!-- <img class="nav-logo" src="../img/logo/SI_logo_white_200px.png"/> -->
          <img class="nav-logo-brandmark" src="../img/logo/SI_brandmark_white_heavier_web.png"/>
          <div class="nav-logo-text">
            <span>Sentience</span>
            <span class="nav-logo-text-institute">Institute</span>
          </div>
        </a>
        <div id="navbarText" class="collapse navbar-collapse">
          <ul class="navbar-nav">
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Research<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <li><a href="/research-agenda">Agenda</a></li>
                <li><a href="/reports">Reports</a></li>
                <li><a href="/aims-survey">Artificial Intelligence, Morality, and Sentience (AIMS) Survey</a></li>
                <li><a href="/aft-survey">Animals, Food, and Technology (AFT) Survey</a></li>
                <!-- <li><a href="/blog">Blog</a></li> -->
                <!-- <li><a href="/press">Press Releases</a></li> -->
              </ul>
            </li>
            <!-- <li class="nav-item"><a class="nav-link" destination="/media">Media</a></li> -->
            <li class="nav-item"><a class="nav-link" destination="/podcast">Podcast</a></li>
            <li class="nav-item"><a class="nav-link" destination="/blog">Blog</a></li>
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                About Us<span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
                <!-- <li><a href="/mission">Our Mission</a></li> -->
                <li><a href="/perspective">Our Perspective</a></li>
                <li><a href="/team">Our Team</a></li>
                <li class="nav-item"><a class="nav-link" href="/get-involved">Get Involved</a></li>
                <li><a href="/transparency">Transparency</a></li>
                <!-- <li><a href="/faq">FAQ</a></li> -->
              </ul>
            </li>
            <li class="nav-item nav-donate"><a class="nav-link" destination="/donate">Donate</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    




<div class="container image-container" img-id="180220" style="background-image: url(/img/blog/180220.png);">
  
  
  

<div data-nosnippet class="container image-info-container">
  <div class="image-credit">
    <i class="material-icons photo-icon">photo_camera</i>
    <span>
      Jo-Anne McArthur / We Animals
    </span>
  </div>
  <div class="image-title">
    <div class="image-title-text">
      Shai, a young gorilla at Ape Action Africa, a sanctuary for primates in Cameroon
    </div>
    <div class="arrow-down"></div>
  </div>
</div>


  
</div>

<div class="container first-container gdoc-html-container blog-container moral-circle-expansion-vs-reducing-extinction-risk-container">
  <div class="title">
    Why I Prioritize Moral Circle Expansion Over Reducing Extinction Risk Through Artificial Intelligence Alignment
  </div>
  <div class="author-info">
    
    <div class="author">
      <div class="author-img"><img src="../img/team/jacy.png"/></div>
      <div class="author-name-and-role">
      <div class="author-name">Jacy Reese Anthis<a class="author-twitter" href="https://twitter.com/jacyanthis"><img src="../img/icons/icon_twitter_blue.png"/></a></div>
      <div class="author-role">Co-Founder</div>
    </div>
  </div>
  
  </div>
  <div class="date">
    February 20, 2018
  </div>
  
<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"></head><body class="c15 doc-content"><p class="c1"><span class="c16 c12 c17">Many thanks for helpful feedback to Jo Anderson, Tobias Baumann, Jesse Clifton, Max Daniel, Michael Dickens, Persis Eskander, Daniel Filan, Kieran Greig, Zach Groff, Amy Halpern-Laff, Jamie Harris, Josh Jacobson, Gregory Lewis, Caspar Oesterheld, Carl Shulman, Brian Tomasik, Johannes Treutlein, Magnus Vinding, Ben West, and Kelly Witwicki. I also forwarded Ben Todd and Rob Wiblin a small section of the draft that discusses an 80,000 Hours article.</span></p><p class="c1 c3"><span class="c0"></span></p><a id="t.b3d126b915b2f612bb237acd045a9bd4915f2f5f"></a><a id="t.0"></a><table class="c27"><tr class="c29"><td class="c20" colspan="1" rowspan="1"><p class="c28 c3"><span class="c0"></span></p><p class="c24"><span class="c4"><a class="c11" href="#h.cj6l72be5mkh">Abstract</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.v03dkms2q7zz">Context</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.semrhplbxj0e">The expected value of the far future</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.gg02jhdm67jl">What if it&rsquo;s close to zero?</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.8z3xjhu8emce">Key considerations</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.304pdwlngjdb">Scale</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.x8nbywncd6vk">Range of outcomes</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.m2q6yfrdrfgq">Likelihood of different far future scenarios</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.fn649izfj2dw">Tractability</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.gjmbi8ndxsjc">Social change versus technical research</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.w7yig7otkfud">Track record</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.q1zibr1u7nw5">Robustness</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.m09slze9huhe">Miscellaneous</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.gv4nosomzue0">Neglectedness</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.7abcgzfuy616">Cooperation</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.9wdylb58crq9">Cooperation with future do-gooders</a></span></p><p class="c22"><span class="c4"><a class="c11" href="#h.flt9dsg85g0s">Bias</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.gii9jrhyrggp">One might be biased towards AIA if...</a></span></p><p class="c23"><span class="c4"><a class="c11" href="#h.igu98gyfpwtx">One might be biased towards MCE if...</a></span></p><p class="c30"><span class="c4"><a class="c11" href="#h.ib668zv9209x">Implications</a></span></p><p class="c3 c28"><span class="c0"></span></p></td></tr></table><h1 class="c13" id="h.cj6l72be5mkh"><span>A</span><span class="c17 c19">bstract</span></h1><p class="c1"><span>When people in the effective altruism (EA) community have worked to affect the far future, they&rsquo;ve typically focused on reducing extinction risk, especially risks associated with superintelligence or general artificial intelligence (AI). I agree with the arguments for the far future being extremely important in our EA decisions, but I tentatively favor improving the quality of the far future by expanding humanity&rsquo;s moral circle more than increasing the likelihood of the far future or humanity&rsquo;s continued existence by reducing AI-based extinction risk because: (1) the far future seems to not be very good in expectation, and there&rsquo;s a significant likelihood of it being very bad, and (2) </span><span>moral circle expansion</span><span class="c0">&nbsp;seems highly neglected both in EA and in society at large. Also, I think considerations of bias are very important here, given how necessarily intuitive and subjective judgment calls make up the bulk of differences in opinion on far future cause prioritization. Finally, I think the scales of MCE and AI alignment are similar, and I find the argument in favor of AI alignment that technical research might be more tractable than social change to be the most compelling counterargument to my position.</span></p><h1 class="c13" id="h.v03dkms2q7zz"><span>Context</span></h1><p class="c1"><span>This post largely aggregates existing content on the topic, rather than making original arguments. I offer my views, mostly intuitions, on the various arguments, but of course I remain highly uncertain given the limited amount of empirical evidence we have on far future </span><span class="c4"><a class="c11" href="https://causeprioritization.org/">cause prioritization</a></span><span class="c0">.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c4"><a class="c11" href="http://effective-altruism.com/ea/1fi/have_ea_priorities_changed_over_time/">Many</a></span><span>&nbsp;in the effective altruism (EA) community think the </span><span class="c4"><a class="c11" href="https://ea-foundation.org/blog/the-importance-of-the-far-future/">far future</a></span><span>&nbsp;is a very important consideration when working to do the most good. The basic argument is that humanity could continue to exist for a very long time and could expand its civilization to the stars, creating a very large amount of moral value. The main narrative has been that this civilization could be a very good one, and that in the coming decades, we face sizable risks of extinctions that could prevent us from obtaining this </span><span class="c4"><a class="c11" href="http://www.existential-risk.org/concept.html">&ldquo;cosmic endowment.&rdquo;</a></span><span class="c0">&nbsp;The argument goes that these risks also seem like they can be reduced with a fairly small amount of additional resources (e.g. time, money), and therefore extinction risk reduction is one of the most important projects of humanity and the EA community.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>(This argument also depends on a moral view that bringing about the existence of sentient beings can be a morally good and important action, comparable to helping sentient beings who currently exist live better lives. This is a contentious view in academic philosophy. See, for example, </span><span class="c4"><a class="c11" href="https://dash.harvard.edu/bitstream/handle/1/13064981/Frick_gsas.harvard.inactive_0084L_11842.pdf">&ldquo;&#39;Making People Happy, Not Making Happy People&#39;: A Defense of</a></span><span><a class="c11" href="https://dash.harvard.edu/bitstream/handle/1/13064981/Frick_gsas.harvard.inactive_0084L_11842.pdf">&nbsp;</a></span><span class="c4"><a class="c11" href="https://dash.harvard.edu/bitstream/handle/1/13064981/Frick_gsas.harvard.inactive_0084L_11842.pdf">the Asymmetry Intuition in Population Ethics.&rdquo;</a></span><span>)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>However, one can accept the first part of this argument &mdash; that there is a very large amount of expected moral value in the far future and it&rsquo;s relatively easy to make a difference in that value &mdash; </span><span class="c4"><a class="c11" href="http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/">without deciding</a></span><span>&nbsp;that extinction risk is the most important project. In </span><span class="c4"><a class="c11" href="http://effective-altruism.com/ea/t3/some_considerations_for_different_ways_to_reduce/">slightly different terms</a></span><span>, one can decide not to work on reducing </span><span class="c18">population risks</span><span>, risks that could reduce the number of morally relevant individuals in the far future (of course, these are only risks of harm if one believes more individuals is a good thing), and instead</span><span>&nbsp;work on reducing </span><span class="c18">quality risks</span><span>, risks that could reduce the quality of morally relevant individuals&rsquo; existence. One specific type of quality risk often discussed is a </span><span class="c4"><a class="c11" href="https://foundational-research.org/risks-of-astronomical-future-suffering/">risk of astronomical suffering</a></span><span>&nbsp;(</span><span class="c18">s-risk</span><span>)</span><span>, defined as &ldquo;events that would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.&rdquo;<br><br>This blog post makes the case for focusing on quality risks over population risks. More specifically, though also more tentatively, it makes the case for focusing on reducing quality risk through </span><span class="c4 c18"><a class="c11" href="http://effective-altruism.com/ea/1b1/introducing_sentience_institute/">moral circle expansion</a></span><span>&nbsp;(MCE), the strategy of impacting the far future through increasing humanity&rsquo;s concern for sentient beings who currently receive little consideration (i.e. widening our moral circle so it includes them), over </span><span class="c4 c18"><a class="c11" href="https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/">AI alignment</a></span><span>&nbsp;(AIA), the strategy of impacting the far future through increasing the likelihood that humanity creates an artificial general intelligence (AGI) that behaves as its designers want it to (known as the alignment problem).</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>The basic case for MCE is very similar to the case for AIA. Humanity could continue to exist for a very long time and could expand its civilization to the stars, creating a very large number of sentient beings. The sort of civilization we create, however, seems highly dependent on our moral values and moral behavior. In particular, it&rsquo;s uncertain whether many of those sentient beings will receive the moral consideration they deserve based on their sentience, i.e. whether they will be in our </span><span class="c4"><a class="c11" href="https://press.princeton.edu/titles/9434.html">&ldquo;moral circle&rdquo;</a></span><span>&nbsp;</span><span>or not, like the</span><span>&nbsp;many sentient beings</span><span>&nbsp;who</span><span>&nbsp;have suffered intensely </span><span>over the course of</span><span>&nbsp;human history (e.g.</span><span>&nbsp;from</span><span class="c0">&nbsp;torture, genocide, oppression, war). It seems the moral circle can be expanded with a fairly small amount of additional resources (e.g. time, money), and therefore MCE is one of the most important projects of humanity and the EA community.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Note that MCE is a specific kind of </span><span class="c4 c18"><a class="c11" href="http://reducing-suffering.org/values-spreading-often-important-extinction-risk/">values spreading</a></span><span>, the parent category of MCE that describes any effort to shift the values and moral behavior of humanity and its decendants (e.g. intelligent machines) in a positive direction</span><span>&nbsp;to benefit the far future</span><span class="c0">. (Of course, some people attempt to spread values in order to benefit the near future, but in this post we&rsquo;re only considering far future impact.)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I&rsquo;m specifically comparing MCE and AIA because AIA is probably the most favored method of reducing extinction risk</span><span>&nbsp;in the EA community</span><span class="c0">. AIA seems to be the default cause area to favor if one wants to have an impact on the far future, and I&rsquo;ve been asked several times why I favor MCE instead.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>This discussion risks conflating AIA with reducing extinction risk. These are two separate ideas, since an unaligned AGI could still lead to a large number of sentient beings, and an aligned AGI </span><span>could</span><span>&nbsp;still potentially cause extinction or population stagnation (e.g. if according to the designers&rsquo; values, even the best civilization the AGI could help build is still worse than nonexistence). However, most EAs focused on AIA seem to believe that the main risk is something quite like extinction, such as the textbook example of an AI that seeks to maximize the number of paperclips in the universe. I&rsquo;ll note when the distinction between AIA and reducing extinction risk is relevant. Similarly, there are sometimes important prioritization differences between MCE and other types of values spreading, and those will be noted when they matter.</span><span>&nbsp;(This paragraph is an important qualification for the whole post. The possibility of unaligned AGI that involves a civilization (and, less so because it seems quite unlikely, the possibility of an AGI that causes extinction) is important to consider for far future cause prioritization. Unfortunately, elaborating on this would make this post far more complicated and far less readable, and would not change many of the conclusions. Perhaps I&rsquo;ll be able to make a second post that adds this discussion at some point.)</span><span><br><br>It&rsquo;s also important to note that I&rsquo;m discussing specifically AIA here, not all AI safety work</span><span>&nbsp;in general</span><span class="c0">. AI safety, which just means increasing the likelihood of beneficial AI outcomes, could be interpreted as including MCE, since MCE plausibly makes it more likely that an AI would be built with good values. However, MCE doesn&rsquo;t seem like a very plausible route to increasing the likelihood that AI is simply aligned with the intentions of its designers, so I think MCE and AIA are fairly distinct cause areas.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c0">AI safety can also include work on reducing s-risks, such as specifically reducing the likelihood of an unaligned AI that causes astronomical suffering, rather than reducing the likelihood of all unaligned AI. I think this is an interesting cause area, though I am unsure about its tractability and am not considering it in the scope of this blog post.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>The post&rsquo;s publication was supported by Greg Lewis, who was interested in this topic and donated $1,000 to </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/">Sentience Institute</a></span><span>, the think tank I co-founded which researches effective strategies to expand humanity&rsquo;s moral circle, conditional on this post being published to the Effective Altruism Forum. Lewis doesn&rsquo;t necessarily agree with any of its content. He decided on the conditional donation prior to the post being written, and </span><span>I</span><span class="c0">&nbsp;did ask him to review the post prior to publication and it was edited based on his feedback.</span></p><h1 class="c13" id="h.semrhplbxj0e"><span>T</span><span>he expected</span><span>&nbsp;value of the far future</span></h1><p class="c1"><span>Whether we prioritize</span><span>&nbsp;reducing extinction risk partly depends on how good or bad </span><span>we expect </span><span>human civilization </span><span>to</span><span class="c0">&nbsp;be in the far future, given it continues to exist. In my opinion, the assumption that it will be very good is a tragically unexamined assumption in the EA community.</span></p><h2 class="c7" id="h.gg02jhdm67jl"><span>What if it&rsquo;s close to zero?</span></h2><p class="c1"><span>If we think the far future is very good, that clearly makes reducing extinction risk more promising. And if we think the far future is very bad, that makes reducing extinction risk not just unpromising, but actively very harmful. But what if it&rsquo;s near the middle, i.e. close to zero?</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span>&nbsp;80,000 Hours </span><span class="c4"><a class="c11" href="https://80000hours.org/articles/extinction-risk/#who-shouldnt-prioritise-safeguarding-the-future">wrote</a></span><span class="c0">&nbsp;that to believe reducing extinction risk is not an EA priority on the basis of the expected moral value of the far future,</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1 c21"><span>...even if you&rsquo;re not sure how good the future will be, or suspect it will be bad, you may want civilisation to survive and keep its options open. People in the future will have much more time to study whether it&rsquo;s desirable for civilisation to expand, stay the same size, or shrink. If you think there&rsquo;s a good chance we will be able to act on those moral concerns, that&rsquo;s a good reason to leave any final decisions to the wisdom of future generations. Overall, we&rsquo;re highly uncertain about these big-picture questions, but that generally makes us more concerned to avoid making any irreversible commitments.</span><span>.</span><span>.</span></p><p class="c1 c3 c21"><span class="c0"></span></p><p class="c1"><span>This reasoning seems mistaken to me because wanting &ldquo;civilisation to survive and keep its options open&rdquo; depends on optimism that civilization will do research and make good</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span>&nbsp;decisions based on that research.</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span>&nbsp;</span><span>In other words, while preventing extinction keeps options open </span><span>for</span><span>&nbsp;good things</span><span>&nbsp;to happen</span><span>, it also keeps options open </span><span>for</span><span>&nbsp;bad things</span><span>&nbsp;to happen</span><span>, and desiring this option value depends on an optimism that the good things are more likely. In other words, t</span><span>he reasoning assumes the optimism</span><span>&nbsp;(thinking the far future is good</span><span>, or at least that humans will make good decisions</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span>)</span><span class="c0">, which is also its conclusion.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Having that optimism makes sense in many decisions, which is why keeping options open is often a good heuristic. In EA, for example, people tend to do good things with their careers, which means career option value is a useful thing. This doesn&rsquo;t readily translate to </span><span>decisions where it&rsquo;s not clear whether the actors involved will have a positive or negative impact</span><span>.</span><span>&nbsp;(Note 80,000 Hours isn&rsquo;t making this comparison. I&rsquo;m just making it to explain my own view here.)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>There&rsquo;s also a sense in which preventing extinction risk decreases option value because if humanity progresses past certain civilizational milestones</span><span>&nbsp;that make extinction more unlikely</span><span>&nbsp;&mdash; </span><span>say, the rise of AGI or expansion beyond our own solar system</span><span>&nbsp;&mdash; </span><span>it might become harder or even impossible to press the &ldquo;off switch</span><span>&rdquo; (ending civilization). However, I think most would agree that there&rsquo;s more overall option value in a civilization that has gotten past these milestones </span><span>because there&rsquo;s a much wider variety of non-extinct civilizations than extinct civilizations</span><span>.</span><sup><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>If you think that the expected moral value</span><span>&nbsp;of the far future</span><span>&nbsp;is close to zero, even if you think it&rsquo;s slightly positive, </span><span>then</span><span>&nbsp;reducing extinction risk is a less promising EA strategy</span><span>&nbsp;than if you </span><span>think</span><span>&nbsp;it</span><span>&rsquo;s</span><span>&nbsp;very positive</span><span class="c0">.</span></p><h2 class="c7" id="h.8z3xjhu8emce"><span>K</span><span class="c10">ey considerations</span></h2><p class="c1"><span class="c0">I think the considerations on this topic are best represented as questions where people&rsquo;s beliefs (mostly just intuitions) vary on a long spectrum. I&rsquo;ll list these in order of where I would guess I have the strongest disagreement with people who believe the far future is highly positive in expected value (shortened as HPEV-EAs), and I&rsquo;ll note where I don&rsquo;t think I would disagree or might even have a more positive-leaning belief than the average such person.</span></p><p class="c1 c3"><span class="c0"></span></p><ol class="c26 lst-kix_4hxoobeo92dt-0 start" start="1"><li class="c1 c8 li-bullet-0"><span>I think there&rsquo;s a significant</span><sup><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span>&nbsp;chance that the moral circle will fail to expand to reach all sentient beings, such as artificial/small/weird minds (e.g. a sophisticated computer program used to mine asteroids, but one that doesn&rsquo;t have the normal features of sentient minds like facial expressions). In other words, I think there&rsquo;s a significant chance that powerful beings in the far future will have low </span><span class="c4"><a class="c11" href="https://en.wikipedia.org/wiki/Willingness_to_pay">willingness to pay</a></span><span>&nbsp;for the welfare of many of the small/weird minds in the future.</span><sup><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup></li><li class="c1 c8 li-bullet-0"><span>I think it&rsquo;s likely that the powerful beings in the far future (analogous to humans as the powerful beings on Earth in 201</span><span>8</span><span>) will use large numbers of less powerful </span><span>sentient </span><span>beings, such as for recreation (e.g. safaris, war games), a labor force (e.g. colonists to distant parts of the galaxy, construction workers), </span><span>scientific experiments, </span><span>threats, (e.g. threatening to create and torture beings that a rival cares about), revenge, </span><span>justice</span><span>, </span><span>religion</span><span>, or even pure sadism.</span><sup><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span>&nbsp;I believe this because there have been less powerful</span><span>&nbsp;sentient</span><span>&nbsp;beings for all of humanity&rsquo;s existence and well before (e.g. predation)</span><span>, many of whom are exploited and harmed by humans and other animals</span><span>, and there seems to be little reason to think such power dynamics won&rsquo;t continue to exist.<br><br>Alternative uses of resources include </span><span>simply working to increase one&rsquo;s own happiness directly</span><span>&nbsp;(e.g. changing one&rsquo;s neurophysiology to be extremely happy all the time), and constructing large non-sentient projects like a work of art. Though each of these types of project could still include sentient beings, such as for experimentation or a labor force.<br><br>With the exception of threats and sadism, the less powerful minds seem like they could suffer intensely because </span><span>their intense suffering</span><span>&nbsp;could be instrumentally useful. For example, if the recreation is nostalgic, or human psychology persists in some form, we could see powerful beings causing intense suffering in order to see good triumph over evil or in order to satisfy curiosity about situations that involve intense suffering (of course, the powerful beings might not acknowledge the suffering as suffering</span><span>, instead conceiving of it as simulated but not actually experienced by the simulated entities</span><span>). For another example, with a sentient labor force, punishment could be a stronger motivator than reward, as indicated by the history of evolution on Earth.</span><sup><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><sup><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup></li><li class="c1 c8 li-bullet-0"><span>I </span><span class="c0">place significant moral value on artificial/small/weird minds.</span></li><li class="c1 c8 li-bullet-0"><span>I think it&rsquo;s quite unlikely that human descendants will find the correct morality (in the sense of </span><span class="c4"><a class="c11" href="https://en.wikipedia.org/wiki/Moral_realism">moral realism</a></span><span>, finding these mind-independent moral facts), and I don&rsquo;t think I would care much about that correct morality even if it existed. For example, I don&rsquo;t think I would be compelled to create suffering if the correct morality said this is what I should do. Of course, such moral facts are very difficult to imagine, so I&rsquo;m quite uncertain about what my reaction to them would be.</span><sup><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m skeptical about the view that technology and efficiency </span><span>will</span><span>&nbsp;remove the need for powerless, high-suffering, instrumental moral patients. An example of this </span><span>predicted trend</span><span>&nbsp;is that</span><span>&nbsp;factory farmed animals seem unlikely to be necessary in the far future because of their </span><span class="c4"><a class="c11" href="http://awfw.org/feed-ratios/">inefficiency at producing animal products</a></span><span>.</span><span>&nbsp;</span><span>Therefore</span><span>, I&rsquo;m not particularly concerned about the factory farming of biological animals continuing into the far future. </span><span>I am, however, concerned </span><span>about similar</span><span>&nbsp;but</span><span>&nbsp;less </span><span>inefficient</span><span>&nbsp;systems.<br><br></span><span>An example of how </span><span>technology might not render sentient labor forces and other instrumental sentient beings obsolete</span><span>&nbsp;is how humans seem motivated </span><span>to have</span><span>&nbsp;power and control over the world, and in particular seem more satisfied by </span><span>having </span><span>power over other sentient beings than </span><span>by having </span><span>power over non-sentient things like barren landscapes.</span><span><br><br>I do still believe there&rsquo;s a strong tendency towards efficiency</span><span>&nbsp;and that this has the potential to render much suffering obsolete</span><span>; I just have more skepticism about it than I think is often assumed by HPEV-EAs.</span><sup><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m skeptical about the view that human descendants will optimize their resources for happiness (i.e. create hedonium) relative to optimizing for suffering (i.e. create dolorium).</span><sup><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span>&nbsp;Humans currently seem more deliberately driven to create hedonium, but creating dolorium might be more instrumentally useful </span><span>(e.g. as a threat to rivals</span><sup><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup><span>).</span><span><br><br>On this topic, I similarly do still believe there&rsquo;s a higher likelihood of creating hedonium; I just have more skepticism about it than I think is often assumed by EAs.</span></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m largely in agreement with the average HPEV-EA in my moral exchange rate between happiness and suffering. However, I think those EAs tend to greatly underestimate how much the empirical tendency towards suffering over happiness (e.g. wild animals seem to endure </span><span class="c4"><a class="c11" href="https://foundational-research.org/the-importance-of-wild-animal-suffering/">much more suffering than happiness</a></span><span>)</span><span>&nbsp;is evidence of a future empirical asymmetry.</span><span><br><br></span><span>My view here is partly informed by the capacities for happiness and suffering that have evolved in humans and other animals, the capacities that seem to be driven by cultural forces (e.g. </span><span>corporations seem to care more about downsides than upsides</span><span>, perhaps because it&rsquo;s easier in general to destroy and harm things than to create and grow them</span><span>), and speculation about what could be done in more advanced civilizations, such as my best guess on what a planet optimized for happiness and a planet optimized for suffering would look like. </span><span>For example, I think a given amount of dolorium/dystopia (say, the amount that can be created with 100 joules of energy) is far larger in absolute moral expected value than hedonium/utopia made with the same resources</span><span class="c0">.</span></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m unsure of how much I would disagree with HPEV-EAs about the argument that we should be highly uncertain about the likelihood of different far future scenarios because of how highly speculative our evidence is, which pushes my estimate of the expected value of the far future towards </span><span>the middle of the possible range, i.e. towards </span><span class="c0">zero.</span></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m unsure of how much I would disagree with HPEV-EAs about the persistence of evolutionary forces into the future (i.e. how much future beings will be determined by fitness, rather than characteristics we might hope for like altruism and happiness).</span><sup><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup></li><li class="c1 c8 li-bullet-0"><span>From the historical perspective, it worries me that many historical humans seem like they would be quite unhappy with the way human morality changed after them, such as the way Western countries are less concerned about previously-considered-immoral behavior like homosexuality and gluttony than their ancestors were in 500 CE.</span><span>&nbsp;(Of course, one might think historical humans would agree with modern humans upon reflection, or think that much of humanity&rsquo;s moral changes have been due to improved empirical understanding of the world.)</span><sup><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup></li><li class="c1 c8 li-bullet-0"><span>I&rsquo;m largely in agreement with HPEV-EAs that humanity&rsquo;s moral circle has a track record of expansion and seems likely to continue </span><span>expanding</span><span>. For example, I think it&rsquo;s quite likely that powerful beings in the far future </span><span>will care a lot about charismatic biological animals like elephants or chimpanzees, or whatever beings have a similar relationship to those powerful beings as humanity has to elephants and chimpanzees</span><span>. (As mentioned above, my </span><span>pessimism</span><span class="c0">&nbsp;about the continued expansion is largely due to concern about the magnitude of bad-but-unlikely outcomes and the harms that could occur due to MCE stagnation.)</span></li></ol><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Unfortunately, we don&rsquo;t have much empirical data or solid theoretical arguments on these topics, so the disagreements I&rsquo;ve had with HPEV-EAs have mostly just </span><span>come down to</span><span>&nbsp;differences in intuition. This is a common theme for prioritization among far future efforts. We can outline the relevant factors and a little empirical data, but the crucial factors seem to be </span><span>left to speculation and intuition</span><span class="c0">.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Most of these considerations are about how society will develop and utilize new technologies, which suggests we can develop relevant intuitions and speculative capacity by studying social </span><span>and technological change</span><span>. So even though the</span><span>se</span><span>&nbsp;judgments are intuitive, we could potentially improve them with more study of big-picture social and technological change, such as Sentience Institute&rsquo;s </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/research">research</a></span><span>&nbsp;of MCE or Robin Hanson&rsquo;s book on </span><span class="c4 c16"><a class="c11" href="https://en.wikipedia.org/wiki/The_Age_of_Em">The Age of Em</a></span><span>&nbsp;that analyzes what a future of brain emulations would look like.</span><span>&nbsp;(This sort of empirical research is what I see as the most promising future research avenue for far future cause prioritization. I worry EAs overemphasize armchair research (like most of this post, actually) for various reasons.</span><sup><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup><span>)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I&rsquo;d personally be quite interested </span><span>in a survey</span><span class="c0">&nbsp;of people with expertise in the relevant fields of social, technological, and philosophical research, in which they&rsquo;re asked about each of the considerations above, though it might be hard to get a decent sample size, and I think it would be quite difficult to debias the respondents (see the Bias section of this post).</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I&rsquo;m also interested in quantitative analys</span><span>e</span><span>s of these considerations</span><span>&nbsp;&mdash; </span><span>&nbsp;calculation</span><span>s</span><span>&nbsp;including all of these potential outcomes and associated likelihoods. As far as I know, this kind of</span><span>&nbsp;analysis</span><span>&nbsp;has only been attempted so far by Michael Dickens in </span><span class="c4"><a class="c11" href="http://effective-altruism.com/ea/xr/a_complete_quantitative_model_for_cause_selection/">&ldquo;A Complete Quantitative Model for Cause Selection,&rdquo;</a></span><span>&nbsp;in which Dickens notes that, &ldquo;Values spreading may be better than existential risk reduction.&rdquo;</span><span>&nbsp;While this quantification might seem hopelessly speculative, </span><span>I think it&rsquo;s </span><span class="c4"><a class="c11" href="https://en.wikipedia.org/wiki/Superforecasting">highly useful</a></span><span>&nbsp;even in such situations. </span><span class="c0">Of course, rigorous debiasing is also very important here.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Overall, I think the far future is close to zero in expected </span><span>moral </span><span>value</span><span>, meaning i</span><span class="c0">t&rsquo;s not nearly as good as is commonly assumed, implicitly or explicitly, in the EA community.</span></p><h1 class="c13" id="h.304pdwlngjdb"><span>Scale</span></h1><h2 class="c7" id="h.x8nbywncd6vk"><span class="c10">Range of outcomes</span></h2><p class="c1"><span>It&rsquo;s difficult to compare the scale of far future impacts since they are all astronomical,</span><span>&nbsp;and I find the consideration of scale here to overall not be very useful.<br><br>T</span><span>echnically, it seems like MCE involves a larger </span><span>range of potential outcomes</span><span>&nbsp;</span><span>than </span><span>reducing extinction risk through </span><span>AIA </span><span>because, at least from a classical consequentialist perspective (giving weight to both negative and positive outcomes), it could make the difference between some of the worst far futures imaginable and the best far futures. Reducing extinction risk through AIA only makes the difference between nonexistence</span><span>&nbsp;(</span><span>a far future of zero value</span><span>)</span><span>&nbsp;and whatever world comes to exist. If one believes the far future is highly positive, this could still be a very large </span><span>range</span><span>, </span><span class="c0">but it would still be less than the potential change from MCE.<br></span></p><p class="c1"><span>How much less depends on one&rsquo;s views of how bad the worst future is relative to the best future. If the absolute value is the same, then MCE has a </span><span>range</span><span>&nbsp;twice as large as extinction risk.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>A</span><span>s mentioned in the Context section above, the change in the far future that AIA could achieve might not exactly be extinction versus non-extinction. While an aligned AI would probably not involve the extinction of all sentient beings, since that would require the values of its creators to prefer extinction over all other options, an unaligned AI might not necessarily involve extinction. To use the canonical AIA example of a &ldquo;</span><span>paperclip</span><span>&nbsp;maximizer&rdquo; (used to illustrate how an AI could easily have a harmful goal without any malicious intention), the rogue AI might create sentient beings as a labor force to implement its goal of maximizing the number of paperclips in the universe, or create sentient beings for some other goal.</span><sup><a href="#ftnt20" id="ftnt_ref20">[20]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>This means that the </span><span>range</span><span>&nbsp;of AIA is the difference between the potential universes with aligned AI and unaligned AI, which could be very good futures contrasted with very bad futures, rather than just very good futures contrasted with nonexistence.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Brian Tomasik has written out a thoughtful (though necessarily speculative and highly uncertain) </span><span class="c4"><a class="c11" href="https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering#Would_a_human-inspired_AI_or_rogue_AI_cause_more_suffering">breakdown</a></span><span class="c0">&nbsp;of the risks of suffering in both aligned and unaligned AI scenarios, which weakly suggests that an aligned AI would lead to more suffering in expectation.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>All things considered, it seems that the </span><span>range</span><span>&nbsp;of </span><span>quality risk reduction (including </span><span>MCE) is larger than that of extinction risk reduction (including </span><span>AIA</span><span>, depending on one&rsquo;s view of what difference AI alignment makes), but this seems like a fairly weak consideration to me because </span><span>(i) it&rsquo;s a difference of roughly two-fold, which is quite small relative to the differences of ten-times, a thousand-times, etc. that we frequently see in cause prioritization</span><span>, (ii) there are numerous fairly arbitrary judgment calls (like considering reducing extinction risk from AI versus AIA versus AI safety) that lead to different results.</span><sup><a href="#ftnt21" id="ftnt_ref21">[21]</a></sup></p><h2 class="c7" id="h.m2q6yfrdrfgq"><span>Likelihood of different far future scenarios</span><sup><a href="#ftnt22" id="ftnt_ref22">[22]</a></sup><sup><a href="#ftnt23" id="ftnt_ref23">[23]</a></sup></h2><p class="c1"><span>MCE is relevant for many far future scenarios where AI doesn&rsquo;t undergo the sort of &ldquo;intelligence explosion&rdquo; or similar progression that makes AIA important; for example, </span><span>if AGI is developed by an institution like a foreign country that has little interest in AIA, or if AI is never developed, or if it&rsquo;s developed slowly in a way that makes safety adjustments quite easy as that development occurs</span><span class="c0">. In each of these scenarios, the way society treats sentient beings, especially those currently outside the moral circle, seems like it could still be affected by MCE. As mentioned earlier, I think there is a significant chance that the moral circle will fail to expand to reach all sentient beings, and I think a small moral circle could very easily lead to suboptimal or dystopian far future outcomes.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>On the other hand, some possible far future civilizations might not involve moral circles, such as </span><span>if there is an egalitarian society where each individual is able to fully represent their own interests in decision-making and this societal structure was not reached through MCE because these beings are all equally powerful for technological reasons (and no other beings exist and they have no interest in creating additional beings).</span><span>&nbsp;Some AI outcomes might not be affected by MCE, such as an unaligned AI that does something like maximizing the number of paperclips for reasons other than human values (such as a programming error) or </span><span>one whose designers create its value function without regard for humanity&rsquo;s current moral views (&ldquo;coherent extrapolated volition&rdquo; could be an example of this, though I agree with </span><span class="c4"><a class="c11" href="https://foundational-research.org/dealing-with-moral-multiplicity/#What_about_reflective_equilibrium">Brian Tomasik</a></span><span>&nbsp;that current moral views will likely be important in this scenario).</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Given my current, highly uncertain estimates of the likelihood of various far future scenarios, I would guess that MCE is applicable in somewhat more cases than AIA, suggesting it&rsquo;s easier to make a difference to the far future through MCE. (This is analogous to saying the risk of MCE-failure seems greater than the risk of AIA-failure, though I&rsquo;m trying to avoid simplifying these into binary outcomes.)</span></p><h1 class="c13" id="h.fn649izfj2dw"><span class="c17 c19">Tractability</span></h1><p class="c1"><span>How much of an impact can we expect our marginal resource</span><span>s</span><span>&nbsp;to have on the </span><span>probability</span><span class="c0">&nbsp;of extinction risk, or on the moral circle of the far future?</span></p><h2 class="c7" id="h.gjmbi8ndxsjc"><span class="c10">Social change versus technical research</span></h2><p class="c1"><span>One may believe</span><span class="c12">&nbsp;changing people&rsquo;s attitudes and behavior is quite difficult, </span><span>and</span><span class="c12">&nbsp;direct work on </span><span>AIA</span><span class="c12">&nbsp;involves a lot less of that. While </span><span>AIA</span><span class="c12">&nbsp;likely involves </span><span>influencing</span><span class="c12">&nbsp;some people (e.g. policymakers, researchers, and corporate executives), MCE is almost entirely </span><span>influencing</span><span class="c12">&nbsp;people</span><span>&rsquo;s attitudes and behavior</span><span class="c12">.</span><sup class="c12"><a href="#ftnt24" id="ftnt_ref24">[24]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>However, one could instead believe that technical research is more difficult in general, pointing to potential evidence such as the large amount of money spent on technical research (e.g. by Silicon Valley) with often very little to show for it, while huge social change seems to sometimes be effected by small groups of advocates with relatively little money</span><span>&nbsp;(e.g. </span><span>organizers</span><span>&nbsp;of revolutions in Egypt, Serbia, and Turkey</span><span>)</span><span class="c0">. (I don&rsquo;t mean this as a very strong or persuasive argument, just as a possibility. There are plenty of examples of tech done with few resources and social change done with many.)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c0">It&rsquo;s hard to speak so generally, but I would guess that technical research tends to be easier than causing social change. And this seems like the strongest argument in favor of working on AIA over working on MCE.</span></p><h2 class="c7" id="h.w7yig7otkfud"><span class="c10">Track record</span></h2><p class="c1"><span>In terms of EA work explicitly focused on the goals of AIA and MCE, AIA has a much better track record. The past few years have seen significant technical research output from organizations like MIRI and FHI, as </span><span class="c4"><a class="c11" href="http://effective-altruism.com/ea/1iu/2018_ai_safety_literature_review_and_charity/">documented</a></span><span>&nbsp;by user Larks on the EA Forum for 2016 and 2017. I&rsquo;d defer readers to those posts, but as a brief example, MIRI had an acclaimed paper on </span><span class="c4"><a class="c11" href="https://intelligence.org/2016/09/12/new-paper-logical-induction/">&ldquo;Logical Induction,&rdquo;</a></span><span>&nbsp;which used a financial market process to estimate the likelihood of logical facts (e.g. mathematical propositions like the Riemann hypothesis) that we aren&rsquo;t yet sure of. This is analogous to how we use probability theory to estimate the likelihood of empirical facts (e.g. a dice roll). In the bigger picture of AIA, this research could help lay the technical foundation for building a</span><span>n</span><span>&nbsp;aligned AGI. See Larks</span><span>&rsquo;</span><span>&nbsp;post for a discussion of more papers like this, as well as non-technical work done by AI-focused organizations such as the Future of Life Institute&rsquo;s </span><span class="c4"><a class="c11" href="https://futureoflife.org/ai-open-letter/">open letter</a></span><span>&nbsp;on AI safety signed by leading AI researchers and cited by the White House&rsquo;s </span><span class="c4"><a class="c11" href="https://obamawhitehouse.archives.gov/blog/2016/10/12/administrations-report-future-artificial-intelligence">&ldquo;Report on the Future of Artificial Intelligence.&rdquo;</a></span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Using an analogous definition for MCE, </span><span>EA work explicitly focused on MCE (meaning expanding the moral circle in order to improve the far future) basically only started in 2017 with the founding of Sentience Institute (SI)</span><span>, though there were various blog posts and articles discussing it before then</span><span>. SI has basically finished four research projects: (1) </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/foundational-questions-summaries">Foundational Question Summaries</a></span><span>&nbsp;that summarize evidence we have on important effective animal advocacy (EAA) questions, including a survey of EAA researchers, (2) a case study of the </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/british-antislavery">British antislavery movement</a></span><span>&nbsp;to better understand how they achieved one of the first </span><span>major </span><span>moral circle expansions in modern history, (3) a case study of </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/nuclear-power-clean-meat">nuclear power</a></span><span>&nbsp;to better understand how some countries (e.g. France) enthusiastically adopted this new technology, but others (e.g. the US) didn&rsquo;t, (4) a </span><span class="c4"><a class="c11" href="https://www.sentienceinstitute.org/animal-farming-attitudes-survey-2017">nationally representative poll</a></span><span>&nbsp;of US attitudes towards animal farming and animal-free food.</span><span><br><br>With a broader definition of MCE that includes activities that people prioritizing MCE tend to think are quite </span><span>indirectly</span><span>&nbsp;effective (see the Neglectedness section for discussion of definitions), we&rsquo;ve seen EA achieve quite a lot more, such as the work done by The Humane League, Mercy For Animals, Animal Equality, and other organizations on corporate </span><span class="c4"><a class="c11" href="https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms#Corporate_cage-free_campaigns_are_extremely_cost-effective">welfare reforms</a></span><span>&nbsp;to animal farming practices</span><span>, </span><span>and </span><span>the work done by The Good Food Institute and others on supporting a shift away from animal farming, especially </span><span>through </span><span>supporting new technologies like so-called </span><span class="c4"><a class="c11" href="http://cleanmeat.org/">&ldquo;clean meat.&rdquo;</a></span></p><p class="c1"><span><br>Since I favor the narrower definition, I think AIA outperforms MCE on track record, but the difference in track record seems largely explained by the greater resources spent on AIA, which makes it a less important consideration. (Also, when I personally decided to focus on MCE, SI did not yet exist, so the lack of track record was an even stronger consideration in favor of AIA (though MCE was also more </span><span>n</span><span>eglected at that time).)</span><span><br><br></span><span>T</span><span>o be clear, the track records of all far future projects tend to be weaker than </span><span>near</span><span>-term projects where we can directly see the results.</span></p><h2 class="c7" id="h.q1zibr1u7nw5"><span>Robustness</span></h2><p class="c1"><span>If one values </span><span>robustness, meaning </span><span>a higher </span><span>certainty that one is having a positive impact</span><span>, either for instrumental or intrinsic reasons, then AIA might be more promising because once we develop an aligned AI</span><span>&nbsp;(that continues to be aligned over time)</span><span>, the work of AIA is done and won&rsquo;t need to be redone in the future. With MCE, assuming the advent of AI or similar developments won&rsquo;t fix society&rsquo;s values in place</span><span>&nbsp;(known as &ldquo;value lock-in&rdquo;)</span><span>, then MCE progress could more easily be undone, especially if one believes there&rsquo;s a social setpoint that humanity drifts back towards when moral progress is made.</span><sup><a href="#ftnt25" id="ftnt_ref25">[25]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I think the assumptions of this argument make it quite weak: </span><span>I&rsquo;d guess an &ldquo;intelligence explosion&rdquo; </span><span>has a significant chance of value lock-in</span><span>,</span><sup><a href="#ftnt26" id="ftnt_ref26">[26]</a></sup><sup><a href="#ftnt27" id="ftnt_ref27">[27]</a></sup><span>&nbsp;and </span><span>I don&rsquo;t think there&rsquo;s a setpoint in the sense that positive moral change increases the risk of negative moral change</span><span>. I also don&rsquo;t value robustness intrinsically at all or instrumentally very much</span><span>;</span><span>&nbsp;I think that there is so much uncertainty in all of these strategies and such weak prior beliefs</span><sup><a href="#ftnt28" id="ftnt_ref28">[28]</a></sup><span class="c0">&nbsp;that differences in certainty of impact matter relatively little.</span></p><h2 class="c7" id="h.m09slze9huhe"><span>Miscellaneous</span></h2><p class="c1"><span>Work on either cause area runs the risk of backfiring. The main risk for AIA seems to be that the technical research done to better understand how to build an aligned AI will increase AI capabilities generally, meaning it&rsquo;s also easier for humanity to produce an unaligned AI. </span><span>The main risk for MCE seems to be that certain advocacy strategies will end up having the opposite effect as intended, such as a confrontational protest for animal rights that ends up </span><span>putting people off of the cause</span><span>.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>It&rsquo;s unclear which project has better </span><span>near-term </span><span>proxies and feedback loops </span><span>to assess and increase long-term impact</span><span>. AIA has technical problems with solutions that can be mathematically proven, but these might end up having little bearing on final AIA </span><span>outcomes</span><span>, such as if an AGI isn&rsquo;t developed using the method that was </span><span>advised</span><span>&nbsp;or if technical solutions aren&rsquo;t implemented by policy-makers. MCE has metrics like public attitudes and practices.</span><span>&nbsp;My weak intuition here, and the weak intuition of other reasonable people I&rsquo;ve discussed this with, is that MCE has better near-term proxies.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>It&rsquo;s unclear which project has more historical evidence that EAs can learn from to be more effective. AIA has previous scientific, mathematical, and philosophical research and technological successes and failures, </span><span>while</span><span>&nbsp;MCE has previous psychological, social</span><span>, political</span><span class="c0">, and economic research and advocacy successes and failures.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Finally, I do think that we learn a lot about tractability just by working directly on an issue. </span><span>Given how little effort has gone into MCE itself (see Neglectedness below), I think we could resolve a significant amount of uncertainty with more work in the field.</span><span><br><br>Overall, considering only direct </span><span>t</span><span>ractability (i.e. </span><span>ignoring information value due to </span><span>n</span><span>eglectedness</span><span class="c0">, which would help other EAs with their cause prioritization), I&rsquo;d guess AIA is a little more tractable.</span></p><h1 class="c13" id="h.gv4nosomzue0"><span class="c17 c19">Neglectedness</span></h1><p class="c1"><span>With neglectedness, we also face a challenge of how broadly to define the cause area. In this case, we have a fairly clear goal with our definition: to best assess how much low-hanging fruit is available. To me, it seems like there are two simple definitions that meet this goal: (i) organizations or individuals working explicitly on the cause area, (ii) organizations or individuals working on the strategies that are seen as top-tier by people focused on the cause area. How much one favors (i) versus (ii) depends largely on whether one thinks the top-tier strategies are fairly well-established and thus (ii) makes sense, or whether they will change over time such that one should favor (i) because those organizations and individuals will be better able to adjust.</span><sup><a href="#ftnt29" id="ftnt_ref29">[29]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>With the explicit focus definitions of AIA and MCE (recall this includes having a far future focus), it seems that MCE is much more neglected and has more low-hanging fruit.</span><sup><a href="#ftnt30" id="ftnt_ref30">[30]</a></sup><span>&nbsp;For example, there is only one organization that I know of explicitly committed to MCE in the EA community (SI), while </span><span>numerous</span><span>&nbsp;organizations (MIRI, CHAI, part of FHI, part of CSER, </span><span>even parts of AI capabilities organizations like Montreal Institute for Learning Algorithms, DeepMind, and OpenAI, etc.</span><span>) are </span><span>explicitly committed to</span><span class="c0">&nbsp;AIA. Because MCE seems more neglected, we could learn a lot about MCE through SI&rsquo;s initial work, such as how easily advocates have achieved MCE throughout history.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>If we include those working on the cause area without an explicit focus, then that seems to widen the definition of MCE to include some of the top strategies being used to expand the moral circle in the near-term, such as farmed animal work done by Animal Charity Evaluators and it&rsquo;s top-recommended charities, which have a </span><span class="c4"><a class="c11" href="https://animalcharityevaluators.org/transparency/financials/">combined</a></span><span>&nbsp;</span><span class="c4"><a class="c11" href="https://animalcharityevaluators.org/charity-review/the-good-food-institute/#c1">budget</a></span><span>&nbsp;</span><span class="c4"><a class="c11" href="https://animalcharityevaluators.org/wp-content/uploads/2017/11/the-humane-leagues-accomplishments-and-budget-2016-2017.pdf">of</a></span><span>&nbsp;</span><span class="c4"><a class="c11" href="https://animalcharityevaluators.org/wp-content/uploads/2017/11/animal-equalitys-expenses-2016-2017.pdf">around</a></span><span>&nbsp;$7.5 million in 2016. The combined budgets of top-tier AIA work is harder to estimate, but the Centre for Effective Altruism </span><span class="c4"><a class="c11" href="https://www.centreforeffectivealtruism.org/blog/changes-in-funding-in-the-ai-safety-field/">estimates</a></span><span>&nbsp;all AIA work in 2016 was around $6.6 million. </span><span>The AIA budgets seem to be increasing more quickly than the MCE budgets, especially given the grant-making of the Open philanthropy project. We could also include</span><span>&nbsp;EA movement-building organizations that place a strong focus on reducing extinction risk, and even AIA specifically, such as 80,000 Hours. </span><span>The categorization for MCE seems to have more room to broaden, perhaps all the way to</span><span>&nbsp;mainstream animal advocacy strategies like the work of People for the Ethical Treatment of Animals (PETA), </span><span>which</span><span class="c0">&nbsp;might make AIA more neglected. (It could potentially go even farther, such as advocating for human sweatshop laborers, but that seems too far removed and I don&rsquo;t know any MCE advocates who think it&rsquo;s plausibly top-tier.)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I think there&rsquo;s a difference in aptitude that suggests MCE is more neglected. Moral advocacy seems like a field </span><span>which</span><span>, while quite crowded, seems relatively easy for deliberate, thoughtful people to vastly outperform the average</span><span>&nbsp;advocate</span><span>,</span><sup><a href="#ftnt31" id="ftnt_ref31">[31]</a></sup><span>&nbsp;which can lead to surprisingly large impact (e.g.</span><span>&nbsp;EAs have already had far more success in publishing their writing, such as books and op-eds, than most writers hope for</span><span>)</span><span>.</span><sup><a href="#ftnt32" id="ftnt_ref32">[32]</a></sup><span>&nbsp;Additionally, despite centuries of advocacy, very little </span><span>quality </span><span>research has been done to critically examine what advocacy is effective and what&rsquo;s not, while the fields of math, computer science, and machine learning </span><span>involve</span><span>&nbsp;substantial self-reflection </span><span>and </span><span>are largely worked on by academics who </span><span>seem to </span><span>use more critical thinking than the average activist</span><span>&nbsp;(e.g. there&rsquo;s far more skepticism in these academic communities, a demand for rigor and experimentation that&rsquo;s rarely seen among advocates)</span><span>. In general, I think the aptitude of the average social change advocate is much lower than </span><span>that of </span><span>the average technological researcher, </span><span>suggesting</span><span>&nbsp;MCE </span><span>is </span><span>more neglected</span><span>, though of course other factors also count</span><span>.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>The relative neglectedness of MCE also seems likely to continue, given the greater self-interest humanity has in AIA relative to MCE and, in my opinion, the net biases towards AIA described in the Biases section of this blog post.</span><span>&nbsp;(This self-interest argument is </span><span>a </span><span>particularly important </span><span>consideration for prioritizing MCE over AIA </span><span>in my view.</span><sup><a href="#ftnt33" id="ftnt_ref33">[33]</a></sup><span>)</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>However, while neglectedness is typically thought to make a project more tractable, it seems that existing work in the extinction risk space has made marginal contributions more impactful in some ways. For example, talented AI researchers can find work relatively easily at an organization dedicated to AIA, while the path for talented MCE researchers is far less clear and easy. This alludes to the difference in tractability that might exist between labor resources and funding resources, as it currently seems like MCE is much more funding-constrained</span><sup><a href="#ftnt34" id="ftnt_ref34">[34]</a></sup><span>&nbsp;while AIA is </span><span class="c4"><a class="c11" href="http://lesswrong.com/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/">largely talent-constrained</a></span><span>.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>As another example, there are already solid inroads between the AIA community and the AI decision-makers, and AI decision-makers have already expressed interest in AIA, suggesting that influencing them with research results will be fairly easy once those research results are in hand.</span><span>&nbsp;This means both that our estimation of AIA&rsquo;s </span><span>n</span><span>eglectedness should decrease, and that our estimation of its </span><span>non-neglectedness t</span><span>ractability should increase, in the sense that </span><span>n</span><span>eglectedness is a part of </span><span>t</span><span>ractability. (The definitions in this framework vary.)</span><span><br><br>All things considered, I find MCE to be more compelling from a </span><span>n</span><span>eglectedness perspective, particularly due to the current EA resource allocation and the self-interest humanity has</span><span>, and will most likely continue to have,</span><span>&nbsp;in AIA. When I decided to focus on MCE, there was an even stronger case for </span><span>n</span><span class="c0">eglectedness because no organization existed committed to that goal (SI was founded in 2017), though there was an increased downside to MCE &mdash; the even more limited track record.</span></p><h1 class="c13" id="h.7abcgzfuy616"><span class="c17 c19">Cooperation</span></h1><p class="c1"><span>Values spreading as a far future intervention has been criticized on the following grounds: People have very different values, so trying to promote your values and change other people&rsquo;s could be seen as uncooperative. Cooperation seems to be useful both directly (e.g. how willing are other people to help us out if we&rsquo;re fighting them?) and in a broader sense because of superrationality, an argument that one should help others even when there&rsquo;s no causal mechanism for reciprocation.</span><sup><a href="#ftnt35" id="ftnt_ref35">[35]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I think this is certainly a good consideration against some forms of values spreading. For example, I don&rsquo;t think it&rsquo;d be wise for a</span><span>n MCE-focused</span><span>&nbsp;EA to </span><span>disrupt</span><span>&nbsp;the </span><span class="c4"><a class="c11" href="https://www.eaglobal.org/">Effective Altruism Global</a></span><span>&nbsp;conferences</span><span>&nbsp;(e.g. yell on stage and try to keep the conference from continuing)</span><span>&nbsp;if they </span><span>have an insufficient focus on MCE</span><span>. This seems highly ineffective because of how uncooperative it is,</span><span>&nbsp;given the EA space is supposed to be one for having challenging discussions and solving problems, not merely advocating one&rsquo;s positions like a political rally</span><span class="c0">.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>However, I don&rsquo;t think it holds much weight against MCE in particular for two reasons: First, because I don&rsquo;t think MCE is particularly uncooperative. </span><span>For example, </span><span>I never bring up MCE with someone and hear</span><span>,</span><span>&nbsp;&ldquo;But I like to keep my moral circle small!&rdquo; </span><span>I think this is because there are many different components of our attitudes and worldview that </span><span>we </span><span>refer to as values and morals.</span><span>&nbsp;People have some deeply</span><span>-</span><span>held values that seem strongly resistant to change, such as </span><span>their </span><span>religion or the welfare of their immediate family, but very few people seem to have small moral circles as a deeply</span><span>-</span><span>held value. Instead, the small moral circle seem</span><span>s</span><span>&nbsp;to mostly be a superficial, </span><span>casual</span><span>&nbsp;</span><span>value</span><span>&nbsp;(though it&rsquo;s often connected to the deeper </span><span>values</span><span>) that people are okay with</span><span>&nbsp;&mdash; </span><span>or even happy about</span><span>&nbsp;&mdash; </span><span>changing.</span><sup><a href="#ftnt36" id="ftnt_ref36">[36]</a></sup></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>Second, insofar as MCE is uncooperative, I think a large number of other EA interventions, including AIA, are similarly uncooperative. Many people even in the EA community are concerned with, or even opposed to, AIA. For example, if one believes an aligned AI would create a worse far future than an unaligned AI, or if one thinks AIA is harmfully distracting from </span><span>more important</span><span>&nbsp;issues and gives EA a bad name. This isn&rsquo;t to say I think AIA is bad because it&rsquo;s uncooperative</span><span>&nbsp;&mdash; o</span><span>n the contrary, this seems like a level of uncooperativeness that&rsquo;s often necessary for dedicated EAs. (In a trivial way, </span><span>basically </span><span>all action involves uncooperativeness because it&rsquo;s always about changing the status quo</span><span>&nbsp;or preventing the status quo from changing</span><span>.</span><sup><a href="#ftnt37" id="ftnt_ref37">[37]</a></sup><span>&nbsp;Even inaction can involve uncooperativeness if it means not working to help someone who would like your help.</span><span>)<br><br>I do think it&rsquo;s more important to be cooperative in some other situations, such as if one has a very different value system than some of their colleagues, as might be the case for the Foundational Research Institute, which </span><span class="c4"><a class="c11" href="https://foundational-research.org/reasons-to-be-nice-to-other-value-systems/">advocates strongly</a></span><span>&nbsp;for cooperation with other EAs.</span></p><h2 class="c7" id="h.9wdylb58crq9"><span class="c10">Cooperation with future do-gooders</span></h2><p class="c1"><span class="c0">Another argument against values spreading goes something like, &ldquo;We can worry about values after we&rsquo;ve safely developed AGI. Our tradeoff isn&rsquo;t, &lsquo;Should we work on values or AI?&rsquo; but instead &lsquo;Should we work on AI now and values later, or values now and maybe AI later if there&rsquo;s time?&rsquo;&rdquo;</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I agree with one interpretation of the first part of this argument, that urgency is an important factor and AIA does seem like a time-sensitive cause area. However, I think MCE is similarly time-sensitive because of risks of value lock-in where our descendants&rsquo; morality becomes much harder to change, such as if AI designers choose to fix the values of an AGI, or at least to make them independent of other people&rsquo;s opinions (they could still be amenable to self-reflection of the designer and new empirical data about the universe other than people&rsquo;s opinions)</span><sup><a href="#ftnt38" id="ftnt_ref38">[38]</a></sup><span>;</span><span>&nbsp;if humanity sends out colonization vessels across the universe that are traveling too fast for us to adjust based on our changing moral views</span><span>;</span><span class="c0">&nbsp;or if society just becomes too wide and disparate to have effective social change mechanisms like we do today on Earth.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>I disagree with the stronger interpretation, that we can count on some sort of cooperation with or control over future people. There might be some extent to which we can do this, such as via superrationality, but that seems like a fairly weak effect. Instead, I think we&rsquo;re largely on our own, deciding what we do in the next few years (or perhaps in our whole career), and just making our best guess of what future people will do. It sounds very difficult to strike a deal with them that will ensure they work on MCE in exchange for us working on AIA.</span></p><h1 class="c13" id="h.flt9dsg85g0s"><span>Bias</span></h1><p class="c1"><span>I&rsquo;m</span><span>&nbsp;always cautious about bringing considerations of bias into an important discussion like this. Considerations easily turn into messy, personal attacks, and often you can fling roughly-equal considerations of counter-biases when accusations of bias are hurled at you. However, I think we should give them serious consideration in this case. First, I want to be exhaustive in this blog post, and that means throwing every consideration on the table, even messy ones. Second, my own cause prioritization &ldquo;journey&rdquo; led me first to </span><span>AIA</span><span>&nbsp;and other non-MCE/non-animal-advocacy EA priorities (mainly EA movement-building)</span><span class="c0">, and it was considerations of bias that allowed me to look at the object-level arguments with fresh eyes and decide that I had been way off in my previous assessment.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c0">Third and most importantly, people&rsquo;s views on this topic are inevitably driven mostly by intuitive, subjective judgment calls. One could easily read everything I&rsquo;ve written in this post and say they lean in the MCE direction on every topic, or the AIA direction, and there would be little object-level criticism one could make against that if they just based their view on a different intuitive synthesis of the considerations. This subjectivity is dangerous, but it is also humbling. It requires us to take an honest look at our own thought processes in order to avoid the subtle, irrational effects that might push us in either direction. It also requires caution when evaluating &ldquo;expert&rdquo; judgment, given how much experts could be affected by personal and social biases themselves.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>The best way I know of to think about bias in this case is to consider the biases and other factors that favor either cause area and see which case seems more powerful, or which particular biases might be affecting our own views. </span><span>The following lists are presumably not exhaustive but lay out what I think are some common key parts of people&rsquo;s journeys to AIA or MCE. Of course, these factors are not entirely deterministic and probably not all will apply to you</span><span>, nor do they necessarily mean that you are wrong in your cause prioritization</span><span>. Based on the circumstances that apply more to you, consider </span><span class="c0">taking a more skeptical look at the project you favor and your current views on the object-level arguments for it.</span></p><h2 class="c7" id="h.gii9jrhyrggp"><span>One might be biased towards AIA if</span><span>...</span></h2><ol class="c26 lst-kix_rc6vb97movl2-0 start" start="1"><li class="c1 c8 li-bullet-0"><span>They eat animal products, and thus are assign lower moral value and </span><span class="c4"><a class="c11" href="http://journals.sagepub.com/doi/abs/10.1177/0146167211424291">less mental faculties</a></span><span class="c0">&nbsp;to animals.</span></li><li class="c1 c8 li-bullet-0"><span>They haven&rsquo;t accounted for the bias of </span><span class="c4"><a class="c11" href="https://luciuscaviola.com/s/Caviola-Everett-and-Faber-2018-Speciesism-JPSP-Pre-Print.pdf">speciesism</a></span><span class="c0">.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They lack personal connections to animals, such as growing up with pets.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are or have been a fan of science fiction and fantasy literature and media, especially if they dreamed of being the hero.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have a tendency towards technical research over social projects.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They lack social skills.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are inclined towards philosophy and mathematics.</span></li><li class="c1 c8 li-bullet-0"><span>They have a </span><span class="c4"><a class="c11" href="http://www.slate.com/blogs/moneybox/2014/01/14/activists_are_unpopular_social_psychologists_say_people_want_them_to_calm.html">negative perception of activists</a></span><span class="c0">, perhaps seeing them as hippies, irrational, idealistic, &ldquo;social justice warriors,&rdquo; or overly emotion-driven.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are a part of the EA community, and therefore drift towards the status quo of EA leaders and peers. (The views of EA leaders can of course be genuine evidence of the correct cause prioritization, but they can also lead to bias.)</span></li><li class="c1 c8 li-bullet-0"><span class="c0">The idea of &ldquo;saving the world&rdquo; appeals to them.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They take pride in their intelligence, and would love if they could save the world just by doing brilliant technical research.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are competitive, and like the feeling/mindset of doing astronomically more good than the average do-gooder, or even the average EA. (I&rsquo;ve argued in this post that MCE has this astronomical impact, but it lacks the feeling of literally &ldquo;saving the world&rdquo; or otherwise having a clear impact that makes a good hero&rsquo;s journey climax, and it&rsquo;s closely tied to lesser, near-term impacts.)</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have little personal experience of extreme suffering, the sort that makes one pessimistic about the far future, especially regarding s-risks. (Personal experience could be one&rsquo;s own experience or the experiences of close friends and family.)</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have little personal experience of oppression, such as due to their gender, race, disabilities, etc.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are generally a happy person.</span></li><li class="c1 c8 li-bullet-0"><span>They are generally optimistic, or at least averse to thinking about bad outcomes like how humanity could cause astronomical suffering. (Though some pessimism is required for AIA in the sense that they don&rsquo;t count on AI capabilities researchers ending up with an aligned AI without their help.)</span></li></ol><h2 class="c7" id="h.igu98gyfpwtx"><span class="c10">One might be biased towards MCE if...</span></h2><ol class="c26 lst-kix_uzlzmpmp25j9-0 start" start="1"><li class="c1 c8 li-bullet-0"><span class="c0">They are vegan, especially if they went vegan for non-animal or non-far-future reasons, such as for better personal health.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">Their gut reaction when they hear about extinction risk or AI risk is to judge it nonsensical.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have personal connections to animals, such as growing up with pets.</span></li><li class="c1 c8 li-bullet-0"><span>They are or have been a fan of social movement/activism literature and media, especially if they dreamed of being </span><span>a movement leader</span><span class="c0">.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have a tendency towards social projects over technical research.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have benefitted from above-average social skills.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are inclined towards social science.</span></li><li class="c1 c8 li-bullet-0"><span>They have a positive perception of activists, perhaps seeing them as the true leaders of history.</span></li><li class="c1 c8 li-bullet-0"><span>They have social ties to vegans and animal advocates. (The views of these people can of course be genuine evidence of the correct cause prioritization, but they can also lead to bias.)</span></li><li class="c1 c8 li-bullet-0"><span class="c0">The idea of &ldquo;helping the worst off&rdquo; appeals to them.</span></li><li class="c1 c8 li-bullet-0"><span>They take pride in their social skills, and would love if they could help the worst off just by being socially savvy.</span></li><li class="c1 c8 li-bullet-0"><span>They are not competitive, and like the thought of being a part of a friendly social movement.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have a lot of personal experience of extreme suffering, the sort that makes one pessimistic about the far future, especially regarding s-risks. (Personal experience could be one&rsquo;s own experience or the experiences of close friends and family.)</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They have a lot of personal experience of oppression, such as due to their gender, race, disabilities, etc.</span></li><li class="c1 c8 li-bullet-0"><span>They are generally a</span><span>n unhappy</span><span class="c0">&nbsp;person.</span></li><li class="c1 c8 li-bullet-0"><span class="c0">They are generally pessimistic, or at least don&rsquo;t like thinking about good outcomes. (Though some optimism is required for MCE in the sense that they believe work on MCE can make a large positive difference in social attitudes and behavior.)</span></li><li class="c1 c8 li-bullet-0"><span>They care a lot about directly seeing the impact of their work, even if the bulk of their impact is hard to see. (E.g. seeing improvements in the conditions of farmed animals, which </span><span>can be</span><span class="c0">&nbsp;seen as a proxy for helping farmed-animal-like beings in the far future.)</span></li></ol><h2 class="c7" id="h.ib668zv9209x"><span class="c10">Implications</span></h2><p class="c1"><span class="c0">I personally found myself far more compelled towards AIA in my early involvement with EA before I had thought in detail about the issues discussed in this post. I think the list items in the AIA section apply to me much more strongly than the MCE list. When I considered these biases, in particular speciesism and my desire to follow the status quo of my EA friends, a fresh look at the object-level arguments changed my mind.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span class="c0">From my reading and conversations in EA, I think the biases in favor of AIA are also quite a bit stronger in the community, though of course some EAs &mdash; mainly those already working on animal issues for near-term reasons &mdash; probably feel a stronger pull in the other direction.</span></p><p class="c1 c3"><span class="c0"></span></p><p class="c1"><span>How you think about these bias considerations also depends on how biased you think the average EA is. If you, for example, think EAs tend to be quite biased in another way like </span><span class="c4"><a class="c11" href="https://www.psy.ox.ac.uk/publications/477895">&ldquo;measurement bias&rdquo; or &ldquo;quantifiability bias&rdquo;</a></span><span class="c0">&nbsp;(a tendency to focus too much on easily-quantifiable, low-risk interventions), then considerations of biases on this topic should probably be more compelling to you than they will be to people who think EAs are less biased.</span></p><hr class="c25"><div><p class="c14"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c5">&nbsp;This post attempts to compare these cause areas overall, but since that&rsquo;s sometimes too vague, I specifically mean the strategies within each cause area that seem most promising. I think this is basically equal to &ldquo;what EAs working on MCE most strongly prioritize&rdquo; and &ldquo;what EAs working on AIA most strongly prioritize.&rdquo;</span></p></div><div><p class="c14"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c5">&nbsp;There&rsquo;s a sense in which AIA is a form of MCE simply because AIA will tend to lead to certain values. I&rsquo;m excluding that AIA approach of MCE from my analysis here to avoid overlap between these two cause areas.</span></p></div><div><p class="c14"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c5">&nbsp;Depending on how close we&rsquo;re talking about, this could be quite unlikely. If we&rsquo;re discussing the range of outcomes from dystopia across the universe to utopia across the universe, then a range like &ldquo;between modern earth and the opposite value of modern earth&rdquo; seems like a very tiny fraction of the total possible range.</span></p></div><div><p class="c14"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c5">&nbsp;I mean &ldquo;good&rdquo; in a &ldquo;positive impact&rdquo; sense here, so it includes not just rationality according to the decision-maker but also value alignment, luck, being empirically well-informed, being capable of doing good things, etc.</span></p></div><div><p class="c14"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c5">&nbsp;One reason for optimism is that you might think most extinction risk is in the next few years, such that you and other EAs you know today will still be around to do this research yourselves and make good decisions after those risks are avoided.</span></p></div><div><p class="c14"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c5">&nbsp;Technically one could believe the far future is negative but also that humans will make good decisions about extinction, such as if one believes the far future (given non-extinction) will be bad only due to nonhuman forces, such as aliens or evolutionary trends, but has optimism about human decision-making, including both that humans will make good decisions about extinction and that they will be logistically able to make those decisions. I think this is an unlikely view to settle on, but it would make option value a good thing in a &ldquo;close to zero&rdquo; scenario.</span></p></div><div><p class="c14"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c5">&nbsp;Non-extinct civilizations could be maximized for happiness, maximized for interestingness, set up like Star Wars or another sci-fi scenario, etc. while extinct civilizations would all be devoid of sentient beings, perhaps with some variation in physical structure like different planets or remnant structures of human civilization. </span></p></div><div><p class="c14"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c5">&nbsp;My views on this are currently largely qualitative, but if I had to put a number on the word &ldquo;significant&rdquo; in this context, it&rsquo;d be somewhere around 5-30%. This is a very intuitive estimate, and I&rsquo;m not prepared to justify it.</span></p></div><div><p class="c14"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c9">&nbsp;Paul Christiano made a general argument in favor of humanity reaching good values in the long run due to reflection in his post </span><span class="c4 c9"><a class="c11" href="https://rationalaltruist.com/2013/06/13/against-moral-advocacy/">&ldquo;Against Moral Advocacy&rdquo;</a></span><span class="c9">&nbsp;(see the &ldquo;Optimism about reflection&rdquo; section) though he doesn&rsquo;t specifically address concern for all sentient beings as a potential outcome, which might be less likely than other good values that are more driven by cooperation.&quot;</span></p></div><div><p class="c14"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c9">&nbsp;Nick Bostrom has considered some of these risks of artificial suffering using the term &ldquo;mind crime,&rdquo; which specifically refers to harming sentient beings created inside a superintelligence. See his book, </span><span class="c4 c2"><a class="c11" href="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/1501227742">Superintelligence</a></span><span class="c5">.</span></p></div><div><p class="c14"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c9">&nbsp;The Foundational Research Institute has written about risks of astronomical suffering in </span><span class="c4 c9"><a class="c11" href="https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/">&ldquo;Reducing Risks of Astronomical Suffering: A Neglected Priority.&rdquo;</a></span><span class="c9">&nbsp;</span><span class="c9">The TV series </span><span class="c2">Black Mirror</span><span class="c5">&nbsp;is an interesting dramatic exploration of how the far future could involve vasts amounts of suffering, such as the episodes &ldquo;White Christmas&rdquo; and &ldquo;USS Callister.&rdquo; Of course, the details of these situations often veer towards entertainment over realism, but their exploration of the potential for dystopias in which people abuse sentient digital entities is thought-provoking.</span></p></div><div><p class="c14"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c5">&nbsp;I&rsquo;m highly uncertain about what sort of motivations (like happiness and suffering in humans) future digital sentient beings will have. For example, is punishment being a stronger motivator in earth-originating life just an evolutionary fluke that we can expect to dissipate in artificial beings? Could they be just as motivated to attain reward as we are to avoid punishment? I think this is a promising avenue for future research, and I&rsquo;m glad it&rsquo;s being discussed by some EAs.</span></p></div><div><p class="c14"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c9">&nbsp;Brian Tomasik discusses this in his essay on </span><span class="c4 c9"><a class="c11" href="http://reducing-suffering.org/values-spreading-often-important-extinction-risk/">&ldquo;Values Spreading is Often More Important than Extinction Risk,&rdquo;</a></span><span class="c9">&nbsp;suggesting that, &ldquo;there&#39;s not an obvious similar mechanism pushing organisms toward the things that I care about.&rdquo; However, Paul Christiano notes in </span><span class="c4 c9"><a class="c11" href="https://rationalaltruist.com/2013/06/13/against-moral-advocacy/">&ldquo;Against Moral Advocacy&rdquo;</a></span><span class="c5">&nbsp;that he expects &ldquo;[c]onvergence of values&rdquo; because &ldquo;the space of all human values is not very broad,&rdquo; though this seems quite dependent on how one defines the possible space of values.</span></p></div><div><p class="c14"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c9">&nbsp;This efficiency argument is also discussed in Ben West&rsquo;s article on </span><span class="c4 c9"><a class="c11" href="http://effective-altruism.com/ea/1cl/an_argument_for_why_the_future_may_be_good/">&ldquo;An Argument for Why the Future May Be Good.&rdquo;</a></span></p></div><div><p class="c14"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c5">&nbsp;The term &ldquo;resources&rdquo; is intentionally quite broad. This means whatever the limitations are on the ability to produce happiness and suffering, such as energy or computation.</span></p></div><div><p class="c14"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c5">&nbsp;One can also create hedonium as a promise to get things from rivals, but promises seem less common than threats because threats tend to be more motivating and easier to implement (e.g it&rsquo;s easier to destroy than create). However, some social norms encourage promises over threats because promises are better for society as a whole. Additionally, threats against powerful beings (e.g. other citizens in the same country) do less than threats against less powerful, or more distant beings, and the latter category might be increasingly common in the future.<br><br>Additionally, threats and promises matter less when one considers that they are often unfulfilled because the other party doesn&rsquo;t do the action that was the subject of the threat or promise.</span></p></div><div><p class="c14"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c9">&nbsp;Paul Christiano&rsquo;s blog post on </span><span class="c4 c9"><a class="c11" href="https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/">&ldquo;Why might the future be good?&rdquo;</a></span><span class="c9">&nbsp;argues that &ldquo;the future will be characterized by much higher influence for altruistic values [than self-interest],&rdquo; though he seems to just be discussing the potential of altruism and self-interest to create positive value, rather than their potential to create negative value.<br><br>Brian Tomasik discusses Christiano&rsquo;s argument and others in </span><span class="c4 c9"><a class="c11" href="http://reducing-suffering.org/the-future-of-darwinism/">&ldquo;The Future of Darwinism&rdquo; </a></span><span class="c5">and concludes, &ldquo;Whether the future will be determined by Darwinism or the deliberate decisions of a unified governing structure remains unclear.&rdquo;</span></p></div><div><p class="c14"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c9">&nbsp;One discussion of changes in morality on a large scale is Robin Hanson&rsquo;s blog post, </span><span class="c4 c9"><a class="c11" href="http://www.overcomingbias.com/2012/05/forager-vs-farmer-morality.html">&ldquo;Forager, Farmer Morals.&rdquo;</a></span></p></div><div><p class="c14"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c5">&nbsp;Armchair research is relatively easy, in the sense that all it requires is writing and thinking rather than also digging through historical texts, running scientific studies, or engaging in substantial conversation with advocates, researchers, and/or other stakeholders. It&rsquo;s also more similar to the mathematical and philosophical work that most EAs are used to doing. And it&rsquo;s more attractive as a demonstration of personal prowess to think your way into a crucial consideration than to arrive at one through the tedious work of research. (These reasons are similar to the reasons I feel most far-future-focused EAs are biased towards AIA over MCE.)</span></p></div><div><p class="c14"><a href="#ftnt_ref20" id="ftnt20">[20]</a><span class="c5">&nbsp;These sentient beings probably won&rsquo;t be the biological animals we know today, but instead digital beings who can more efficiently achieve the AI&rsquo;s goals.</span></p></div><div><p class="c14"><a href="#ftnt_ref21" id="ftnt21">[21]</a><span class="c5">&nbsp;The neglectedness heuristic involves a similar messiness of definitions, but the choices seem less arbitrary to me, and the different definitions lead to more similar results.</span></p></div><div><p class="c14"><a href="#ftnt_ref22" id="ftnt22">[22]</a><span class="c5">&nbsp;Arguably this consideration should be under Tractability rather than Scale.</span></p></div><div><p class="c14"><a href="#ftnt_ref23" id="ftnt23">[23]</a><span class="c5">&nbsp;There&rsquo;s a related framing here of &ldquo;leverage,&rdquo; with the basic argument being that AIA seems more compelling than MCE because AIA is specifically targeted at an important, narrow far future factor (the development of AGI) while MCE is not as specifically targeted. This also suggests that we should consider specific MCE tactics focused on important, narrow far future factors, such as ensuring the AI decision-makers have wide moral circles even if the rest of society lags behind. I find this argument fairly compelling, including the implication that MCE advocates should focus more on advocating for digital sentience and advocating in the EA community than they would otherwise.</span></p></div><div><p class="c14"><a href="#ftnt_ref24" id="ftnt24">[24]</a><span class="c5">&nbsp;Though plausibly MCE involves only influencing a few decision-makers, such as the designers of an AGI.</span></p></div><div><p class="c14"><a href="#ftnt_ref25" id="ftnt25">[25]</a><span class="c9">&nbsp;Brian Tomasik discusses this in, </span><span class="c4 c9"><a class="c11" href="http://reducing-suffering.org/values-spreading-often-important-extinction-risk/">&ldquo;Values Spreading is Often More Important than Extinction Risk,&rdquo;</a></span><span class="c5">&nbsp;arguing that, &ldquo;Very likely our values will be lost to entropy or Darwinian forces beyond our control. However, there&#39;s some chance that we&#39;ll create a singleton in the next few centuries that includes goal-preservation mechanisms allowing our values to be &quot;locked in&quot; indefinitely. Even absent a singleton, as long as the vastness of space allows for distinct regions to execute on their own values without take-over by other powers, then we don&#39;t even need a singleton; we just need goal-preservation mechanisms.&rdquo;</span></p></div><div><p class="c14"><a href="#ftnt_ref26" id="ftnt26">[26]</a><span class="c9">&nbsp;Brian Tomasik discusses the likelihood of value lock-in in his essay, </span><span class="c9"><a class="c11" href="http://reducing-suffering.org/will-future-civilization-eventually-achieve-goal-preservation/">&ldquo;Will Future Civilization Eventually Achieve Goal Preservation?&rdquo;</a></span></p></div><div><p class="c14"><a href="#ftnt_ref27" id="ftnt27">[27]</a><span class="c5">&nbsp;The advent of AGI seems like it will have similar effects on the lock-in of values and alignment, so if you think AI timelines are shorter (i.e. advanced AI will be developed sooner), then that increases the urgency of both cause areas. If you think timelines are so short that we will struggle to successfully reach AI alignment, then that decreases the tractability of AIA, but MCE seems like it could more easily have a partial effect on AI outcomes than AIA could.</span></p></div><div><p class="c14"><a href="#ftnt_ref28" id="ftnt28">[28]</a><span class="c9">&nbsp;In the case of near-term, direct interventions, one might believe that </span><span class="c4 c9"><a class="c11" href="https://80000hours.org/articles/effective-social-program/">&ldquo;most social programmes don&rsquo;t work,&rdquo;</a></span><span class="c5">&nbsp;which suggests that we should have low, strong priors for intervention effectiveness that we need robustness to overcome.</span></p></div><div><p class="c14"><a href="#ftnt_ref29" id="ftnt29">[29]</a><span class="c9">&nbsp;Caspar Oesterheld discusses the ambiguity of neglectedness definitions in his blog post,</span><span class="c9"><a class="c11" href="https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/">&nbsp;&ldquo;Complications in evaluating neglectedness.&rdquo;</a></span><span class="c5">&nbsp;Other EAs have also raised concern about this commonly-used heuristic, and I almost included it in this post under the &ldquo;Tractability&rdquo; section.</span></p></div><div><p class="c14"><a href="#ftnt_ref30" id="ftnt30">[30]</a><span class="c5">&nbsp;This is a fairly intuitive sense of the word &ldquo;matched.&rdquo; I&rsquo;m taking the topic of ways to affect the far future, dividing it into population risk and quality risk categories, then treating AIA and MCE as subcategories of each. I&rsquo;m also thinking in terms of each project (AIA and MCE) being in the category of &ldquo;cause areas with at least pretty good arguments in their favor,&rdquo; and I think &ldquo;put decent resources into all such projects until the arguments are rebutted&rdquo; is a good approach for the EA community.</span></p></div><div><p class="c14"><a href="#ftnt_ref31" id="ftnt31">[31]</a><span class="c5">&nbsp;I mean &ldquo;advocate&rdquo; quite broadly here, just anyone working to effect social change, such as people submitting op-eds to newspapers or trying to get pedestrians to look at their protest or take their leaflets.</span></p></div><div><p class="c14"><a href="#ftnt_ref32" id="ftnt32">[32]</a><span class="c5">&nbsp;It&rsquo;s unclear what the explanation is for this. It could just be demographic differences such as high IQ, going to elite universities, etc. but it could also be exceptional &ldquo;rationality skills&rdquo; like finding loopholes in the publishing system.</span></p></div><div><p class="c14"><a href="#ftnt_ref33" id="ftnt33">[33]</a><span class="c9">&nbsp;In Brian Tomasik&rsquo;s essay on </span><span class="c4 c9"><a class="c11" href="http://reducing-suffering.org/values-spreading-often-important-extinction-risk/">&ldquo;Values Spreading is Often More Important than Extinction Risk,&rdquo;</a></span><span class="c9">&nbsp;he argues that &ldquo;[m]ost people want to prevent extinction&rdquo; while, &ldquo;In contrast, you may have particular things that you value that aren&#39;t widely shared. These things might be easy to create, and the intuition that they matter is probably not too hard to spread. Thus, it seems likely that you would have higher leverage in spreading your own values than in working on safety measures against extinction.&rdquo;</span></p></div><div><p class="c14"><a href="#ftnt_ref34" id="ftnt34">[34]</a><span class="c5">&nbsp;This is just my personal impression from working in MCE, especially with my organization Sentience Institute. With indirect work, The Good Food Institute is a potential exception since they have struggled to quickly hired talented people after their large amounts of funding.</span></p></div><div><p class="c14"><a href="#ftnt_ref35" id="ftnt35">[35]</a><span class="c9">&nbsp;See </span><span class="c4 c9"><a class="c11" href="https://foundational-research.org/reasons-to-be-nice-to-other-value-systems/#Superrationality">&ldquo;Superrationality&rdquo;</a></span><span class="c9">&nbsp;in &ldquo;Reasons to Be Nice to Other Value Systems&rdquo; for an EA introduction to the idea. See &ldquo;In favor of &lsquo;being nice&rsquo;&rdquo; in </span><span class="c4 c9"><a class="c11" href="https://rationalaltruist.com/2013/06/13/against-moral-advocacy/">&ldquo;Against Moral Advocacy&rdquo;</a></span><span class="c9">&nbsp;as example of cooperation as an argument against values spreading. In </span><span class="c4 c9"><a class="c11" href="https://foundational-research.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf#page%3D76">&ldquo;Multiverse-wide Cooperation via Correlated Decision Making,&rdquo;</a></span><span class="c5">&nbsp;Caspar Oesterheld argues that superrational cooperation makes MCE more important.</span></p></div><div><p class="c14"><a href="#ftnt_ref36" id="ftnt36">[36]</a><span class="c5">&nbsp;This discussion is complicated by the widely varying degrees of MCE. While, for example, most US residents seem perfectly okay with expanding concern to vertebrates, there would be more opposition to expanding to insects, and even more to some simple computer programs that some argue should fit into the edges of our moral circles. I do think the farthest expansions are much less cooperative in this sense, though if the message is just framed as, &ldquo;expand our moral circle to all sentient beings,&rdquo; I still expect strong agreement.</span></p></div><div><p class="c14"><a href="#ftnt_ref37" id="ftnt37">[37]</a><span class="c5">&nbsp;One exception is a situation where everyone wants a change to happen, but nobody else wants it badly enough to put the work into changing the status quo.</span></p></div><div><p class="c14"><a href="#ftnt_ref38" id="ftnt38">[38]</a><span class="c5">&nbsp;My impression is that the AI safety community currently wants to avoid fixing these values, though they might still be trying to make them resistant to advocacy from other people, and in general I think many people today would prefer to fix the values of an AGI when they consider that they might not agree with potential future values.</span></p></div></body></html>

</div>




    <hr>
    <div class="container newsletter-container ">
      <p>Subscribe to our newsletter to receive updates on our research and activities. We average one to two emails per year.</p>
      <div id="mc_embed_signup">
        <form action="//sentienceinstitute.us15.list-manage.com/subscribe/post?u=d898f823d035e0601866e68d6&amp;id=cbf2d915a6" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
          <div id="mc_embed_signup_scroll">
            <input type="email" value="" name="EMAIL" class="email form-input" id="mce-EMAIL" placeholder="Email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d898f823d035e0601866e68d6_cbf2d915a6" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
          </div>
        </form>
      </div>
    </div>
    
    <footer class="footer">
      <div class="container">
        <div class="row">
          <div class="col-md-2">
            <div><span class="bold">Contact us: </span><a href="mailto:info@sentienceinstitute.org">info@sentienceinstitute.org</a></div>
            <div class="icons">
              <!-- <a href="/rss.xml"><i class="material-icons">rss_feed</i></a> -->
              <a href="https://www.facebook.com/sentienceinstitute"><img class="icon" src="../img/icons/icon_facebook_white.png"/></a>
              <a href="https://www.twitter.com/sentienceinst"><img class="icon" src="../img/icons/icon_twitter_white.png"/></a>
            </div>
          </div>
          <div class="col-md-10 last-column">
            <div>
              © 2017–2024 Sentience Institute
            </div>
            <div>
              <a href="/terms">Terms and Conditions &amp; Privacy Policy</a>
            </div>
            <div>
              Thank you, <a href="https://weanimals.org/">Jo-Anne McArthur</a>, for granting us the use of so many photos.
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
    <script src="/js/ready.js?v=@version@"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-100318911-1', 'auto');
      ga('send', 'pageview');

    </script>
    
  </body>
</html>
